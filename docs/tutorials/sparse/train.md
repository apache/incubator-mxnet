# Training a Linear Regression Model with Sparse Symbols
In previous tutorials, we introduced `CSRNDArray` and `RowSparseNDArray`,
the basic data structures for manipulating sparse data.
MXNet also provides `Sparse Symbol` API, which enables symbolic expressions that handle sparse arrays.
In this tutorial, we first focus on how to compose a symbolic graph with sparse operators,
then train a linear classification model using sparse symbols with the Module API.

## Prerequisites

To complete this tutorial, we need:

- MXNet. See the instructions for your operating system in [Setup and Installation](http://mxnet.io/get_started/install.html).  

- [Jupyter Notebook](http://jupyter.org/index.html) and [Python Requests](http://docs.python-requests.org/en/master/) packages.
```
pip install jupyter requests
```

## Variables

Variables are placeholder for arrays. We can use them to hold sparse arrays, too.

### Variable Storage Types

The `stype` attribute of a variable is used to indicate the storage type of the array.
By default, the `stype` of a variable is "default" which indicates the default dense storage format.
We can specify the `stype` of a variable as "csr" or "row_sparse" to hold sparse arrays.

```python
import mxnet as mx
# create a variable to hold an NDArray
a = mx.sym.Variable('a')
# create a variable to hold a CSRNDArray
b = mx.sym.Variable('b', stype='csr')
# create a variable to hold a RowSparseNDArray
c = mx.sym.Variable('c', stype='row_sparse')
(a, b, c)
```

### Bind with Sparse Arrays

The sparse symbols constructed above declare storage types of the arrays to hold.
To evaluate them, we need to feed the free variables with sparse data.

We can instantiate an executor from a sparse symbol by using the `simple_bind` method,
which allocate zeros to all free variables according to their storage types.
The executor provides `forward` method for evaluation and an attribute
`outputs` to get all the results.

```python
shape = (2,2)
# instantiate an executor from sparse symbols
var_exec = mx.sym.Group([b, c]).simple_bind(ctx=mx.cpu(), b=shape, c=shape)
var_exec.forward()
# sparse arrays of zeros are bound to b and c
{'[b, c]': var_exec.outputs}
```

We can update the array held by the variable by accessing executor's `arg_dict` and assigning new values.

```python
var_exec.arg_dict['b'][:] = mx.nd.ones(shape).tostype('csr')
var_exec.forward()
# the array `b` holds are updated to be ones
eval_b = var_exec.outputs[0]
{'eval_b': eval_b, 'eval_b.asnumpy()': eval_b.asnumpy()}
```

## Symbol Composition and Storage Type Inference

### Basic Symbol Composition

The following example builds a simple element-wise addition expression with different storage types.
The sparse symbols are available in the `mx.sym.sparse` package.

```python
# element-wise addition of variables with "default" stype
d = mx.sym.elemwise_add(a, a)
# element-wise addition of variables with "csr" stype
e = mx.sym.sparse.elemwise_add(b, b)
# element-wise addition of variables with "row_sparse" stype
f = mx.sym.sparse.elemwise_add(c, c)
{'d':d, 'e':e, 'f':f}
```

### Storage Type Inference

What will be the output storage types of sparse symbols? In MXNet, for any sparse symbol, the result storage types are
inferred based on storage types of inputs.
You can read the [Sparse Symbol API](mxnet.io/api/python/symbol/sparse.html) documentation to find
what output storage types are.

```python
add_exec = mx.sym.Group([d, e, f]).simple_bind(ctx=mx.cpu(), a=shape, b=shape, c=shape)
add_exec.forward()
dense_add = add_exec.outputs[0]
# the output storage type of elemwise_add(csr, csr) will be inferred as "csr"
csr_add = add_exec.outputs[1]
# the output storage type of elemwise_add(row_sparse, row_sparse) will be inferred as "row_sparse"
rsp_add = add_exec.outputs[2]
{'dense_add.stype': dense_add.stype, 'csr_add.stype':csr_add.stype, 'rsp_add.stype': rsp_add.stype}
```

### Storage Type Fallback

For operators that don't specialize in certain sparse arrays, we can still use them with sparse inputs with some performance penalty.
In MXNet, dense operators require all inputs and outputs to be in the dense format.
If sparse inputs are provided, MXNet will convert sparse inputs into dense ones temporarily so that the dense operator can be used.
If sparse outputs are provided, MXNet will convert the dense outputs generated by the dense operator into the provided sparse format.
Warning messages will be printed when such a storage fallback event happens.

```python
# `log` operator doesn't support sparse inputs at all, but we can fallback on the dense implementation
csr_log = mx.sym.log(a)
# `elemwise_add` operator doesn't support adding csr with row_sparse, but we can fallback on the dense implementation
csr_rsp_add = mx.sym.elemwise_add(b, c)
fallback_exec = mx.sym.Group([csr_rsp_add, csr_log]).simple_bind(ctx=mx.cpu(), a=shape, b=shape, c=shape)
fallback_exec.forward()
fallback_add = fallback_exec.outputs[0]
fallback_log = fallback_exec.outputs[1]
{'fallback_add': fallback_add, 'fallback_log': fallback_log}
```

### Inspecting Storage Types of the Symbol Graph

When the environment variable `MXNET_EXEC_LOG_LEVEL` is set to `INFO`, MXNet will log the information of the graph executor,
including storage types of operators' inputs and outputs in the graph. For example, we can inspect the storage types of
a linear classification network with sparse operators as follows:

```python
# set logging level for executor
import os
os.environ['MXNET_EXEC_LOG_LEVEL'] = "INFO"
# data in csr format
data = mx.sym.var('data', stype='csr', shape=(32, 10000))
# weight in row_sparse format
weight = mx.sym.var('weight', stype='row_sparse', shape=(10000, 2))
bias = mx.symbol.Variable("bias", shape=(2,))
dot = mx.symbol.sparse.dot(x, weight)
pred = mx.symbol.broadcast_add(dot, bias)
y = mx.symbol.Variable("label")
output = mx.symbol.SoftmaxOutput(data=pred, label=y, name="output")
executor = output.simple_bind(ctx=mx.cpu())
```

<!-- INSERT SOURCE DOWNLOAD BUTTONS -->
