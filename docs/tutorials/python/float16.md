# Using float16 on supported devices

In this tutorial we'll walk through how one can train and infer deep learning modes with float16 on supported hardware.
The float16 data type also known as half precision float, is a 16 bit floating point representation according to the IEEE 754 standard.
It has a dynamic range where the precision can go from 0.0000000596046 (highest, for values closest to 0) to 32 (lowest, for values in the range 32768-65536).

Despite the decreased precision when compared to single precision (float32), the motivation for using float16 for deep learning comes from the idea that deep neural network architectures have natural resilience to errors due to backpropagation.
Half precision is typically sufficient for training neural networks.
This means that on hardware with specialized support for float16 computation we can greatly improve the speed of training and inference.
This speedup results from faster matrix multiplication, saving on memory bandwidth and reduced communication costs.
It can also reduce the size of the model, allowing us to train larger models and use larger batch sizes.

The Volta range of Graphics Processing Units (GPUs) from Nvidia have Tensor Cores for efficient float16 computation.
For the rest of this tutorial we assume that we are working with Nvidia's Tensor Cores on a Volta GPU.

## Prerequisites

- Volta or newer range of Nvidia GPUs
- Cuda 9 or higher
- CUDNN v7 or higher
The following python modules
- MXNet
- wget

## Using the Gluon API
### Training
This section shows how to train a Resnet50 model with Caltech256 dataset.

#### Preparing the dataset
We will create ImageFolderDataset objects from Caltech256 data as our dataset.

```python
import mxnet as mx

url = "http://www.vision.caltech.edu/Image_Datasets/Caltech256/256_ObjectCategories.tar"
mx.gluon.utils.download(url, path='data')

```



We need to take care of two things to convert a model to support float16.
1. Cast the Block to cast the parameters of layers and change the type of input expected to float16.
2. Cast the data to float16 to match the input type expected by the blocks.

Gluon Blocks have a cast method which casts parameters and changes the types of input expected.
This is not all though, we still need to ensure that data input to the block is in the form of float16.
This can be done by casting the data to float16. If the iterator supports generating data in float16 representation, we are set.
Else, we need to cast the data generated by data iterator.
An example of this can be seen in [example/gluon/image_classification.py](example/gluon/image_classification.py).

```python
for i, batch in enumerate(train_data):
    if batch.data[0].dtype != np.dtype(opt.dtype):
        batch.data[0] = batch.data[0].astype(opt.dtype)
    data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)
    label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)
    ...
```





### Training

MXNet's layers can generally work with any data type.
Operators infer the data type from input data and perform computation with that data type.
So to enable training or inference in float16, all we need to do is add a Cast layer before the first layer.
In this section, let us walk through the changes required to allow training with float16 precision.
We will be training a VGG11 network with Imagenet shaped dummy data in this example.
The complete example is in [train_imagenet.py](example/image-classification/train_imagenet.py) and the network is defined in [vgg.py](example/image-classification/symbols/vgg.py).

Firstly, we need to ensure that the data received by the network is of type float16.
If the data iterator itself is producing float16 data, then we are set.
If not, we can add a cast layer as the first layer of the network.

But softmax requires fp32, so cast it back at the end.

### Fine tuning a model trained in float32

This requires the pre-trained symbol to support fp16.
If we load a symbol trained with float32, then it would continue to expect float32.
We need to create the same model as used to train the model, and add cast layers to use float16 input.

## Using Gluon API

### Training

We need to take care of two things to convert a model to support float16.
Gluon Blocks have a cast method which casts parameters and changes the types of input expected.
This is not all though, we still need to ensure that data input to the block is in the form of float16.
This can be done by casting the data to float16. If the iterator supports generating data in float16 representation, we are set.
Else, we need to cast the data generated by data iterator.
An example of this can be seen in [example/gluon/image_classification.py](example/gluon/image_classification.py).

```python
for i, batch in enumerate(train_data):
    if batch.data[0].dtype != np.dtype(opt.dtype):
        batch.data[0] = batch.data[0].astype(opt.dtype)
    data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)
    label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)
    ...
```


### Fine tuning

Looks like we can just cast the block we load. Need to test it. Segfaults.


## Things to keep in mind
- Setting the environment variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 2 can help run tests to pick the fastest convolution algorithm at runtime.
Refer [Environment variables in MXNet](env_var.md) for more details.
- Batch size: It's recommended to use batch sizes which are multiples of 8, as Nvidia's Tensor Cores perform best when dimensions of inputs are multiples of 8.
- Model size: For smaller models like when training Resnet50 for Cifar10, most matrices involved in the computation can not benefit from Tensor cores.
Training on a single GPU with float16 in such a case can thus even be slower than training with float32.
- When training using multiple GPUs, reduced communication times with float16 also contribute to improved performance.
- You can check whether your program is using Tensor cores for fast float16 computation by profiling with `nvprof`.
Operations with `s884cudnn` in their names represent the use of Tensor cores.


