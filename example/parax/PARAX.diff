Only in /home/xxx/diff/example: image_classification
diff -r /home/xxx/diff/include/mxnet/ndarray.h /home/xxx/yyy/incubator-mxnet/include/mxnet/ndarray.h
193,197c193
<   void SetShapeFromChunk() {
<     if (!(ptr_->storage_shape.ndim() == 1 && ptr_->storage_shape[0] == 0)) {
<       shape_ = ptr_->storage_shape;
<     }
<   }
---
>   void SetShapeFromChunk();
200c196
<    * reshape or slice). If an array is a view and the the data is stored in
---
>    * reshape or slice). If an array is a view and the data is stored in
287,296d282
< 
<   const void shareTB(void *handle) const{
<     ptr_->shandle.dptr = handle;
<     ptr_->is_shared = true;
<   }
<   
<   const bool isshared() const{
<     return ptr_->is_shared;
<   }
<   
749,751d734
<   int TimesVisit() const;
<   void Visit() const;
<   void SetSharedMem(const mkldnn::memory::primitive_desc &pd, void* handle);
753,754c736
<   
< /*
---
>   /*
864,867d845
<     bool is_shared = false;
< 
< //    void* true_handle = NULL;
<     
889c867
<     Chunk() : static_data(true), delay_alloc(false), is_shared(false),
---
>     Chunk() : static_data(true), delay_alloc(false),
895c873
<         : static_data(false), delay_alloc(true), is_shared(false), ctx(ctx_),
---
>         : static_data(false), delay_alloc(true), ctx(ctx_),
910,911c888,889
<         : static_data(true), delay_alloc(false), is_shared(false),
<           storage_ref_(Storage::_GetSharedRef()), 
---
>         : static_data(true), delay_alloc(false),
>           storage_ref_(Storage::_GetSharedRef()),
929c907
<         : static_data(false), delay_alloc(false), is_shared(false),
---
>         : static_data(false), delay_alloc(false),
945c923
<         : static_data(false), delay_alloc(delay_alloc_), /*is_shared(false),*/ storage_type(storage_type_),
---
>         : static_data(false), delay_alloc(delay_alloc_), storage_type(storage_type_),
965c943
<         : static_data(true), delay_alloc(false), /*is_shared(false),*/ storage_type(storage_type_),
---
>         : static_data(true), delay_alloc(false), storage_type(storage_type_),
1074d1051
<     bool IsShared();
diff -r /home/xxx/diff/python/mxnet/model.py /home/xxx/yyy/incubator-mxnet/python/mxnet/model.py
1,1051c1,1036
< # Licensed to the Apache Software Foundation (ASF) under one
< # or more contributor license agreements.  See the NOTICE file
< # distributed with this work for additional information
< # regarding copyright ownership.  The ASF licenses this file
< # to you under the Apache License, Version 2.0 (the
< # "License"); you may not use this file except in compliance
< # with the License.  You may obtain a copy of the License at
< #
< #   http://www.apache.org/licenses/LICENSE-2.0
< #
< # Unless required by applicable law or agreed to in writing,
< # software distributed under the License is distributed on an
< # "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
< # KIND, either express or implied.  See the License for the
< # specific language governing permissions and limitations
< # under the License.
< 
< # pylint: disable=fixme, invalid-name, too-many-arguments, too-many-locals, too-many-lines
< # pylint: disable=too-many-branches, too-many-statements
< """MXNet model module"""
< from __future__ import absolute_import, print_function
< 
< import os
< import time
< import logging
< import warnings
< from collections import namedtuple
< import numpy as np
< import posix_ipc
< import mmap
< from . import io
< from . import ndarray as nd
< from . import symbol as sym
< from . import optimizer as opt
< from . import metric
< from . import kvstore as kvs
< from .context import Context, cpu
< from .initializer import Uniform
< from .optimizer import get_updater
< from .executor_manager import DataParallelExecutorManager, _check_arguments, _load_data
< from .io import DataDesc
< from .base import mx_real_t
< import mxnet as mx
< import logging as logger
< 
< BASE_ESTIMATOR = object
< 
< try:
<     from sklearn.base import BaseEstimator
<     BASE_ESTIMATOR = BaseEstimator
< except ImportError:
<     SKLEARN_INSTALLED = False
< 
< # Parameter to pass to batch_end_callback
< BatchEndParam = namedtuple('BatchEndParams',
<                            ['epoch',
<                             'nbatch',
<                             'eval_metric',
<                             'locals'])
< 
< def _create_kvstore(kvstore, num_device, arg_params):
<     """Create kvstore
<     This function select and create a proper kvstore if given the kvstore type.
< 
<     Parameters
<     ----------
<     kvstore : KVStore or str
<         The kvstore.
<     num_device : int
<         The number of devices
<     arg_params : dict of str to `NDArray`.
<         Model parameter, dict of name to `NDArray` of net's weights.
<     """
<     update_on_kvstore = True
<     if kvstore is None:
<         kv = None
<     elif isinstance(kvstore, kvs.KVStore):
<         kv = kvstore
<     elif isinstance(kvstore, str):
<         # create kvstore using the string type
<         if num_device is 1 and 'dist' not in kvstore:
<             # no need to use kv for single device and single machine
<             kv = None
<         else:
<             kv = kvs.create(kvstore)
<             if kvstore == 'local':
<             # automatically select a proper local
<                 max_size = max(np.prod(param.shape) for param in
<                                arg_params.values())
<                 if max_size > 1024 * 1024 * 16:
<                     update_on_kvstore = False
<     else:
<         raise TypeError('kvstore must be KVStore, str or None')
< 
<     if kv is None:
<         update_on_kvstore = False
< 
<     return (kv, update_on_kvstore)
< 
< def _initialize_kvstore(kvstore, param_arrays, arg_params, param_names, update_on_kvstore):
<     """Initialize kvstore"""
<     for idx, param_on_devs in enumerate(param_arrays):
<         name = param_names[idx]
<         kvstore.init(name, arg_params[name])
< 
<         if update_on_kvstore:
<             kvstore.pull(name, param_on_devs, priority=-idx)
< 
< def _update_params_on_kvstore_nccl(param_arrays, grad_arrays, kvstore, param_names):
<     """Perform update of param_arrays from grad_arrays on NCCL kvstore."""
<     valid_indices = [index for index, grad_list in
<                      enumerate(grad_arrays) if grad_list[0] is not None]
<     valid_grad_arrays = [grad_arrays[i] for i in valid_indices]
<     valid_param_arrays = [param_arrays[i] for i in valid_indices]
<     valid_param_names = [param_names[i] for i in valid_indices]
<     size = len(valid_grad_arrays)
<     start = 0
<     # Use aggregation by default only with NCCL
<     default_batch = 16
<     batch = int(os.getenv('MXNET_UPDATE_AGGREGATION_SIZE', default_batch))
<     while start < size:
<         end = start + batch if start + batch < size else size
<         # push gradient, priority is negative index
<         kvstore.push(valid_param_names[start:end], valid_grad_arrays[start:end], priority=-start)
<         # pull back the weights
<         kvstore.pull(valid_param_names[start:end], valid_param_arrays[start:end], priority=-start)
<         start = end
< 
< import time
< from mxnet.ndarray import ndarray as nnd
< #from ndarray import _new_from_shared_mem
< 
< def _update_params_on_kvstore(param_arrays, grad_arrays, kvstore, param_names):
<     """Perform update of param_arrays from grad_arrays on kvstore."""
<     for index, pair in enumerate(zip(param_arrays, grad_arrays)):
<         arg_list, grad_list = pair
<         if grad_list[0] is None:
<             continue
<         name = param_names[index]
<         # push gradient, priority is negative index
<         kvstore.push(name, grad_list, priority=-index)
<         # pull back the weights
<         kvstore.pull(name, arg_list, priority=-index)
< 
< def _init_shnd(param_arrays, grad_arrays, nd, is_chief):
< 
<     l = len(grad_arrays)
<     size = 24 * 2 * l
<     print(is_chief)
<     for i, pair in enumerate(zip(param_arrays, grad_arrays)):
<         arg_list, grad_list = pair
<         a,b,c,d = grad_list[0]._to_shared_mem()
< 
<         shm = posix_ipc.SharedMemory('/home/grad', posix_ipc.O_CREAT, size=size)
<         buf = mmap.mmap(shm.fd, size)
<         shape = (l, 2)
<         mx_shm_key = np.ndarray(shape, int, buf, order = 'C')
<         if is_chief == 0:
<             mx_shm_key[i][0] = a
<             mx_shm_key[i][1] = b
<         y = nnd._new_from_shared_mem(mx_shm_key[i][0], mx_shm_key[i][1], grad_list[0].shape, np.float32)
<         nd.append(nnd.NDArray(y))
<         if is_chief == 0:
<             nd[i].zeros_like()
< 
<     if is_chief == 0:
<         size = 12 * 24
<         shm = posix_ipc.SharedMemory('/home/sync', posix_ipc.O_CREAT, size=size)
<         buf = mmap.mmap(shm.fd, size)
<         shape = 12
<         sync = np.ndarray(shape, int, buf, order = 'C')
<         for i in range(shape):
<             sync[i] = 0
< 
< def _update_params(param_arrays, grad_arrays, work_id, updater, num_device,
<                    kvstore=None, param_names=None, nd = None):
<     """Perform update of param_arrays from grad_arrays not on kvstore."""
<     for i, pair in enumerate(zip(param_arrays, grad_arrays)):
<         arg_list, grad_list = pair
<         if grad_list[0] is None:
<             continue
<         nd[i] += grad_list[0]
< 
<     #sync by shared_memory
<     size = 12 * 24
<     shm = posix_ipc.SharedMemory('/home/sync', posix_ipc.O_CREAT, size = size)
<     buf = mmap.mmap(shm.fd, size)
<     shape = 12
<     sync = np.ndarray(shape, int, buf, order = 'C')
<     sync[work_id] += 1
< 
<     while (sync[0] != sync[1]) or (sync[0] != sync[3]) or (sync[0] != sync[2]):
<         pass
< 
<     for i, pair in enumerate(zip(param_arrays, grad_arrays)):
<         arg_list, grad_list = pair
<         if grad_list[0] is None:
<             continue
<         index = i
<         updater(index*num_device, nd[i], arg_list[0])
< 
<     sync[work_id + 4] += 1
<     while (sync[4] != sync[5]) or (sync[4] != sync[6]) or (sync[4] != sync[7]):
<         pass
< 
<     for i, pair in enumerate(zip(param_arrays, grad_arrays)):
<         arg_list, grad_list = pair
<         if grad_list[0] is None:
<             continue
<         nd[i].zeros_like()
<     sync[work_id + 8] += 1
< 
<     while (sync[8] != sync[9]) or (sync[8] != sync[10]) or (sync[8] != sync[11]):
<         pass
< 
< def _multiple_callbacks(callbacks, *args, **kwargs):
<     """Sends args and kwargs to any configured callbacks.
<     This handles the cases where the 'callbacks' variable
<     is ``None``, a single function, or a list.
<     """
<     if isinstance(callbacks, list):
<         for cb in callbacks:
<             cb(*args, **kwargs)
<         return
<     if callbacks:
<         callbacks(*args, **kwargs)
< 
< 
< def _train_multi_device(symbol, ctx, arg_names, param_names, aux_names,
<                         arg_params, aux_params,
<                         begin_epoch, end_epoch, epoch_size, optimizer,
<                         kvstore, update_on_kvstore,
<                         train_data, eval_data=None, eval_metric=None,
<                         epoch_end_callback=None, batch_end_callback=None,
<                         logger=None, work_load_list=None, monitor=None,
<                         eval_end_callback=None,
<                         eval_batch_end_callback=None, sym_gen=None):
<     """Internal training function on multiple devices.
<     This function will also work for single device as well.
< 
<     Parameters
<     ----------
<     symbol : Symbol
<         The network configuration.
<     ctx : list of Context
<         The training devices.
<     arg_names: list of str
<         Name of all arguments of the network.
<     param_names: list of str
<         Name of all trainable parameters of the network.
<     aux_names: list of str
<         Name of all auxiliary states of the network.
<     arg_params : dict of str to NDArray
<         Model parameter, dict of name to NDArray of net's weights.
<     aux_params : dict of str to NDArray
<         Model parameter, dict of name to NDArray of net's auxiliary states.
<     begin_epoch : int
<         The begining training epoch.
<     end_epoch : int
<         The end training epoch.
<     epoch_size : int, optional
<         Number of batches in a epoch. In default, it is set to
<         ``ceil(num_train_examples / batch_size)``.
<     optimizer : Optimizer
<         The optimization algorithm
<     train_data : DataIter
<         Training data iterator.
<     eval_data : DataIter
<         Validation data iterator.
<     eval_metric : EvalMetric
<         An evaluation function or a list of evaluation functions.
<     epoch_end_callback : callable(epoch, symbol, arg_params, aux_states)
<         A callback that is invoked at end of each epoch.
<         This can be used to checkpoint model each epoch.
<     batch_end_callback : callable(BatchEndParams)
<         A callback that is invoked at end of each batch.
<         This can be used to measure speed, get result from evaluation metric. etc.
<     kvstore : KVStore
<         The KVStore.
<     update_on_kvstore : bool
<         Whether or not perform weight updating on kvstore.
<     logger : logging logger
<         When not specified, default logger will be used.
<     work_load_list : list of float or int, optional
<         The list of work load for different devices,
<         in the same order as ``ctx``.
<     monitor : Monitor, optional
<         Monitor installed to executor,
<         for monitoring outputs, weights, and gradients for debugging.
<     Notes
<     -----
<     - This function will inplace update the NDArrays in `arg_params` and `aux_states`.
<     """
<     if logger is None:
<         logger = logging
<     executor_manager = DataParallelExecutorManager(symbol=symbol,
<                                                    sym_gen=sym_gen,
<                                                    ctx=ctx,
<                                                    train_data=train_data,
<                                                    param_names=param_names,
<                                                    arg_names=arg_names,
<                                                    aux_names=aux_names,
<                                                    work_load_list=work_load_list,
<                                                    logger=logger)
<     if monitor:
<         executor_manager.install_monitor(monitor)
< 
<     executor_manager.set_params(arg_params, aux_params)
< 
<     if not update_on_kvstore:
<         updater = get_updater(optimizer)
<     else:
<         kvstore.set_optimizer(optimizer)
< 
<     if kvstore:
<         _initialize_kvstore(kvstore=kvstore,
<                             param_arrays=executor_manager.param_arrays,
<                             arg_params=arg_params,
<                             param_names=executor_manager.param_names,
<                             update_on_kvstore=update_on_kvstore)
< 
<     # Now start training
<     train_data.reset()
<     for epoch in range(begin_epoch, end_epoch):
<         # Training phase
<         tic = time.time()
<         eval_metric.reset()
<         nbatch = 0
<         # Iterate over training data.
<         while True:
<             do_reset = True
<             for data_batch in train_data:
<                 executor_manager.load_data_batch(data_batch)
< 
<                 if monitor is not None:
<                     monitor.tic()
< 
<                 executor_manager.forward(is_train=True)
<                 executor_manager.backward()
< 
<                 if update_on_kvstore:
<                     if 'nccl' in kvstore.type:
<                         _update_params_on_kvstore_nccl(executor_manager.param_arrays,
<                                                        executor_manager.grad_arrays,
<                                                        kvstore, executor_manager.param_names)
<                     else:
<                         _update_params_on_kvstore(executor_manager.param_arrays,
<                                                   executor_manager.grad_arrays,
<                                                   kvstore, executor_manager.param_names)
<                 else:
<                     print('fsfs')
<                     _update_params(executor_manager.param_arrays,
<                                    executor_manager.grad_arrays,
<                                    updater=updater,
<                                    num_device=len(ctx),
<                                    kvstore=kvstore,
<                                    param_names=executor_manager.param_names)
< 
<                 if monitor is not None:
<                     monitor.toc_print()
< 
<                 # evaluate at end, so we can lazy copy
<                 executor_manager.update_metric(eval_metric, data_batch.label)
< 
<                 nbatch += 1
<                 # batch callback (for print purpose)
<                 if batch_end_callback is not None:
<                     batch_end_params = BatchEndParam(epoch=epoch,
<                                                      nbatch=nbatch,
<                                                      eval_metric=eval_metric,
<                                                      locals=locals())
<                     _multiple_callbacks(batch_end_callback, batch_end_params)
< 
<                 # this epoch is done possibly earlier
<                 if epoch_size is not None and nbatch >= epoch_size:
<                     do_reset = False
<                     break
< 
<             if do_reset:
<                 logger.info('Epoch[%d] Resetting Data Iterator', epoch)
<                 train_data.reset()
< 
<             # this epoch is done
<             if epoch_size is None or nbatch >= epoch_size:
<                 break
< 
<         toc = time.time()
<         logger.info('Epoch[%d] Time cost=%.3f', epoch, (toc - tic))
< 
<         if epoch_end_callback or epoch + 1 == end_epoch:
<             executor_manager.copy_to(arg_params, aux_params)
< 
<         _multiple_callbacks(epoch_end_callback, epoch, symbol, arg_params, aux_params)
< 
<         # evaluation
<         if eval_data:
<             eval_metric.reset()
<             eval_data.reset()
<             total_num_batch = 0
<             for i, eval_batch in enumerate(eval_data):
<                 executor_manager.load_data_batch(eval_batch)
<                 executor_manager.forward(is_train=False)
<                 executor_manager.update_metric(eval_metric, eval_batch.label)
<                 if eval_batch_end_callback is not None:
<                     batch_end_params = BatchEndParam(epoch=epoch,
<                                                      nbatch=i,
<                                                      eval_metric=eval_metric,
<                                                      locals=locals())
<                     _multiple_callbacks(eval_batch_end_callback, batch_end_params)
<                 total_num_batch += 1
<             if eval_end_callback is not None:
<                 eval_end_params = BatchEndParam(epoch=epoch,
<                                                 nbatch=total_num_batch,
<                                                 eval_metric=eval_metric,
<                                                 locals=locals())
<                 _multiple_callbacks(eval_end_callback, eval_end_params)
<             eval_data.reset()
<     # end of all epochs
<     return
< 
< 
< def save_checkpoint(prefix, epoch, symbol, arg_params, aux_params):
<     """Checkpoint the model data into file.
< 
<     Parameters
<     ----------
<     prefix : str
<         Prefix of model name.
<     epoch : int
<         The epoch number of the model.
<     symbol : Symbol
<         The input Symbol.
<     arg_params : dict of str to NDArray
<         Model parameter, dict of name to NDArray of net's weights.
<     aux_params : dict of str to NDArray
<         Model parameter, dict of name to NDArray of net's auxiliary states.
<     Notes
<     -----
<     - ``prefix-symbol.json`` will be saved for symbol.
<     - ``prefix-epoch.params`` will be saved for parameters.
<     """
<     if symbol is not None:
<         symbol.save('%s-symbol.json' % prefix)
< 
<     save_dict = {('arg:%s' % k) : v.as_in_context(cpu()) for k, v in arg_params.items()}
<     save_dict.update({('aux:%s' % k) : v.as_in_context(cpu()) for k, v in aux_params.items()})
<     param_name = '%s-%04d.params' % (prefix, epoch)
<     nd.save(param_name, save_dict)
<     logging.info('Saved checkpoint to \"%s\"', param_name)
< 
< 
< def load_checkpoint(prefix, epoch):
<     """Load model checkpoint from file.
< 
<     Parameters
<     ----------
<     prefix : str
<         Prefix of model name.
<     epoch : int
<         Epoch number of model we would like to load.
< 
<     Returns
<     -------
<     symbol : Symbol
<         The symbol configuration of computation network.
<     arg_params : dict of str to NDArray
<         Model parameter, dict of name to NDArray of net's weights.
<     aux_params : dict of str to NDArray
<         Model parameter, dict of name to NDArray of net's auxiliary states.
< 
<     Notes
<     -----
<     - Symbol will be loaded from ``prefix-symbol.json``.
<     - Parameters will be loaded from ``prefix-epoch.params``.
<     """
<     symbol = sym.load('%s-symbol.json' % prefix)
<     save_dict = nd.load('%s-%04d.params' % (prefix, epoch))
<     arg_params = {}
<     aux_params = {}
<     for k, v in save_dict.items():
<         tp, name = k.split(':', 1)
<         if tp == 'arg':
<             arg_params[name] = v
<         if tp == 'aux':
<             aux_params[name] = v
<     return (symbol, arg_params, aux_params)
< 
< from .callback import LogValidationMetricsCallback # pylint: disable=wrong-import-position
< 
< class FeedForward(BASE_ESTIMATOR):
<     """Model class of MXNet for training and predicting feedforward nets.
<     This class is designed for a single-data single output supervised network.
< 
<     Parameters
<     ----------
<     symbol : Symbol
<         The symbol configuration of computation network.
<     ctx : Context or list of Context, optional
<         The device context of training and prediction.
<         To use multi GPU training, pass in a list of gpu contexts.
<     num_epoch : int, optional
<         Training parameter, number of training epochs(epochs).
<     epoch_size : int, optional
<         Number of batches in a epoch. In default, it is set to
<         ``ceil(num_train_examples / batch_size)``.
<     optimizer : str or Optimizer, optional
<         Training parameter, name or optimizer object for training.
<     initializer : initializer function, optional
<         Training parameter, the initialization scheme used.
<     numpy_batch_size : int, optional
<         The batch size of training data.
<         Only needed when input array is numpy.
<     arg_params : dict of str to NDArray, optional
<         Model parameter, dict of name to NDArray of net's weights.
<     aux_params : dict of str to NDArray, optional
<         Model parameter, dict of name to NDArray of net's auxiliary states.
<     allow_extra_params : boolean, optional
<         Whether allow extra parameters that are not needed by symbol
<         to be passed by aux_params and ``arg_params``.
<         If this is True, no error will be thrown when ``aux_params`` and ``arg_params``
<         contain more parameters than needed.
<     begin_epoch : int, optional
<         The begining training epoch.
<     kwargs : dict
<         The additional keyword arguments passed to optimizer.
<     """
<     def __init__(self, symbol, ctx=None,
<                  num_epoch=None, epoch_size=None, optimizer='sgd',
<                  initializer=Uniform(0.01),
<                  numpy_batch_size=128,
<                  arg_params=None, aux_params=None,
<                  allow_extra_params=False,
<                  begin_epoch=0,
<                  **kwargs):
<         warnings.warn(
<             '\033[91mmxnet.model.FeedForward has been deprecated. ' + \
<             'Please use mxnet.mod.Module instead.\033[0m',
<             DeprecationWarning, stacklevel=2)
< 
<         if isinstance(symbol, sym.Symbol):
<             self.symbol = symbol
<             self.sym_gen = None
<         else:
<             assert(callable(symbol))
<             self.symbol = None
<             self.sym_gen = symbol
< 
<         # model parameters
<         self.arg_params = arg_params
<         self.aux_params = aux_params
<         self.allow_extra_params = allow_extra_params
< 
<         self.argument_checked = False
<         if self.sym_gen is None:
<             self._check_arguments()
< 
<         # basic configuration
<         if ctx is None:
<             ctx = [cpu()]
<         elif isinstance(ctx, Context):
<             ctx = [ctx]
<         self.ctx = ctx
<         # training parameters
<         self.num_epoch = num_epoch
<         self.epoch_size = epoch_size
<         self.kwargs = kwargs.copy()
<         self.optimizer = optimizer
<         self.initializer = initializer
<         self.numpy_batch_size = numpy_batch_size
<         # internal helper state
<         self._pred_exec = None
<         self.begin_epoch = begin_epoch
< 
<     def _check_arguments(self):
<         """verify the argument of the default symbol and user provided parameters"""
<         if self.argument_checked:
<             return
< 
<         assert(self.symbol is not None)
<         self.argument_checked = True
< 
<         # check if symbol contain duplicated names.
<         _check_arguments(self.symbol)
<         # rematch parameters to delete useless ones
<         if self.allow_extra_params:
<             if self.arg_params:
<                 arg_names = set(self.symbol.list_arguments())
<                 self.arg_params = {k : v for k, v in self.arg_params.items()
<                                    if k in arg_names}
<             if self.aux_params:
<                 aux_names = set(self.symbol.list_auxiliary_states())
<                 self.aux_params = {k : v for k, v in self.aux_params.items()
<                                    if k in aux_names}
< 
< 
<     @staticmethod
<     def _is_data_arg(name):
<         """Check if name is a data argument."""
<         return name.endswith('data') or name.endswith('label')
< 
<     def _init_params(self, inputs, overwrite=False):
<         """Initialize weight parameters and auxiliary states."""
<         inputs = [x if isinstance(x, DataDesc) else DataDesc(*x) for x in inputs]
<         input_shapes = {item.name: item.shape for item in inputs}
<         arg_shapes, _, aux_shapes = self.symbol.infer_shape(**input_shapes)
<         assert arg_shapes is not None
<         input_dtypes = {item.name: item.dtype for item in inputs}
<         arg_dtypes, _, aux_dtypes = self.symbol.infer_type(**input_dtypes)
<         assert arg_dtypes is not None
< 
<         arg_names = self.symbol.list_arguments()
<         input_names = input_shapes.keys()
<         param_names = [key for key in arg_names if key not in input_names]
<         aux_names = self.symbol.list_auxiliary_states()
< 
<         param_name_attrs = [x for x in zip(arg_names, arg_shapes, arg_dtypes)
<                             if x[0] in param_names]
<         arg_params = {k : nd.zeros(shape=s, dtype=t)
<                       for k, s, t in param_name_attrs}
<         aux_name_attrs = [x for x in zip(aux_names, aux_shapes, aux_dtypes)
<                           if x[0] in aux_names]
<         aux_params = {k : nd.zeros(shape=s, dtype=t)
<                       for k, s, t in aux_name_attrs}
< 
<         for k, v in arg_params.items():
<             if self.arg_params and k in self.arg_params and (not overwrite):
<                 arg_params[k][:] = self.arg_params[k][:]
<             else:
<                 self.initializer(k, v)
< 
<         for k, v in aux_params.items():
<             if self.aux_params and k in self.aux_params and (not overwrite):
<                 aux_params[k][:] = self.aux_params[k][:]
<             else:
<                 self.initializer(k, v)
< 
<         self.arg_params = arg_params
<         self.aux_params = aux_params
<         return (arg_names, list(param_names), aux_names)
< 
<     def __getstate__(self):
<         this = self.__dict__.copy()
<         this['_pred_exec'] = None
<         return this
< 
<     def __setstate__(self, state):
<         self.__dict__.update(state)
< 
<     def _init_predictor(self, input_shapes, type_dict=None):
<         """Initialize the predictor module for running prediction."""
<         shapes = {name: self.arg_params[name].shape for name in self.arg_params}
<         shapes.update(dict(input_shapes))
<         if self._pred_exec is not None:
<             arg_shapes, _, _ = self.symbol.infer_shape(**shapes)
<             assert arg_shapes is not None, "Incomplete input shapes"
<             pred_shapes = [x.shape for x in self._pred_exec.arg_arrays]
<             if arg_shapes == pred_shapes:
<                 return
<         # for now only use the first device
<         pred_exec = self.symbol.simple_bind(
<             self.ctx[0], grad_req='null', type_dict=type_dict, **shapes)
<         pred_exec.copy_params_from(self.arg_params, self.aux_params)
< 
<         _check_arguments(self.symbol)
<         self._pred_exec = pred_exec
< 
<     def _init_iter(self, X, y, is_train):
<         """Initialize the iterator given input."""
<         if isinstance(X, (np.ndarray, nd.NDArray)):
<             if y is None:
<                 if is_train:
<                     raise ValueError('y must be specified when X is numpy.ndarray')
<                 else:
<                     y = np.zeros(X.shape[0])
<             if not isinstance(y, (np.ndarray, nd.NDArray)):
<                 raise TypeError('y must be ndarray when X is numpy.ndarray')
<             if X.shape[0] != y.shape[0]:
<                 raise ValueError("The numbers of data points and labels not equal")
<             if y.ndim == 2 and y.shape[1] == 1:
<                 y = y.flatten()
<             if y.ndim != 1:
<                 raise ValueError("Label must be 1D or 2D (with 2nd dimension being 1)")
<             if is_train:
<                 return io.NDArrayIter(X, y, min(X.shape[0], self.numpy_batch_size),
<                                       shuffle=is_train, last_batch_handle='roll_over')
<             else:
<                 return io.NDArrayIter(X, y, min(X.shape[0], self.numpy_batch_size), shuffle=False)
<         if not isinstance(X, io.DataIter):
<             raise TypeError('X must be DataIter, NDArray or numpy.ndarray')
<         return X
< 
<     def _init_eval_iter(self, eval_data):
<         """Initialize the iterator given eval_data."""
<         if eval_data is None:
<             return eval_data
<         if isinstance(eval_data, (tuple, list)) and len(eval_data) == 2:
<             if eval_data[0] is not None:
<                 if eval_data[1] is None and isinstance(eval_data[0], io.DataIter):
<                     return eval_data[0]
<                 input_data = (np.array(eval_data[0]) if isinstance(eval_data[0], list)
<                               else eval_data[0])
<                 input_label = (np.array(eval_data[1]) if isinstance(eval_data[1], list)
<                                else eval_data[1])
<                 return self._init_iter(input_data, input_label, is_train=True)
<             else:
<                 raise ValueError("Eval data is NONE")
<         if not isinstance(eval_data, io.DataIter):
<             raise TypeError('Eval data must be DataIter, or ' \
<                             'NDArray/numpy.ndarray/list pair (i.e. tuple/list of length 2)')
<         return eval_data
< 
<     def predict(self, X, num_batch=None, return_data=False, reset=True):
<         """Run the prediction, always only use one device.
< 
<         Parameters
<         ----------
<         X : mxnet.DataIter
<         num_batch : int or None
<             The number of batch to run. Go though all batches if ``None``.
<         Returns
<         -------
<         y : numpy.ndarray or a list of numpy.ndarray if the network has multiple outputs.
<             The predicted value of the output.
<         """
<         X = self._init_iter(X, None, is_train=False)
< 
<         if reset:
<             X.reset()
<         data_shapes = X.provide_data
<         data_names = [x[0] for x in data_shapes]
<         type_dict = dict((key, value.dtype) for (key, value) in self.arg_params.items())
<         for x in X.provide_data:
<             if isinstance(x, DataDesc):
<                 type_dict[x.name] = x.dtype
<             else:
<                 type_dict[x[0]] = mx_real_t
< 
<         self._init_predictor(data_shapes, type_dict)
<         batch_size = X.batch_size
<         data_arrays = [self._pred_exec.arg_dict[name] for name in data_names]
<         output_list = [[] for _ in range(len(self._pred_exec.outputs))]
<         if return_data:
<             data_list = [[] for _ in X.provide_data]
<             label_list = [[] for _ in X.provide_label]
< 
<         i = 0
<         for batch in X:
< 
<             _load_data(batch, data_arrays)
<             self._pred_exec.forward(is_train=False)
<             padded = batch.pad
<             real_size = batch_size - padded
< 
<             for o_list, o_nd in zip(output_list, self._pred_exec.outputs):
<                 o_list.append(o_nd[0:real_size].asnumpy())
< 
<             if return_data:
<                 for j, x in enumerate(batch.data):
<                     data_list[j].append(x[0:real_size].asnumpy())
<                 for j, x in enumerate(batch.label):
<                     label_list[j].append(x[0:real_size].asnumpy())
<             i += 1
<             if num_batch is not None and i == num_batch:
<                 break
< 
<         outputs = [np.concatenate(x) for x in output_list]
<         if len(outputs) == 1:
<             outputs = outputs[0]
< 
<         if return_data:
<             data = [np.concatenate(x) for x in data_list]
<             label = [np.concatenate(x) for x in label_list]
<             if len(data) == 1:
<                 data = data[0]
<             if len(label) == 1:
<                 label = label[0]
<             return outputs, data, label
<         else:
<             return outputs
< 
<     def score(self, X, eval_metric='acc', num_batch=None, batch_end_callback=None, reset=True):
<         """Run the model given an input and calculate the score
<         as assessed by an evaluation metric.
< 
<         Parameters
<         ----------
<         X : mxnet.DataIter
<         eval_metric : metric.metric
<             The metric for calculating score.
<         num_batch : int or None
<             The number of batches to run. Go though all batches if ``None``.
<         Returns
<         -------
<         s : float
<             The final score.
<         """
<         # setup metric
<         if not isinstance(eval_metric, metric.EvalMetric):
<             eval_metric = metric.create(eval_metric)
< 
<         X = self._init_iter(X, None, is_train=False)
<         if reset:
<             X.reset()
< 
<         data_shapes = X.provide_data
<         data_names = [x[0] for x in data_shapes]
<         type_dict = dict((key, value.dtype) for (key, value) in self.arg_params.items())
<         for x in X.provide_data:
<             if isinstance(x, DataDesc):
<                 type_dict[x.name] = x.dtype
<             else:
<                 type_dict[x[0]] = mx_real_t
< 
<         self._init_predictor(data_shapes, type_dict)
<         data_arrays = [self._pred_exec.arg_dict[name] for name in data_names]
< 
<         for i, batch in enumerate(X):
<             if num_batch is not None and i == num_batch:
<                 break
<             _load_data(batch, data_arrays)
<             self._pred_exec.forward(is_train=False)
<             eval_metric.update(batch.label, self._pred_exec.outputs)
< 
<             if batch_end_callback is not None:
<                 batch_end_params = BatchEndParam(epoch=0,
<                                                  nbatch=i,
<                                                  eval_metric=eval_metric,
<                                                  locals=locals())
<                 _multiple_callbacks(batch_end_callback, batch_end_params)
<         return eval_metric.get()[1]
< 
<     def fit(self, X, y=None, eval_data=None, eval_metric='acc',
<             epoch_end_callback=None, batch_end_callback=None, kvstore='local', logger=None,
<             work_load_list=None, monitor=None, eval_end_callback=LogValidationMetricsCallback(),
<             eval_batch_end_callback=None):
<         """Fit the model.
< 
<         Parameters
<         ----------
<         X : DataIter, or numpy.ndarray/NDArray
<             Training data. If `X` is a `DataIter`, the name or (if name not available)
<             the position of its outputs should match the corresponding variable
<             names defined in the symbolic graph.
<         y : numpy.ndarray/NDArray, optional
<             Training set label.
<             If X is ``numpy.ndarray`` or `NDArray`, `y` is required to be set.
<             While y can be 1D or 2D (with 2nd dimension as 1), its first dimension must be
<             the same as `X`, i.e. the number of data points and labels should be equal.
<         eval_data : DataIter or numpy.ndarray/list/NDArray pair
<             If eval_data is numpy.ndarray/list/NDArray pair,
<             it should be ``(valid_data, valid_label)``.
<         eval_metric : metric.EvalMetric or str or callable
<             The evaluation metric. This could be the name of evaluation metric
<             or a custom evaluation function that returns statistics
<             based on a minibatch.
<         epoch_end_callback : callable(epoch, symbol, arg_params, aux_states)
<             A callback that is invoked at end of each epoch.
<             This can be used to checkpoint model each epoch.
<         batch_end_callback: callable(epoch)
<             A callback that is invoked at end of each batch for purposes of printing.
<         kvstore: KVStore or str, optional
<            The KVStore or a string kvstore type: 'local', 'dist_sync', 'dist_async'
<            In default uses 'local', often no need to change for single machiine.
<         logger : logging logger, optional
<             When not specified, default logger will be used.
<         work_load_list : float or int, optional
<             The list of work load for different devices,
<             in the same order as `ctx`.
< 
<         Note
<         ----
<         KVStore behavior
<         - 'local', multi-devices on a single machine, will automatically choose best type.
<         - 'dist_sync', multiple machines communicating via BSP.
<         - 'dist_async', multiple machines with asynchronous communication.
<         """
< 
<         data = self._init_iter(X, y, is_train=True)
<         eval_data = self._init_eval_iter(eval_data)
< 
<         if self.sym_gen:
<             self.symbol = self.sym_gen(data.default_bucket_key) # pylint: disable=no-member
<             self._check_arguments()
<         self.kwargs["sym"] = self.symbol
< 
<         arg_names, param_names, aux_names = \
<                 self._init_params(data.provide_data+data.provide_label)
< 
<         # setup metric
<         if not isinstance(eval_metric, metric.EvalMetric):
<             eval_metric = metric.create(eval_metric)
< 
<         # create kvstore
<         (kvstore, update_on_kvstore) = _create_kvstore(
<             kvstore, len(self.ctx), self.arg_params)
< 
<         param_idx2name = {}
<         if update_on_kvstore:
<             param_idx2name.update(enumerate(param_names))
<         else:
<             for i, n in enumerate(param_names):
<                 for k in range(len(self.ctx)):
<                     param_idx2name[i*len(self.ctx)+k] = n
<         self.kwargs["param_idx2name"] = param_idx2name
< 
<         # init optmizer
<         if isinstance(self.optimizer, str):
<             batch_size = data.batch_size
<             if kvstore and 'dist' in kvstore.type and '_async' not in kvstore.type:
<                 batch_size *= kvstore.num_workers
<             optimizer = opt.create(self.optimizer,
<                                    rescale_grad=(1.0/batch_size),
<                                    **(self.kwargs))
<         elif isinstance(self.optimizer, opt.Optimizer):
<             optimizer = self.optimizer
< 
<         # do training
<         _train_multi_device(self.symbol, self.ctx, arg_names, param_names, aux_names,
<                             self.arg_params, self.aux_params,
<                             begin_epoch=self.begin_epoch, end_epoch=self.num_epoch,
<                             epoch_size=self.epoch_size,
<                             optimizer=optimizer,
<                             train_data=data, eval_data=eval_data,
<                             eval_metric=eval_metric,
<                             epoch_end_callback=epoch_end_callback,
<                             batch_end_callback=batch_end_callback,
<                             kvstore=kvstore, update_on_kvstore=update_on_kvstore,
<                             logger=logger, work_load_list=work_load_list, monitor=monitor,
<                             eval_end_callback=eval_end_callback,
<                             eval_batch_end_callback=eval_batch_end_callback,
<                             sym_gen=self.sym_gen)
< 
< 
<     def save(self, prefix, epoch=None):
<         """Checkpoint the model checkpoint into file.
<         You can also use `pickle` to do the job if you only work on Python.
<         The advantage of `load` and `save` (as compared to `pickle`) is that
<         the resulting file can be loaded from other MXNet language bindings.
<         One can also directly `load`/`save` from/to cloud storage(S3, HDFS)
< 
<         Parameters
<         ----------
<         prefix : str
<             Prefix of model name.
< 
<         Notes
<         -----
<         - ``prefix-symbol.json`` will be saved for symbol.
<         - ``prefix-epoch.params`` will be saved for parameters.
<         """
<         if epoch is None:
<             epoch = self.num_epoch
<         assert epoch is not None
<         save_checkpoint(prefix, epoch, self.symbol, self.arg_params, self.aux_params)
< 
<     @staticmethod
<     def load(prefix, epoch, ctx=None, **kwargs):
<         """Load model checkpoint from file.
< 
<         Parameters
<         ----------
<         prefix : str
<             Prefix of model name.
<         epoch : int
<             epoch number of model we would like to load.
<         ctx : Context or list of Context, optional
<             The device context of training and prediction.
<         kwargs : dict
<             Other parameters for model, including `num_epoch`, optimizer and `numpy_batch_size`.
< 
<         Returns
<         -------
<         model : FeedForward
<             The loaded model that can be used for prediction.
< 
<         Notes
<         -----
<         - ``prefix-symbol.json`` will be saved for symbol.
<         - ``prefix-epoch.params`` will be saved for parameters.
<         """
<         symbol, arg_params, aux_params = load_checkpoint(prefix, epoch)
<         return FeedForward(symbol, ctx=ctx,
<                            arg_params=arg_params, aux_params=aux_params,
<                            begin_epoch=epoch,
<                            **kwargs)
< 
<     @staticmethod
<     def create(symbol, X, y=None, ctx=None,
<                num_epoch=None, epoch_size=None, optimizer='sgd', initializer=Uniform(0.01),
<                eval_data=None, eval_metric='acc',
<                epoch_end_callback=None, batch_end_callback=None,
<                kvstore='local', logger=None, work_load_list=None,
<                eval_end_callback=LogValidationMetricsCallback(),
<                eval_batch_end_callback=None, **kwargs):
<         """Functional style to create a model.
<         This function is more consistent with functional
<         languages such as R, where mutation is not allowed.
< 
<         Parameters
<         ----------
<         symbol : Symbol
<             The symbol configuration of a computation network.
<         X : DataIter
<             Training data.
<         y : numpy.ndarray, optional
<             If `X` is a ``numpy.ndarray``, `y` must be set.
<         ctx : Context or list of Context, optional
<             The device context of training and prediction.
<             To use multi-GPU training, pass in a list of GPU contexts.
<         num_epoch : int, optional
<             The number of training epochs(epochs).
<         epoch_size : int, optional
<             Number of batches in a epoch. In default, it is set to
<             ``ceil(num_train_examples / batch_size)``.
<         optimizer : str or Optimizer, optional
<             The name of the chosen optimizer, or an optimizer object, used for training.
<         initializer : initializer function, optional
<             The initialization scheme used.
<         eval_data : DataIter or numpy.ndarray pair
<             If `eval_set` is ``numpy.ndarray`` pair, it should
<             be (`valid_data`, `valid_label`).
<         eval_metric : metric.EvalMetric or str or callable
<             The evaluation metric. Can be the name of an evaluation metric
<             or a custom evaluation function that returns statistics
<             based on a minibatch.
<         epoch_end_callback : callable(epoch, symbol, arg_params, aux_states)
<             A callback that is invoked at end of each epoch.
<             This can be used to checkpoint model each epoch.
<         batch_end_callback: callable(epoch)
<             A callback that is invoked at end of each batch for print purposes.
<         kvstore: KVStore or str, optional
<            The KVStore or a string kvstore type: 'local', 'dist_sync', 'dis_async'.
<            Defaults to 'local', often no need to change for single machine.
<         logger : logging logger, optional
<             When not specified, default logger will be used.
<         work_load_list : list of float or int, optional
<             The list of work load for different devices,
<             in the same order as `ctx`.
<         """
<         model = FeedForward(symbol, ctx=ctx, num_epoch=num_epoch,
<                             epoch_size=epoch_size,
<                             optimizer=optimizer, initializer=initializer, **kwargs)
<         model.fit(X, y, eval_data=eval_data, eval_metric=eval_metric,
<                   epoch_end_callback=epoch_end_callback,
<                   batch_end_callback=batch_end_callback,
<                   kvstore=kvstore,
<                   logger=logger,
<                   work_load_list=work_load_list,
<                   eval_end_callback=eval_end_callback,
<                   eval_batch_end_callback=eval_batch_end_callback)
<         return model
---
> # Licensed to the Apache Software Foundation (ASF) under one
> # or more contributor license agreements.  See the NOTICE file
> # distributed with this work for additional information
> # regarding copyright ownership.  The ASF licenses this file
> # to you under the Apache License, Version 2.0 (the
> # "License"); you may not use this file except in compliance
> # with the License.  You may obtain a copy of the License at
> #
> #   http://www.apache.org/licenses/LICENSE-2.0
> #
> # Unless required by applicable law or agreed to in writing,
> # software distributed under the License is distributed on an
> # "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
> # KIND, either express or implied.  See the License for the
> # specific language governing permissions and limitations
> # under the License.
> 
> # pylint: disable=fixme, invalid-name, too-many-arguments, too-many-locals, too-many-lines
> # pylint: disable=too-many-branches, too-many-statements
> """MXNet model module"""
> from __future__ import absolute_import, print_function
> 
> import os
> import time
> import logging
> import warnings
> from collections import namedtuple
> import numpy as np
> 
> from . import io
> from . import ndarray as nd
> from . import symbol as sym
> from . import optimizer as opt
> from . import metric
> from . import kvstore as kvs
> from .context import Context, cpu
> from .initializer import Uniform
> from .optimizer import get_updater
> from .executor_manager import DataParallelExecutorManager, _check_arguments, _load_data
> from .io import DataDesc
> from .base import mx_real_t
> 
> BASE_ESTIMATOR = object
> 
> try:
>     from sklearn.base import BaseEstimator
>     BASE_ESTIMATOR = BaseEstimator
> except ImportError:
>     SKLEARN_INSTALLED = False
> 
> # Parameter to pass to batch_end_callback
> BatchEndParam = namedtuple('BatchEndParams',
>                            ['epoch',
>                             'nbatch',
>                             'eval_metric',
>                             'locals'])
> 
> def _create_sparse_kvstore(kvstore):
>     """Create kvstore assuming some parameters' storage types are row_sparse.
> 
>     Parameters
>     ----------
>     kvstore : KVStore or str
>         The kvstore.
> 
>     Returns
>     -------
>     kvstore : KVStore
>     update_on_kvstore : bool. Always True.
>     """
>     # always update on kvstore
>     update_on_kvstore = True
>     if isinstance(kvstore, kvs.KVStore):
>         kv = kvstore
>     elif isinstance(kvstore, str):
>         kv = kvs.create(kvstore)
>     else:
>         raise TypeError("Cannot create '%s' KVStore with row_sparse parameters. "
>                         "The type must be KVStore or str." % kvstore)
>     return (kv, update_on_kvstore)
> 
> def _create_kvstore(kvstore, num_device, arg_params):
>     """Create kvstore
>     This function select and create a proper kvstore if given the kvstore type.
> 
>     Parameters
>     ----------
>     kvstore : KVStore or str
>         The kvstore.
>     num_device : int
>         The number of devices
>     arg_params : dict of str to `NDArray`.
>         Model parameter, dict of name to `NDArray` of net's weights.
>     """
>     update_on_kvstore = bool(int(os.getenv('MXNET_UPDATE_ON_KVSTORE', "1")))
>     if kvstore is None:
>         kv = None
>     elif isinstance(kvstore, kvs.KVStore):
>         kv = kvstore
>     elif isinstance(kvstore, str):
>         # create kvstore using the string type
>         if num_device == 1 and 'dist' not in kvstore:
>             # no need to use kv for single device and single machine
>             kv = None
>         else:
>             kv = kvs.create(kvstore)
>             if kvstore == 'local':
>             # automatically select a proper local
>                 max_size = max(np.prod(param.shape) for param in
>                                arg_params.values())
>                 if max_size > 1024 * 1024 * 16:
>                     update_on_kvstore = False
>     else:
>         raise TypeError('kvstore must be KVStore, str or None')
> 
>     if kv is None:
>         update_on_kvstore = False
> 
>     return (kv, update_on_kvstore)
> 
> def _initialize_kvstore(kvstore, param_arrays, arg_params, param_names, update_on_kvstore):
>     """Initialize kvstore"""
>     for idx, param_on_devs in enumerate(param_arrays):
>         name = param_names[idx]
>         kvstore.init(name, arg_params[name])
> 
>         if update_on_kvstore:
>             kvstore.pull(name, param_on_devs, priority=-idx)
> 
> def _update_params_on_kvstore_nccl(param_arrays, grad_arrays, kvstore, param_names):
>     """Perform update of param_arrays from grad_arrays on NCCL kvstore."""
>     valid_indices = [index for index, grad_list in
>                      enumerate(grad_arrays) if grad_list[0] is not None]
>     valid_grad_arrays = [grad_arrays[i] for i in valid_indices]
>     valid_param_arrays = [param_arrays[i] for i in valid_indices]
>     valid_param_names = [param_names[i] for i in valid_indices]
>     size = len(valid_grad_arrays)
>     start = 0
>     # Use aggregation by default only with NCCL
>     default_batch = '16'
>     batch = int(os.getenv('MXNET_UPDATE_AGGREGATION_SIZE', default_batch))
>     while start < size:
>         end = start + batch if start + batch < size else size
>         # push gradient, priority is negative index
>         kvstore.push(valid_param_names[start:end], valid_grad_arrays[start:end], priority=-start)
>         # pull back the weights
>         kvstore.pull(valid_param_names[start:end], valid_param_arrays[start:end], priority=-start)
>         start = end
> 
> def _update_params_on_kvstore(param_arrays, grad_arrays, kvstore, param_names):
>     """Perform update of param_arrays from grad_arrays on kvstore."""
>     for index, pair in enumerate(zip(param_arrays, grad_arrays)):
>         arg_list, grad_list = pair
>         if grad_list[0] is None:
>             continue
>         name = param_names[index]
>         # push gradient, priority is negative index
>         kvstore.push(name, grad_list, priority=-index)
>         # pull back the weights
>         kvstore.pull(name, arg_list, priority=-index)
> 
> def _update_params(param_arrays, grad_arrays, updater, num_device,
>                    kvstore=None, param_names=None):
>     """Perform update of param_arrays from grad_arrays not on kvstore."""
>     updates = [[] for _ in range(num_device)]
>     for i, pair in enumerate(zip(param_arrays, grad_arrays)):
>         arg_list, grad_list = pair
>         if grad_list[0] is None:
>             continue
>         index = i
>         if kvstore:
>             name = param_names[index]
>             # push gradient, priority is negative index
>             kvstore.push(name, grad_list, priority=-index)
>             # pull back the sum gradients, to the same locations.
>             kvstore.pull(name, grad_list, priority=-index)
>         for k, p in enumerate(zip(arg_list, grad_list)):
>             # faked an index here, to make optimizer create diff
>             # state for the same index but on diff devs, TODO(mli)
>             # use a better solution later
>             w, g = p
>             updates[k].append((index*num_device+k, g, w))
>     for dev_updates in updates:
>         # update params if param_arrays and grad_arrays are not empty
>         if dev_updates:
>             i, w, g = zip(*dev_updates)
>             updater(i, w, g)
> 
> 
> def _multiple_callbacks(callbacks, *args, **kwargs):
>     """Sends args and kwargs to any configured callbacks.
>     This handles the cases where the 'callbacks' variable
>     is ``None``, a single function, or a list.
>     """
>     if isinstance(callbacks, list):
>         for cb in callbacks:
>             cb(*args, **kwargs)
>         return
>     if callbacks:
>         callbacks(*args, **kwargs)
> 
> 
> def _train_multi_device(symbol, ctx, arg_names, param_names, aux_names,
>                         arg_params, aux_params,
>                         begin_epoch, end_epoch, epoch_size, optimizer,
>                         kvstore, update_on_kvstore,
>                         train_data, eval_data=None, eval_metric=None,
>                         epoch_end_callback=None, batch_end_callback=None,
>                         logger=None, work_load_list=None, monitor=None,
>                         eval_end_callback=None,
>                         eval_batch_end_callback=None, sym_gen=None):
>     """Internal training function on multiple devices.
>     This function will also work for single device as well.
> 
>     Parameters
>     ----------
>     symbol : Symbol
>         The network configuration.
>     ctx : list of Context
>         The training devices.
>     arg_names: list of str
>         Name of all arguments of the network.
>     param_names: list of str
>         Name of all trainable parameters of the network.
>     aux_names: list of str
>         Name of all auxiliary states of the network.
>     arg_params : dict of str to NDArray
>         Model parameter, dict of name to NDArray of net's weights.
>     aux_params : dict of str to NDArray
>         Model parameter, dict of name to NDArray of net's auxiliary states.
>     begin_epoch : int
>         The begining training epoch.
>     end_epoch : int
>         The end training epoch.
>     epoch_size : int, optional
>         Number of batches in a epoch. In default, it is set to
>         ``ceil(num_train_examples / batch_size)``.
>     optimizer : Optimizer
>         The optimization algorithm
>     train_data : DataIter
>         Training data iterator.
>     eval_data : DataIter
>         Validation data iterator.
>     eval_metric : EvalMetric
>         An evaluation function or a list of evaluation functions.
>     epoch_end_callback : callable(epoch, symbol, arg_params, aux_states)
>         A callback that is invoked at end of each epoch.
>         This can be used to checkpoint model each epoch.
>     batch_end_callback : callable(BatchEndParams)
>         A callback that is invoked at end of each batch.
>         This can be used to measure speed, get result from evaluation metric. etc.
>     kvstore : KVStore
>         The KVStore.
>     update_on_kvstore : bool
>         Whether or not perform weight updating on kvstore.
>     logger : logging logger
>         When not specified, default logger will be used.
>     work_load_list : list of float or int, optional
>         The list of work load for different devices,
>         in the same order as ``ctx``.
>     monitor : Monitor, optional
>         Monitor installed to executor,
>         for monitoring outputs, weights, and gradients for debugging.
>     Notes
>     -----
>     - This function will inplace update the NDArrays in `arg_params` and `aux_states`.
>     """
>     if logger is None:
>         logger = logging
>     executor_manager = DataParallelExecutorManager(symbol=symbol,
>                                                    sym_gen=sym_gen,
>                                                    ctx=ctx,
>                                                    train_data=train_data,
>                                                    param_names=param_names,
>                                                    arg_names=arg_names,
>                                                    aux_names=aux_names,
>                                                    work_load_list=work_load_list,
>                                                    logger=logger)
>     if monitor:
>         executor_manager.install_monitor(monitor)
> 
>     executor_manager.set_params(arg_params, aux_params)
> 
>     if not update_on_kvstore:
>         updater = get_updater(optimizer)
>     else:
>         kvstore.set_optimizer(optimizer)
> 
>     if kvstore:
>         _initialize_kvstore(kvstore=kvstore,
>                             param_arrays=executor_manager.param_arrays,
>                             arg_params=arg_params,
>                             param_names=executor_manager.param_names,
>                             update_on_kvstore=update_on_kvstore)
> 
>     # Now start training
>     train_data.reset()
>     for epoch in range(begin_epoch, end_epoch):
>         # Training phase
>         tic = time.time()
>         eval_metric.reset()
>         nbatch = 0
>         # Iterate over training data.
>         while True:
>             do_reset = True
>             for data_batch in train_data:
>                 executor_manager.load_data_batch(data_batch)
> 
>                 if monitor is not None:
>                     monitor.tic()
> 
>                 executor_manager.forward(is_train=True)
>                 executor_manager.backward()
> 
>                 if update_on_kvstore:
>                     if 'nccl' in kvstore.type:
>                         _update_params_on_kvstore_nccl(executor_manager.param_arrays,
>                                                        executor_manager.grad_arrays,
>                                                        kvstore, executor_manager.param_names)
>                     else:
>                         _update_params_on_kvstore(executor_manager.param_arrays,
>                                                   executor_manager.grad_arrays,
>                                                   kvstore, executor_manager.param_names)
>                 else:
>                     _update_params(executor_manager.param_arrays,
>                                    executor_manager.grad_arrays,
>                                    updater=updater,
>                                    num_device=len(ctx),
>                                    kvstore=kvstore,
>                                    param_names=executor_manager.param_names)
> 
>                 if monitor is not None:
>                     monitor.toc_print()
> 
>                 # evaluate at end, so we can lazy copy
>                 executor_manager.update_metric(eval_metric, data_batch.label)
> 
>                 nbatch += 1
>                 # batch callback (for print purpose)
>                 if batch_end_callback is not None:
>                     batch_end_params = BatchEndParam(epoch=epoch,
>                                                      nbatch=nbatch,
>                                                      eval_metric=eval_metric,
>                                                      locals=locals())
>                     _multiple_callbacks(batch_end_callback, batch_end_params)
> 
>                 # this epoch is done possibly earlier
>                 if epoch_size is not None and nbatch >= epoch_size:
>                     do_reset = False
>                     break
> 
>             if do_reset:
>                 logger.info('Epoch[%d] Resetting Data Iterator', epoch)
>                 train_data.reset()
> 
>             # this epoch is done
>             if epoch_size is None or nbatch >= epoch_size:
>                 break
> 
>         toc = time.time()
>         logger.info('Epoch[%d] Time cost=%.3f', epoch, (toc - tic))
> 
>         if epoch_end_callback or epoch + 1 == end_epoch:
>             executor_manager.copy_to(arg_params, aux_params)
> 
>         _multiple_callbacks(epoch_end_callback, epoch, symbol, arg_params, aux_params)
> 
>         # evaluation
>         if eval_data:
>             eval_metric.reset()
>             eval_data.reset()
>             total_num_batch = 0
>             for i, eval_batch in enumerate(eval_data):
>                 executor_manager.load_data_batch(eval_batch)
>                 executor_manager.forward(is_train=False)
>                 executor_manager.update_metric(eval_metric, eval_batch.label)
>                 if eval_batch_end_callback is not None:
>                     batch_end_params = BatchEndParam(epoch=epoch,
>                                                      nbatch=i,
>                                                      eval_metric=eval_metric,
>                                                      locals=locals())
>                     _multiple_callbacks(eval_batch_end_callback, batch_end_params)
>                 total_num_batch += 1
>             if eval_end_callback is not None:
>                 eval_end_params = BatchEndParam(epoch=epoch,
>                                                 nbatch=total_num_batch,
>                                                 eval_metric=eval_metric,
>                                                 locals=locals())
>                 _multiple_callbacks(eval_end_callback, eval_end_params)
>             eval_data.reset()
>     # end of all epochs
> 
> 
> def save_checkpoint(prefix, epoch, symbol, arg_params, aux_params, remove_amp_cast=True):
>     """Checkpoint the model data into file.
> 
>     Parameters
>     ----------
>     prefix : str
>         Prefix of model name.
>     epoch : int
>         The epoch number of the model.
>     symbol : Symbol
>         The input Symbol.
>     arg_params : dict of str to NDArray
>         Model parameter, dict of name to NDArray of net's weights.
>     aux_params : dict of str to NDArray
>         Model parameter, dict of name to NDArray of net's auxiliary states.
>     remove_amp_cast : bool, optional
>         Whether to remove the amp_cast and amp_multicast operators, before saving the model.
>     Notes
>     -----
>     - ``prefix-symbol.json`` will be saved for symbol.
>     - ``prefix-epoch.params`` will be saved for parameters.
>     """
>     if symbol is not None:
>         symbol.save('%s-symbol.json' % prefix, remove_amp_cast=remove_amp_cast)
> 
>     save_dict = {('arg:%s' % k) : v.as_in_context(cpu()) for k, v in arg_params.items()}
>     save_dict.update({('aux:%s' % k) : v.as_in_context(cpu()) for k, v in aux_params.items()})
>     param_name = '%s-%04d.params' % (prefix, epoch)
>     nd.save(param_name, save_dict)
>     logging.info('Saved checkpoint to \"%s\"', param_name)
> 
> 
> def load_params(prefix, epoch):
>     """Load params from a file
>     """
>     save_dict = nd.load("%s-%04d.params" % (prefix, epoch))
>     arg_params = {}
>     aux_params = {}
>     if not save_dict:
>         logging.warning("Params file '%s' is empty", '%s-%04d.params' % (prefix, epoch))
>     for k, v in save_dict.items():
>         tp, name = k.split(":", 1)
>         if tp == "arg":
>             arg_params[name] = v
>         if tp == "aux":
>             aux_params[name] = v
>     return (arg_params, aux_params)
> 
> def load_checkpoint(prefix, epoch):
>     """Load model checkpoint from file.
> 
>     Parameters
>     ----------
>     prefix : str
>         Prefix of model name.
>     epoch : int
>         Epoch number of model we would like to load.
> 
>     Returns
>     -------
>     symbol : Symbol
>         The symbol configuration of computation network.
>     arg_params : dict of str to NDArray
>         Model parameter, dict of name to NDArray of net's weights.
>     aux_params : dict of str to NDArray
>         Model parameter, dict of name to NDArray of net's auxiliary states.
> 
>     Notes
>     -----
>     - Symbol will be loaded from ``prefix-symbol.json``.
>     - Parameters will be loaded from ``prefix-epoch.params``.
>     """
>     symbol = sym.load('%s-symbol.json' % prefix)
>     arg_params, aux_params = load_params(prefix, epoch)
>     return (symbol, arg_params, aux_params)
> 
> from .callback import LogValidationMetricsCallback # pylint: disable=wrong-import-position
> 
> class FeedForward(BASE_ESTIMATOR):
>     """Model class of MXNet for training and predicting feedforward nets.
>     This class is designed for a single-data single output supervised network.
> 
>     Parameters
>     ----------
>     symbol : Symbol
>         The symbol configuration of computation network.
>     ctx : Context or list of Context, optional
>         The device context of training and prediction.
>         To use multi GPU training, pass in a list of gpu contexts.
>     num_epoch : int, optional
>         Training parameter, number of training epochs(epochs).
>     epoch_size : int, optional
>         Number of batches in a epoch. In default, it is set to
>         ``ceil(num_train_examples / batch_size)``.
>     optimizer : str or Optimizer, optional
>         Training parameter, name or optimizer object for training.
>     initializer : initializer function, optional
>         Training parameter, the initialization scheme used.
>     numpy_batch_size : int, optional
>         The batch size of training data.
>         Only needed when input array is numpy.
>     arg_params : dict of str to NDArray, optional
>         Model parameter, dict of name to NDArray of net's weights.
>     aux_params : dict of str to NDArray, optional
>         Model parameter, dict of name to NDArray of net's auxiliary states.
>     allow_extra_params : boolean, optional
>         Whether allow extra parameters that are not needed by symbol
>         to be passed by aux_params and ``arg_params``.
>         If this is True, no error will be thrown when ``aux_params`` and ``arg_params``
>         contain more parameters than needed.
>     begin_epoch : int, optional
>         The begining training epoch.
>     kwargs : dict
>         The additional keyword arguments passed to optimizer.
>     """
>     def __init__(self, symbol, ctx=None,
>                  num_epoch=None, epoch_size=None, optimizer='sgd',
>                  initializer=Uniform(0.01),
>                  numpy_batch_size=128,
>                  arg_params=None, aux_params=None,
>                  allow_extra_params=False,
>                  begin_epoch=0,
>                  **kwargs):
>         warnings.warn(
>             '\033[91mmxnet.model.FeedForward has been deprecated. ' + \
>             'Please use mxnet.mod.Module instead.\033[0m',
>             DeprecationWarning, stacklevel=2)
> 
>         if isinstance(symbol, sym.Symbol):
>             self.symbol = symbol
>             self.sym_gen = None
>         else:
>             assert(callable(symbol))
>             self.symbol = None
>             self.sym_gen = symbol
> 
>         # model parameters
>         self.arg_params = arg_params
>         self.aux_params = aux_params
>         self.allow_extra_params = allow_extra_params
> 
>         self.argument_checked = False
>         if self.sym_gen is None:
>             self._check_arguments()
> 
>         # basic configuration
>         if ctx is None:
>             ctx = [cpu()]
>         elif isinstance(ctx, Context):
>             ctx = [ctx]
>         self.ctx = ctx
>         # training parameters
>         self.num_epoch = num_epoch
>         self.epoch_size = epoch_size
>         self.kwargs = kwargs.copy()
>         self.optimizer = optimizer
>         self.initializer = initializer
>         self.numpy_batch_size = numpy_batch_size
>         # internal helper state
>         self._pred_exec = None
>         self.begin_epoch = begin_epoch
> 
>     def _check_arguments(self):
>         """verify the argument of the default symbol and user provided parameters"""
>         if self.argument_checked:
>             return
> 
>         assert(self.symbol is not None)
>         self.argument_checked = True
> 
>         # check if symbol contain duplicated names.
>         _check_arguments(self.symbol)
>         # rematch parameters to delete useless ones
>         if self.allow_extra_params:
>             if self.arg_params:
>                 arg_names = set(self.symbol.list_arguments())
>                 self.arg_params = {k : v for k, v in self.arg_params.items()
>                                    if k in arg_names}
>             if self.aux_params:
>                 aux_names = set(self.symbol.list_auxiliary_states())
>                 self.aux_params = {k : v for k, v in self.aux_params.items()
>                                    if k in aux_names}
> 
> 
>     @staticmethod
>     def _is_data_arg(name):
>         """Check if name is a data argument."""
>         return name.endswith('data') or name.endswith('label')
> 
>     def _init_params(self, inputs, overwrite=False):
>         """Initialize weight parameters and auxiliary states."""
>         inputs = [x if isinstance(x, DataDesc) else DataDesc(*x) for x in inputs]
>         input_shapes = {item.name: item.shape for item in inputs}
>         arg_shapes, _, aux_shapes = self.symbol.infer_shape(**input_shapes)
>         assert arg_shapes is not None
>         input_dtypes = {item.name: item.dtype for item in inputs}
>         arg_dtypes, _, aux_dtypes = self.symbol.infer_type(**input_dtypes)
>         assert arg_dtypes is not None
> 
>         arg_names = self.symbol.list_arguments()
>         input_names = input_shapes.keys()
>         param_names = [key for key in arg_names if key not in input_names]
>         aux_names = self.symbol.list_auxiliary_states()
> 
>         param_name_attrs = [x for x in zip(arg_names, arg_shapes, arg_dtypes)
>                             if x[0] in param_names]
>         arg_params = {k : nd.zeros(shape=s, dtype=t)
>                       for k, s, t in param_name_attrs}
>         aux_name_attrs = [x for x in zip(aux_names, aux_shapes, aux_dtypes)
>                           if x[0] in aux_names]
>         aux_params = {k : nd.zeros(shape=s, dtype=t)
>                       for k, s, t in aux_name_attrs}
> 
>         for k, v in arg_params.items():
>             if self.arg_params and k in self.arg_params and (not overwrite):
>                 arg_params[k][:] = self.arg_params[k][:]
>             else:
>                 self.initializer(k, v)
> 
>         for k, v in aux_params.items():
>             if self.aux_params and k in self.aux_params and (not overwrite):
>                 aux_params[k][:] = self.aux_params[k][:]
>             else:
>                 self.initializer(k, v)
> 
>         self.arg_params = arg_params
>         self.aux_params = aux_params
>         return (arg_names, list(param_names), aux_names)
> 
>     def __getstate__(self):
>         this = self.__dict__.copy()
>         this['_pred_exec'] = None
>         return this
> 
>     def __setstate__(self, state):
>         self.__dict__.update(state)
> 
>     def _init_predictor(self, input_shapes, type_dict=None):
>         """Initialize the predictor module for running prediction."""
>         shapes = {name: self.arg_params[name].shape for name in self.arg_params}
>         shapes.update(dict(input_shapes))
>         if self._pred_exec is not None:
>             arg_shapes, _, _ = self.symbol.infer_shape(**shapes)
>             assert arg_shapes is not None, "Incomplete input shapes"
>             pred_shapes = [x.shape for x in self._pred_exec.arg_arrays]
>             if arg_shapes == pred_shapes:
>                 return
>         # for now only use the first device
>         pred_exec = self.symbol.simple_bind(
>             self.ctx[0], grad_req='null', type_dict=type_dict, **shapes)
>         pred_exec.copy_params_from(self.arg_params, self.aux_params)
> 
>         _check_arguments(self.symbol)
>         self._pred_exec = pred_exec
> 
>     def _init_iter(self, X, y, is_train):
>         """Initialize the iterator given input."""
>         if isinstance(X, (np.ndarray, nd.NDArray)):
>             if y is None:
>                 if is_train:
>                     raise ValueError('y must be specified when X is numpy.ndarray')
>                 y = np.zeros(X.shape[0])
>             if not isinstance(y, (np.ndarray, nd.NDArray)):
>                 raise TypeError('y must be ndarray when X is numpy.ndarray')
>             if X.shape[0] != y.shape[0]:
>                 raise ValueError("The numbers of data points and labels not equal")
>             if y.ndim == 2 and y.shape[1] == 1:
>                 y = y.flatten()
>             if y.ndim != 1:
>                 raise ValueError("Label must be 1D or 2D (with 2nd dimension being 1)")
>             if is_train:
>                 return io.NDArrayIter(X, y, min(X.shape[0], self.numpy_batch_size),
>                                       shuffle=is_train, last_batch_handle='roll_over')
>             else:
>                 return io.NDArrayIter(X, y, min(X.shape[0], self.numpy_batch_size), shuffle=False)
>         if not isinstance(X, io.DataIter):
>             raise TypeError('X must be DataIter, NDArray or numpy.ndarray')
>         return X
> 
>     def _init_eval_iter(self, eval_data):
>         """Initialize the iterator given eval_data."""
>         if eval_data is None:
>             return eval_data
>         if isinstance(eval_data, (tuple, list)) and len(eval_data) == 2:
>             if eval_data[0] is not None:
>                 if eval_data[1] is None and isinstance(eval_data[0], io.DataIter):
>                     return eval_data[0]
>                 input_data = (np.array(eval_data[0]) if isinstance(eval_data[0], list)
>                               else eval_data[0])
>                 input_label = (np.array(eval_data[1]) if isinstance(eval_data[1], list)
>                                else eval_data[1])
>                 return self._init_iter(input_data, input_label, is_train=True)
>             else:
>                 raise ValueError("Eval data is NONE")
>         if not isinstance(eval_data, io.DataIter):
>             raise TypeError('Eval data must be DataIter, or ' \
>                             'NDArray/numpy.ndarray/list pair (i.e. tuple/list of length 2)')
>         return eval_data
> 
>     def predict(self, X, num_batch=None, return_data=False, reset=True):
>         """Run the prediction, always only use one device.
> 
>         Parameters
>         ----------
>         X : mxnet.DataIter
>         num_batch : int or None
>             The number of batch to run. Go though all batches if ``None``.
>         Returns
>         -------
>         y : numpy.ndarray or a list of numpy.ndarray if the network has multiple outputs.
>             The predicted value of the output.
>         """
>         X = self._init_iter(X, None, is_train=False)
> 
>         if reset:
>             X.reset()
>         data_shapes = X.provide_data
>         data_names = [x[0] for x in data_shapes]
>         type_dict = dict((key, value.dtype) for (key, value) in self.arg_params.items())
>         for x in X.provide_data:
>             if isinstance(x, DataDesc):
>                 type_dict[x.name] = x.dtype
>             else:
>                 type_dict[x[0]] = mx_real_t
> 
>         self._init_predictor(data_shapes, type_dict)
>         batch_size = X.batch_size
>         data_arrays = [self._pred_exec.arg_dict[name] for name in data_names]
>         output_list = [[] for _ in range(len(self._pred_exec.outputs))]
>         if return_data:
>             data_list = [[] for _ in X.provide_data]
>             label_list = [[] for _ in X.provide_label]
> 
>         i = 0
>         for batch in X:
> 
>             _load_data(batch, data_arrays)
>             self._pred_exec.forward(is_train=False)
>             padded = batch.pad
>             real_size = batch_size - padded
> 
>             for o_list, o_nd in zip(output_list, self._pred_exec.outputs):
>                 o_list.append(o_nd[0:real_size].asnumpy())
> 
>             if return_data:
>                 for j, x in enumerate(batch.data):
>                     data_list[j].append(x[0:real_size].asnumpy())
>                 for j, x in enumerate(batch.label):
>                     label_list[j].append(x[0:real_size].asnumpy())
>             i += 1
>             if num_batch is not None and i == num_batch:
>                 break
> 
>         outputs = [np.concatenate(x) for x in output_list]
>         if len(outputs) == 1:
>             outputs = outputs[0]
> 
>         if return_data:
>             data = [np.concatenate(x) for x in data_list]
>             label = [np.concatenate(x) for x in label_list]
>             if len(data) == 1:
>                 data = data[0]
>             if len(label) == 1:
>                 label = label[0]
>             return outputs, data, label
>         else:
>             return outputs
> 
>     def score(self, X, eval_metric='acc', num_batch=None, batch_end_callback=None, reset=True):
>         """Run the model given an input and calculate the score
>         as assessed by an evaluation metric.
> 
>         Parameters
>         ----------
>         X : mxnet.DataIter
>         eval_metric : metric.metric
>             The metric for calculating score.
>         num_batch : int or None
>             The number of batches to run. Go though all batches if ``None``.
>         Returns
>         -------
>         s : float
>             The final score.
>         """
>         # setup metric
>         if not isinstance(eval_metric, metric.EvalMetric):
>             eval_metric = metric.create(eval_metric)
> 
>         X = self._init_iter(X, None, is_train=False)
>         if reset:
>             X.reset()
> 
>         data_shapes = X.provide_data
>         data_names = [x[0] for x in data_shapes]
>         type_dict = dict((key, value.dtype) for (key, value) in self.arg_params.items())
>         for x in X.provide_data:
>             if isinstance(x, DataDesc):
>                 type_dict[x.name] = x.dtype
>             else:
>                 type_dict[x[0]] = mx_real_t
> 
>         self._init_predictor(data_shapes, type_dict)
>         data_arrays = [self._pred_exec.arg_dict[name] for name in data_names]
> 
>         for i, batch in enumerate(X):
>             if num_batch is not None and i == num_batch:
>                 break
>             _load_data(batch, data_arrays)
>             self._pred_exec.forward(is_train=False)
>             eval_metric.update(batch.label, self._pred_exec.outputs)
> 
>             if batch_end_callback is not None:
>                 batch_end_params = BatchEndParam(epoch=0,
>                                                  nbatch=i,
>                                                  eval_metric=eval_metric,
>                                                  locals=locals())
>                 _multiple_callbacks(batch_end_callback, batch_end_params)
>         return eval_metric.get()[1]
> 
>     def fit(self, X, y=None, eval_data=None, eval_metric='acc',
>             epoch_end_callback=None, batch_end_callback=None, kvstore='local', logger=None,
>             work_load_list=None, monitor=None, eval_end_callback=LogValidationMetricsCallback(),
>             eval_batch_end_callback=None):
>         """Fit the model.
> 
>         Parameters
>         ----------
>         X : DataIter, or numpy.ndarray/NDArray
>             Training data. If `X` is a `DataIter`, the name or (if name not available)
>             the position of its outputs should match the corresponding variable
>             names defined in the symbolic graph.
>         y : numpy.ndarray/NDArray, optional
>             Training set label.
>             If X is ``numpy.ndarray`` or `NDArray`, `y` is required to be set.
>             While y can be 1D or 2D (with 2nd dimension as 1), its first dimension must be
>             the same as `X`, i.e. the number of data points and labels should be equal.
>         eval_data : DataIter or numpy.ndarray/list/NDArray pair
>             If eval_data is numpy.ndarray/list/NDArray pair,
>             it should be ``(valid_data, valid_label)``.
>         eval_metric : metric.EvalMetric or str or callable
>             The evaluation metric. This could be the name of evaluation metric
>             or a custom evaluation function that returns statistics
>             based on a minibatch.
>         epoch_end_callback : callable(epoch, symbol, arg_params, aux_states)
>             A callback that is invoked at end of each epoch.
>             This can be used to checkpoint model each epoch.
>         batch_end_callback: callable(epoch)
>             A callback that is invoked at end of each batch for purposes of printing.
>         kvstore: KVStore or str, optional
>            The KVStore or a string kvstore type: 'local', 'dist_sync', 'dist_async'
>            In default uses 'local', often no need to change for single machiine.
>         logger : logging logger, optional
>             When not specified, default logger will be used.
>         work_load_list : float or int, optional
>             The list of work load for different devices,
>             in the same order as `ctx`.
> 
>         Note
>         ----
>         KVStore behavior
>         - 'local', multi-devices on a single machine, will automatically choose best type.
>         - 'dist_sync', multiple machines communicating via BSP.
>         - 'dist_async', multiple machines with asynchronous communication.
>         """
> 
>         data = self._init_iter(X, y, is_train=True)
>         eval_data = self._init_eval_iter(eval_data)
> 
>         if self.sym_gen:
>             self.symbol = self.sym_gen(data.default_bucket_key) # pylint: disable=no-member
>             self._check_arguments()
>         self.kwargs["sym"] = self.symbol
> 
>         arg_names, param_names, aux_names = \
>                 self._init_params(data.provide_data+data.provide_label)
> 
>         # setup metric
>         if not isinstance(eval_metric, metric.EvalMetric):
>             eval_metric = metric.create(eval_metric)
> 
>         # create kvstore
>         (kvstore, update_on_kvstore) = _create_kvstore(
>             kvstore, len(self.ctx), self.arg_params)
> 
>         param_idx2name = {}
>         if update_on_kvstore:
>             param_idx2name.update(enumerate(param_names))
>         else:
>             for i, n in enumerate(param_names):
>                 for k in range(len(self.ctx)):
>                     param_idx2name[i*len(self.ctx)+k] = n
>         self.kwargs["param_idx2name"] = param_idx2name
> 
>         # init optmizer
>         if isinstance(self.optimizer, str):
>             batch_size = data.batch_size
>             if kvstore and 'dist' in kvstore.type and '_async' not in kvstore.type:
>                 batch_size *= kvstore.num_workers
>             optimizer = opt.create(self.optimizer,
>                                    rescale_grad=(1.0/batch_size),
>                                    **(self.kwargs))
>         elif isinstance(self.optimizer, opt.Optimizer):
>             if not optimizer.idx2name:
>                 optimizer.idx2name = param_idx2name.copy()
>             optimizer = self.optimizer
> 
>         # do training
>         _train_multi_device(self.symbol, self.ctx, arg_names, param_names, aux_names,
>                             self.arg_params, self.aux_params,
>                             begin_epoch=self.begin_epoch, end_epoch=self.num_epoch,
>                             epoch_size=self.epoch_size,
>                             optimizer=optimizer,
>                             train_data=data, eval_data=eval_data,
>                             eval_metric=eval_metric,
>                             epoch_end_callback=epoch_end_callback,
>                             batch_end_callback=batch_end_callback,
>                             kvstore=kvstore, update_on_kvstore=update_on_kvstore,
>                             logger=logger, work_load_list=work_load_list, monitor=monitor,
>                             eval_end_callback=eval_end_callback,
>                             eval_batch_end_callback=eval_batch_end_callback,
>                             sym_gen=self.sym_gen)
> 
> 
>     def save(self, prefix, epoch=None, remove_amp_cast=True):
>         """Checkpoint the model checkpoint into file.
>         You can also use `pickle` to do the job if you only work on Python.
>         The advantage of `load` and `save` (as compared to `pickle`) is that
>         the resulting file can be loaded from other MXNet language bindings.
>         One can also directly `load`/`save` from/to cloud storage(S3, HDFS)
> 
>         Parameters
>         ----------
>         prefix : str
>             Prefix of model name.
>         remove_amp_cast : bool, optional
>             Whether to remove the amp_cast and amp_multicast operators, before saving the model.
> 
>         Notes
>         -----
>         - ``prefix-symbol.json`` will be saved for symbol.
>         - ``prefix-epoch.params`` will be saved for parameters.
>         """
>         if epoch is None:
>             epoch = self.num_epoch
>         assert epoch is not None
>         save_checkpoint(prefix, epoch, self.symbol, self.arg_params, self.aux_params, remove_amp_cast=remove_amp_cast)
> 
>     @staticmethod
>     def load(prefix, epoch, ctx=None, **kwargs):
>         """Load model checkpoint from file.
> 
>         Parameters
>         ----------
>         prefix : str
>             Prefix of model name.
>         epoch : int
>             epoch number of model we would like to load.
>         ctx : Context or list of Context, optional
>             The device context of training and prediction.
>         kwargs : dict
>             Other parameters for model, including `num_epoch`, optimizer and `numpy_batch_size`.
> 
>         Returns
>         -------
>         model : FeedForward
>             The loaded model that can be used for prediction.
> 
>         Notes
>         -----
>         - ``prefix-symbol.json`` will be saved for symbol.
>         - ``prefix-epoch.params`` will be saved for parameters.
>         """
>         symbol, arg_params, aux_params = load_checkpoint(prefix, epoch)
>         return FeedForward(symbol, ctx=ctx,
>                            arg_params=arg_params, aux_params=aux_params,
>                            begin_epoch=epoch,
>                            **kwargs)
> 
>     @staticmethod
>     def create(symbol, X, y=None, ctx=None,
>                num_epoch=None, epoch_size=None, optimizer='sgd', initializer=Uniform(0.01),
>                eval_data=None, eval_metric='acc',
>                epoch_end_callback=None, batch_end_callback=None,
>                kvstore='local', logger=None, work_load_list=None,
>                eval_end_callback=LogValidationMetricsCallback(),
>                eval_batch_end_callback=None, **kwargs):
>         """Functional style to create a model.
>         This function is more consistent with functional
>         languages such as R, where mutation is not allowed.
> 
>         Parameters
>         ----------
>         symbol : Symbol
>             The symbol configuration of a computation network.
>         X : DataIter
>             Training data.
>         y : numpy.ndarray, optional
>             If `X` is a ``numpy.ndarray``, `y` must be set.
>         ctx : Context or list of Context, optional
>             The device context of training and prediction.
>             To use multi-GPU training, pass in a list of GPU contexts.
>         num_epoch : int, optional
>             The number of training epochs(epochs).
>         epoch_size : int, optional
>             Number of batches in a epoch. In default, it is set to
>             ``ceil(num_train_examples / batch_size)``.
>         optimizer : str or Optimizer, optional
>             The name of the chosen optimizer, or an optimizer object, used for training.
>         initializer : initializer function, optional
>             The initialization scheme used.
>         eval_data : DataIter or numpy.ndarray pair
>             If `eval_set` is ``numpy.ndarray`` pair, it should
>             be (`valid_data`, `valid_label`).
>         eval_metric : metric.EvalMetric or str or callable
>             The evaluation metric. Can be the name of an evaluation metric
>             or a custom evaluation function that returns statistics
>             based on a minibatch.
>         epoch_end_callback : callable(epoch, symbol, arg_params, aux_states)
>             A callback that is invoked at end of each epoch.
>             This can be used to checkpoint model each epoch.
>         batch_end_callback: callable(epoch)
>             A callback that is invoked at end of each batch for print purposes.
>         kvstore: KVStore or str, optional
>            The KVStore or a string kvstore type: 'local', 'dist_sync', 'dis_async'.
>            Defaults to 'local', often no need to change for single machine.
>         logger : logging logger, optional
>             When not specified, default logger will be used.
>         work_load_list : list of float or int, optional
>             The list of work load for different devices,
>             in the same order as `ctx`.
>         """
>         model = FeedForward(symbol, ctx=ctx, num_epoch=num_epoch,
>                             epoch_size=epoch_size,
>                             optimizer=optimizer, initializer=initializer, **kwargs)
>         model.fit(X, y, eval_data=eval_data, eval_metric=eval_metric,
>                   epoch_end_callback=epoch_end_callback,
>                   batch_end_callback=batch_end_callback,
>                   kvstore=kvstore,
>                   logger=logger,
>                   work_load_list=work_load_list,
>                   eval_end_callback=eval_end_callback,
>                   eval_batch_end_callback=eval_batch_end_callback)
>         return model
diff -r /home/xxx/diff/python/mxnet/module/module.py /home/xxx/yyy/incubator-mxnet/python/mxnet/module/module.py
1,857c1,876
< # Licensed to the Apache Software Foundation (ASF) under one
< # or more contributor license agreements.  See the NOTICE file
< # distributed with this work for additional information
< # regarding copyright ownership.  The ASF licenses this file
< # to you under the Apache License, Version 2.0 (the
< # "License"); you may not use this file except in compliance
< # with the License.  You may obtain a copy of the License at
< #
< #   http://www.apache.org/licenses/LICENSE-2.0
< #
< # Unless required by applicable law or agreed to in writing,
< # software distributed under the License is distributed on an
< # "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
< # KIND, either express or implied.  See the License for the
< # specific language governing permissions and limitations
< # under the License.
< 
< # pylint: disable=too-many-instance-attributes, too-many-arguments, protected-access, too-many-branches
< # pylint: disable=too-many-public-methods
< """A `Module` implement the `BaseModule` API by wrapping a `Symbol` and one or
< more `Executor` for data parallelization.
< """
< 
< import logging
< import warnings
< 
< from .. import context as ctx
< from .. import optimizer as opt
< from .. import ndarray as nd
< 
< from .executor_group import DataParallelExecutorGroup
< from ..model import _create_kvstore, _initialize_kvstore, _update_params, _update_params_on_kvstore, _init_shnd
< from ..model import load_checkpoint
< from ..initializer import Uniform, InitDesc
< from ..io import DataDesc
< from ..ndarray import zeros
< 
< from .base_module import BaseModule, _check_input_names, _parse_data_desc
< 
< class Module(BaseModule):
<     """Module is a basic module that wrap a `Symbol`. It is functionally the same
<     as the `FeedForward` model, except under the module API.
< 
<     Parameters
<     ----------
<     symbol : Symbol
<     data_names : list of str
<         Defaults to `('data')` for a typical model used in image classification.
<     label_names : list of str
<         Defaults to `('softmax_label')` for a typical model used in image
<         classification.
<     logger : Logger
<         Defaults to `logging`.
<     context : Context or list of Context
<         Defaults to ``mx.cpu()``.
<     work_load_list : list of number
<         Default ``None``, indicating uniform workload.
<     fixed_param_names: list of str
<         Default ``None``, indicating no network parameters are fixed.
<     state_names : list of str
<         states are similar to data and label, but not provided by data iterator.
<         Instead they are initialized to 0 and can be set by `set_states()`.
<     group2ctxs : dict of str to context or list of context,
<                  or list of dict of str to context
<         Default is `None`. Mapping the `ctx_group` attribute to the context assignment.
<     compression_params : dict
<         Specifies type of gradient compression and additional arguments depending
<         on the type of compression being used. For example, 2bit compression requires a threshold.
<         Arguments would then be {'type':'2bit', 'threshold':0.5}
<         See mxnet.KVStore.set_gradient_compression method for more details on gradient compression.
<     """
<     def __init__(self, symbol, data_names=('data',), label_names=('softmax_label',),
<                  logger=logging, context=ctx.cpu(), work_load_list=None,
<                  fixed_param_names=None, state_names=None, group2ctxs=None,
<                  compression_params=None):
<         super(Module, self).__init__(logger=logger)
< 
<         if isinstance(context, ctx.Context):
<             context = [context]
<         self._context = context
<         if work_load_list is None:
<             work_load_list = [1] * len(self._context)
<         assert len(work_load_list) == len(self._context)
<         self._work_load_list = work_load_list
< 
<         self._group2ctxs = group2ctxs
< 
<         self._symbol = symbol
< 
<         data_names = list(data_names) if data_names is not None else []
<         label_names = list(label_names) if label_names is not None else []
<         state_names = list(state_names) if state_names is not None else []
<         fixed_param_names = list(fixed_param_names) if fixed_param_names is not None else []
< 
<         _check_input_names(symbol, data_names, "data", True)
<         _check_input_names(symbol, label_names, "label", False)
<         _check_input_names(symbol, state_names, "state", True)
<         _check_input_names(symbol, fixed_param_names, "fixed_param", True)
< 
<         arg_names = symbol.list_arguments()
<         input_names = data_names + label_names + state_names
<         self._param_names = [x for x in arg_names if x not in input_names]
<         self._fixed_param_names = fixed_param_names
<         self._aux_names = symbol.list_auxiliary_states()
<         self._data_names = data_names
<         self._label_names = label_names
<         self._state_names = state_names
<         self._output_names = symbol.list_outputs()
< 
<         self._arg_params = None
<         self._aux_params = None
<         self._params_dirty = False
< 
<         self._compression_params = compression_params
<         self._optimizer = None
<         self._kvstore = None
<         self._update_on_kvstore = None
<         self._updater = None
<         self._preload_opt_states = None
<         self._grad_req = None
< 
<         self._exec_group = None
<         self._data_shapes = None
<         self._label_shapes = None
<         self.shnd = []
< 
<     @staticmethod
<     def load(prefix, epoch, load_optimizer_states=False, **kwargs):
<         """Creates a model from previously saved checkpoint.
< 
<         Parameters
<         ----------
<         prefix : str
<             path prefix of saved model files. You should have
<             "prefix-symbol.json", "prefix-xxxx.params", and
<             optionally "prefix-xxxx.states", where xxxx is the
<             epoch number.
<         epoch : int
<             epoch to load.
<         load_optimizer_states : bool
<             whether to load optimizer states. Checkpoint needs
<             to have been made with save_optimizer_states=True.
<         data_names : list of str
<             Default is `('data')` for a typical model used in image classification.
<         label_names : list of str
<             Default is `('softmax_label')` for a typical model used in image
<             classification.
<         logger : Logger
<             Default is `logging`.
<         context : Context or list of Context
<             Default is ``cpu()``.
<         work_load_list : list of number
<             Default ``None``, indicating uniform workload.
<         fixed_param_names: list of str
<             Default ``None``, indicating no network parameters are fixed.
<         """
<         sym, args, auxs = load_checkpoint(prefix, epoch)
<         mod = Module(symbol=sym, **kwargs)
<         mod._arg_params = args
<         mod._aux_params = auxs
<         mod.params_initialized = True
<         if load_optimizer_states:
<             mod._preload_opt_states = '%s-%04d.states'%(prefix, epoch)
<         return mod
< 
<     def save_checkpoint(self, prefix, epoch, save_optimizer_states=False):
<         """Saves current progress to checkpoint.
<         Use `mx.callback.module_checkpoint` as `epoch_end_callback` to save during training.
< 
<         Parameters
<         ----------
<         prefix : str
<             The file prefix to checkpoint to.
<         epoch : int
<             The current epoch number.
<         save_optimizer_states : bool
<             Whether to save optimizer states to continue training.
<         """
<         self._symbol.save('%s-symbol.json'%prefix)
<         param_name = '%s-%04d.params' % (prefix, epoch)
<         self.save_params(param_name)
<         logging.info('Saved checkpoint to \"%s\"', param_name)
<         if save_optimizer_states:
<             state_name = '%s-%04d.states' % (prefix, epoch)
<             self.save_optimizer_states(state_name)
<             logging.info('Saved optimizer state to \"%s\"', state_name)
< 
<     def _reset_bind(self):
<         """Internal function to reset binded state."""
<         self.binded = False
<         self._exec_group = None
<         self._data_shapes = None
<         self._label_shapes = None
< 
<     @property
<     def data_names(self):
<         """A list of names for data required by this module."""
<         return self._data_names
< 
<     @property
<     def label_names(self):
<         """A list of names for labels required by this module."""
<         return self._label_names
< 
<     @property
<     def output_names(self):
<         """A list of names for the outputs of this module."""
<         return self._output_names
< 
<     @property
<     def data_shapes(self):
<         """Gets data shapes.
< 
<         Returns
<         -------
<         A list of `(name, shape)` pairs.
<         """
<         assert self.binded
<         return self._data_shapes
< 
<     @property
<     def label_shapes(self):
<         """Gets label shapes.
< 
<         Returns
<         -------
<         A list of `(name, shape)` pairs.
<             The return value could be ``None`` if
<             the module does not need labels, or if the module is not bound for
<             training (in this case, label information is not available).
<         """
<         assert self.binded
<         return self._label_shapes
< 
<     @property
<     def output_shapes(self):
<         """Gets output shapes.
< 
<         Returns
<         -------
<         A list of `(name, shape)` pairs.
<         """
<         assert self.binded
<         return self._exec_group.get_output_shapes()
< 
<     def get_params(self):
<         """Gets current parameters.
< 
<         Returns
<         -------
<         `(arg_params, aux_params)`
<             A pair of dictionaries each mapping parameter names to NDArray values.
<         """
<         assert self.binded and self.params_initialized
< 
<         if self._params_dirty:
<             self._sync_params_from_devices()
<         return (self._arg_params, self._aux_params)
< 
<     def init_params(self, initializer=Uniform(0.01), arg_params=None, aux_params=None,
<                     allow_missing=False, force_init=False, allow_extra=False):
<         """Initializes the parameters and auxiliary states.
< 
<         Parameters
<         ----------
<         initializer : Initializer
<             Called to initialize parameters if needed.
<         arg_params : dict
<             If not ``None``, should be a dictionary of existing arg_params. Initialization
<             will be copied from that.
<         aux_params : dict
<             If not ``None``, should be a dictionary of existing aux_params. Initialization
<             will be copied from that.
<         allow_missing : bool
<             If ``True``, params could contain missing values, and the initializer will be
<             called to fill those missing params.
<         force_init : bool
<             If ``True``, will force re-initialize even if already initialized.
<         allow_extra : boolean, optional
<             Whether allow extra parameters that are not needed by symbol.
<             If this is True, no error will be thrown when arg_params or aux_params
<             contain extra parameters that is not needed by the executor.
<         """
<         if self.params_initialized and not force_init:
<             warnings.warn("Parameters already initialized and force_init=False. "
<                           "init_params call ignored.", stacklevel=2)
<             return
<         assert self.binded, 'call bind before initializing the parameters'
< 
<         def _impl(name, arr, cache):
<             """Internal helper for parameter initialization"""
<             if cache is not None:
<                 if name in cache:
<                     cache_arr = cache[name]
< 
<                     # just in case the cached array is just the target itself
<                     if cache_arr is not arr:
<                         cache_arr.copyto(arr)
<                 else:
<                     if not allow_missing:
<                         raise RuntimeError("%s is not presented" % name)
<                     if initializer is not None:
<                         initializer(name, arr)
<             else:
<                 initializer(name, arr)
< 
<         attrs = self._symbol.attr_dict()
<         for name, arr in sorted(self._arg_params.items()):
<             desc = InitDesc(name, attrs.get(name, None))
<             _impl(desc, arr, arg_params)
< 
<         for name, arr in sorted(self._aux_params.items()):
<             desc = InitDesc(name, attrs.get(name, None))
<             _impl(desc, arr, aux_params)
< 
<         self.params_initialized = True
<         self._params_dirty = False
< 
<         # copy the initialized parameters to devices
<         self._exec_group.set_params(self._arg_params, self._aux_params,
<                                     allow_extra=allow_extra)
< 
<     def set_params(self, arg_params, aux_params, allow_missing=False, force_init=True,
<                    allow_extra=False):
<         """Assigns parameter and aux state values.
< 
<         Parameters
<         ----------
<         arg_params : dict
<             Dictionary of name to `NDArray`.
<         aux_params : dict
<             Dictionary of name to `NDArray`.
<         allow_missing : bool
<             If ``True``, params could contain missing values, and the initializer will be
<             called to fill those missing params.
<         force_init : bool
<             If ``True``, will force re-initialize even if already initialized.
<         allow_extra : boolean, optional
<             Whether allow extra parameters that are not needed by symbol.
<             If this is True, no error will be thrown when arg_params or aux_params
<             contain extra parameters that is not needed by the executor.
<         Examples
<         --------
<         >>> # An example of setting module parameters.
<         >>> sym, arg_params, aux_params = mx.model.load_checkpoint(model_prefix, n_epoch_load)
<         >>> mod.set_params(arg_params=arg_params, aux_params=aux_params)
<         """
<         if not allow_missing:
<             self.init_params(initializer=None, arg_params=arg_params, aux_params=aux_params,
<                              allow_missing=allow_missing, force_init=force_init,
<                              allow_extra=allow_extra)
<             return
< 
<         if self.params_initialized and not force_init:
<             warnings.warn("Parameters already initialized and force_init=False. "
<                           "set_params call ignored.", stacklevel=2)
<             return
< 
<         self._exec_group.set_params(arg_params, aux_params, allow_extra=allow_extra)
< 
<         # because we didn't update self._arg_params, they are dirty now.
<         self._params_dirty = True
<         self.params_initialized = True
< 
<     def bind(self, data_shapes, label_shapes=None, for_training=True,
<              inputs_need_grad=False, force_rebind=False, shared_module=None,
<              grad_req='write'):
<         """Binds the symbols to construct executors. This is necessary before one
<         can perform computation with the module.
< 
<         Parameters
<         ----------
<         data_shapes : list of (str, tuple)
<             Typically is ``data_iter.provide_data``.
<         label_shapes : list of (str, tuple)
<             Typically is ``data_iter.provide_label``.
<         for_training : bool
<             Default is ``True``. Whether the executors should be bound for training.
<         inputs_need_grad : bool
<             Default is ``False``. Whether the gradients to the input data need to be computed.
<             Typically this is not needed. But this might be needed when implementing composition
<             of modules.
<         force_rebind : bool
<             Default is ``False``. This function does nothing if the executors are already
<             bound. But with this ``True``, the executors will be forced to rebind.
<         shared_module : Module
<             Default is ``None``. This is used in bucketing. When not ``None``, the shared module
<             essentially corresponds to a different bucket -- a module with different symbol
<             but with the same sets of parameters (e.g. unrolled RNNs with different lengths).
<         """
<         # force rebinding is typically used when one want to switch from
<         # training to prediction phase.
<         if force_rebind:
<             self._reset_bind()
< 
<         if self.binded:
<             self.logger.warning('Already bound, ignoring bind()')
<             return
< 
<         self.for_training = for_training
<         self.inputs_need_grad = inputs_need_grad
<         self.binded = True
<         self._grad_req = grad_req
< 
<         if not for_training:
<             assert not inputs_need_grad
<         else:
<             pass
<             # this is not True, as some module might not contains a loss function
<             # that consumes the labels
<             # assert label_shapes is not None
< 
<         self._data_shapes, self._label_shapes = _parse_data_desc(
<             self.data_names, self.label_names, data_shapes, label_shapes)
< 
<         if shared_module is not None:
<             assert isinstance(shared_module, Module) and \
<                     shared_module.binded and shared_module.params_initialized
<             shared_group = shared_module._exec_group
<             assert len(shared_group.execs) >= len(self._context)
<         else:
<             shared_group = None
< 
<         self._exec_group = DataParallelExecutorGroup(self._symbol, self._context,
<                                                      self._work_load_list, self._data_shapes,
<                                                      self._label_shapes, self._param_names,
<                                                      for_training, inputs_need_grad,
<                                                      shared_group, logger=self.logger,
<                                                      fixed_param_names=self._fixed_param_names,
<                                                      grad_req=grad_req, group2ctxs=self._group2ctxs,
<                                                      state_names=self._state_names)
<         self._total_exec_bytes = self._exec_group._total_exec_bytes
<         if shared_module is not None:
<             self.params_initialized = True
<             self._arg_params = shared_module._arg_params
<             self._aux_params = shared_module._aux_params
<         elif self.params_initialized:
<             # if the parameters are already initialized, we are re-binding
<             # so automatically copy the already initialized params
<             self._exec_group.set_params(self._arg_params, self._aux_params)
<         else:
<             assert self._arg_params is None and self._aux_params is None
<             param_arrays = [
<                 zeros(shape=x[0].shape, dtype=x[0].dtype, stype=x[0].stype)
<                 for x in self._exec_group.param_arrays
<             ]
<             self._arg_params = {name:arr for name, arr in zip(self._param_names, param_arrays)}
< 
<             aux_arrays = [
<                 zeros(x[0].shape, dtype=x[0].dtype)
<                 for x in self._exec_group.aux_arrays
<             ]
<             self._aux_params = {name:arr for name, arr in zip(self._aux_names, aux_arrays)}
< 
<         if shared_module is not None and shared_module.optimizer_initialized:
<             self.borrow_optimizer(shared_module)
< 
<     def reshape(self, data_shapes, label_shapes=None):
<         """Reshapes the module for new input shapes.
< 
<         Parameters
<         ----------
<         data_shapes : list of (str, tuple)
<             Typically is ``data_iter.provide_data``.
<         label_shapes : list of (str, tuple)
<             Typically is ``data_iter.provide_label``.
<         """
<         assert self.binded
<         self._data_shapes, self._label_shapes = _parse_data_desc(
<             self.data_names, self.label_names, data_shapes, label_shapes)
< 
<         self._exec_group.reshape(self._data_shapes, self._label_shapes)
< 
<     def init_optimizer(self, kvstore='local', optimizer='sgd',
<                        optimizer_params=(('learning_rate', 0.01),), force_init=False):
<         """Installs and initializes optimizers.
< 
<         Parameters
<         ----------
<         kvstore : str or KVStore
<             Default `'local'`.
<         optimizer : str or Optimizer
<             Default `'sgd'`
<         optimizer_params : dict
<             Default `(('learning_rate', 0.01),)`. The default value is not a dictionary,
<             just to avoid pylint warning of dangerous default values.
<         force_init : bool
<             Default ``False``, indicating whether we should force re-initializing the
<             optimizer in the case an optimizer is already installed.
<         """
<         assert self.binded and self.params_initialized
< 
<         if self.optimizer_initialized and not force_init:
<             self.logger.warning('optimizer already initialized, ignoring...')
<             return
< 
<         if self._params_dirty:
<             self._sync_params_from_devices()
< 
<         (kvstore, update_on_kvstore) = \
<                 _create_kvstore(kvstore, len(self._context), self._arg_params)
< 
<         batch_size = self._exec_group.batch_size
<         if kvstore and 'dist' in kvstore.type and '_sync' in kvstore.type:
<             batch_size *= kvstore.num_workers
<         rescale_grad = 1.0/batch_size
< 
<         if isinstance(optimizer, str):
<             idx2name = {}
<             if update_on_kvstore:
<                 idx2name.update(enumerate(self._exec_group.param_names))
<             else:
<                 for k in range(len(self._context)):
<                     idx2name.update({i*len(self._context)+k: n
<                                      for i, n in enumerate(self._exec_group.param_names)})
<             optimizer_params = dict(optimizer_params)
<             if 'rescale_grad' not in optimizer_params:
<                 optimizer_params['rescale_grad'] = rescale_grad
<             optimizer = opt.create(optimizer,
<                                    sym=self.symbol, param_idx2name=idx2name,
<                                    **optimizer_params)
<         else:
<             assert isinstance(optimizer, opt.Optimizer)
<             if optimizer.rescale_grad != rescale_grad:
<                 #pylint: disable=no-member
<                 warnings.warn(
<                     "Optimizer created manually outside Module but rescale_grad " +
<                     "is not normalized to 1.0/batch_size/num_workers (%s vs. %s). "%(
<                         optimizer.rescale_grad, rescale_grad) +
<                     "Is this intended?", stacklevel=2)
< 
<         self._optimizer = optimizer
<         self._kvstore = kvstore
<         self._update_on_kvstore = update_on_kvstore
<         self._updater = None
< 
<         if kvstore:
<             if self._compression_params:
<                 kvstore.set_gradient_compression(self._compression_params)
<             if update_on_kvstore:
<                 kvstore.set_optimizer(self._optimizer)
<             # copy initialized local parameters to kvstore
<             _initialize_kvstore(kvstore=kvstore,
<                                 param_arrays=self._exec_group.param_arrays,
<                                 arg_params=self._arg_params,
<                                 param_names=self._param_names,
<                                 update_on_kvstore=update_on_kvstore)
< 
<         if not update_on_kvstore:
<             self._updater = opt.get_updater(optimizer)
< 
<         self.optimizer_initialized = True
< 
<         if self._preload_opt_states is not None:
<             self.load_optimizer_states(self._preload_opt_states)
<             self._preload_opt_states = None
< 
<     def borrow_optimizer(self, shared_module):
<         """Borrows optimizer from a shared module. Used in bucketing, where exactly the same
<         optimizer (esp. kvstore) is used.
< 
<         Parameters
<         ----------
<         shared_module : Module
<         """
<         assert shared_module.optimizer_initialized
<         self._optimizer = shared_module._optimizer
<         self._kvstore = shared_module._kvstore
<         self._update_on_kvstore = shared_module._update_on_kvstore
<         self._updater = shared_module._updater
<         self.optimizer_initialized = True
< 
<     def forward(self, data_batch, is_train=None):
<         """Forward computation. It supports data batches with different shapes, such as
<         different batch sizes or different image sizes.
<         If reshaping of data batch relates to modification of symbol or module, such as
<         changing image layout ordering or switching from training to predicting, module
<         rebinding is required.
< 
<         See Also
<         ----------
<         :meth:`BaseModule.forward`.
< 
<         Parameters
<         ----------
<         data_batch : DataBatch
<             Could be anything with similar API implemented.
<         is_train : bool
<             Default is ``None``, which means ``is_train`` takes the value of ``self.for_training``.
<         """
<         assert self.binded and self.params_initialized
< 
<         curr_data_shapes = tuple(i.shape for i in self._data_shapes)
<         new_data_shapes = tuple(i.shape for i in data_batch.data)
< 
<         if curr_data_shapes != new_data_shapes:
<             if hasattr(data_batch, "provide_data") and data_batch.provide_data:
<                 new_dshape = data_batch.provide_data
<             else:
<                 new_dshape = [DataDesc(i.name, shape, i.dtype, i.layout) \
<                               for i, shape in zip(self._data_shapes, new_data_shapes)]
< 
<             if hasattr(data_batch, "provide_label") and data_batch.provide_label:
<                 new_lshape = data_batch.provide_label
<             elif hasattr(data_batch, "label") and data_batch.label:
<                 new_lshape = [DataDesc(i.name, j.shape, i.dtype, i.layout) \
<                               for i, j in zip(self._label_shapes, data_batch.label)]
<             else:
<                 new_lshape = None
< 
<             self.reshape(new_dshape, new_lshape)
< 
<         self._exec_group.forward(data_batch, is_train)
< 
<     def backward(self, out_grads=None):
<         """Backward computation.
< 
<         See Also
<         ----------
<         :meth:`BaseModule.backward`.
< 
<         Parameters
<         ----------
<         out_grads : NDArray or list of NDArray, optional
<             Gradient on the outputs to be propagated back.
<             This parameter is only needed when bind is called
<             on outputs that are not a loss function.
<         """
<         assert self.binded and self.params_initialized
<         self._exec_group.backward(out_grads=out_grads)
< 
<     def update(self, work_id):
<         """Updates parameters according to the installed optimizer and the gradients computed
<         in the previous forward-backward batch.
< 
<         When KVStore is used to update parameters for multi-device or multi-machine training,
<         a copy of the parameters are stored in KVStore. Note that for `row_sparse` parameters,
<         this function does update the copy of parameters in KVStore, but doesn't broadcast the
<         updated parameters to all devices / machines. Please call `prepare` to broadcast
<         `row_sparse` parameters with the next batch of data.
< 
<         See Also
<         ----------
<         :meth:`BaseModule.update`.
<         """
<         assert self.binded and self.params_initialized and self.optimizer_initialized
< 
<         self._params_dirty = True
<         if self._update_on_kvstore:
<             _update_params_on_kvstore(self._exec_group.param_arrays,
<                                       self._exec_group.grad_arrays,
<                                       self._kvstore, self._exec_group.param_names)
<         else:
<             if len(self.shnd)==0 :
<                 _init_shnd(self._exec_group.param_arrays,
<                         self._exec_group.grad_arrays, self.shnd, work_id)
<          #   print(self.shnd)
<             _update_params(self._exec_group.param_arrays,
<                            self._exec_group.grad_arrays, work_id,
<                            updater=self._updater,
<                            num_device=len(self._context),
<                            kvstore=self._kvstore,
<                            param_names=self._exec_group.param_names, nd=self.shnd)
< 
<     def get_outputs(self, merge_multi_context=True):
<         """Gets outputs of the previous forward computation.
< 
<         If ``merge_multi_context`` is ``True``, it is like ``[out1, out2]``. Otherwise, it
<         is like ``[[out1_dev1, out1_dev2], [out2_dev1, out2_dev2]]``. All the output
<         elements are `NDArray`. When `merge_multi_context` is `False`, those `NDArray`
<         might live on different devices.
< 
<         Parameters
<         ----------
<         merge_multi_context : bool
<             Default is ``True``. In the case when data-parallelism is used, the outputs
<             will be collected from multiple devices. A ``True`` value indicate that we
<             should merge the collected results so that they look like from a single
<             executor.
< 
<         Returns
<         -------
<         list of NDArray or list of list of NDArray
<             Output.
<         """
<         assert self.binded and self.params_initialized
<         return self._exec_group.get_outputs(merge_multi_context=merge_multi_context)
< 
<     def get_input_grads(self, merge_multi_context=True):
<         """Gets the gradients with respect to the inputs of the module.
< 
<         If ``merge_multi_context`` is ``True``, it is like ``[grad1, grad2]``. Otherwise, it
<         is like ``[[grad1_dev1, grad1_dev2], [grad2_dev1, grad2_dev2]]``. All the output
<         elements are `NDArray`.
< 
<         Parameters
<         ----------
<         merge_multi_context : bool
<             Default is ``True``. In the case when data-parallelism is used, the outputs
<             will be collected from multiple devices. A ``True`` value indicate that we
<             should merge the collected results so that they look like from a single
<             executor.
< 
<         Returns
<         -------
<         list of NDArray or list of list of NDArray
<               Input gradients
<         """
<         assert self.binded and self.params_initialized and self.inputs_need_grad
<         return self._exec_group.get_input_grads(merge_multi_context=merge_multi_context)
< 
<     def get_states(self, merge_multi_context=True):
<         """Gets states from all devices.
< 
<         If `merge_multi_context` is ``True``, it is like ``[out1, out2]``. Otherwise, it
<         is like ``[[out1_dev1, out1_dev2], [out2_dev1, out2_dev2]]``. All the output
<         elements are `NDArray`.
< 
<         Parameters
<         ----------
<         merge_multi_context : bool
<             Default is ``True``. In the case when data-parallelism is used, the states
<             will be collected from multiple devices. A ``True`` value indicate that we
<             should merge the collected results so that they look like from a single
<             executor.
< 
<         Returns
<         -------
<         list of NDArray or list of list of NDArray
<             States
<         """
<         assert self.binded and self.params_initialized
<         return self._exec_group.get_states(merge_multi_context=merge_multi_context)
< 
<     def set_states(self, states=None, value=None):
<         """Sets value for states. Only one of the states & value can be specified.
< 
<         Parameters
<         ----------
<         states : list of list of NDArrays
<             source states arrays formatted like ``[[state1_dev1, state1_dev2],
<             [state2_dev1, state2_dev2]]``.
<         value : number
<             a single scalar value for all state arrays.
<         """
<         assert self.binded and self.params_initialized
<         self._exec_group.set_states(states, value)
< 
<     def update_metric(self, eval_metric, labels):
<         """Evaluates and accumulates evaluation metric on outputs of the last forward computation.
< 
<         See Also
<         ----------
<         :meth:`BaseModule.update_metric`.
< 
<         Parameters
<         ----------
<         eval_metric : EvalMetric
<         labels : list of NDArray
<             Typically ``data_batch.label``.
<         """
<         self._exec_group.update_metric(eval_metric, labels)
< 
<     def _sync_params_from_devices(self):
<         """Synchronizes parameters from devices to CPU. This function should be called after
<         calling `update` that updates the parameters on the devices, before one can read the
<         latest parameters from ``self._arg_params`` and ``self._aux_params``.
< 
<         For row_sparse parameters on devices, ther are pulled from KVStore with all row ids.
< 
<         """
<         self._exec_group.get_params(self._arg_params, self._aux_params)
<         if self._kvstore and self._update_on_kvstore:
<             for param_name, param_val in sorted(self._arg_params.items()):
<                 if param_val.stype == 'row_sparse':
<                     row_ids = nd.arange(0, param_val.shape[0], dtype='int64')
<                     self._kvstore.row_sparse_pull(param_name, param_val, row_ids=row_ids)
<         self._params_dirty = False
< 
<     def save_optimizer_states(self, fname):
<         """Saves optimizer (updater) state to a file.
< 
<         Parameters
<         ----------
<         fname : str
<             Path to output states file.
<         """
<         assert self.optimizer_initialized
< 
<         if self._update_on_kvstore:
<             self._kvstore.save_optimizer_states(fname)
<         else:
<             with open(fname, 'wb') as fout:
<                 fout.write(self._updater.get_states())
< 
<     def load_optimizer_states(self, fname):
<         """Loads optimizer (updater) state from a file.
< 
<         Parameters
<         ----------
<         fname : str
<             Path to input states file.
<         """
<         assert self.optimizer_initialized
< 
<         if self._update_on_kvstore:
<             self._kvstore.load_optimizer_states(fname)
<         else:
<             self._updater.set_states(open(fname, 'rb').read())
< 
<     def install_monitor(self, mon):
<         """Installs monitor on all executors. """
<         assert self.binded
<         self._exec_group.install_monitor(mon)
< 
<     def prepare(self, data_batch, sparse_row_id_fn=None):
<         '''Prepares the module for processing a data batch.
< 
<         Usually involves switching bucket and reshaping.
<         For modules that contain `row_sparse` parameters in KVStore,
<         it prepares the `row_sparse` parameters based on the sparse_row_id_fn.
< 
<         When KVStore is used to update parameters for multi-device or multi-machine training,
<         a copy of the parameters are stored in KVStore. Note that for `row_sparse` parameters,
<         the `update()` updates the copy of parameters in KVStore, but doesn't broadcast
<         the updated parameters to all devices / machines. The `prepare` function is used to
<         broadcast `row_sparse` parameters with the next batch of data.
< 
<         Parameters
<         ----------
<         data_batch : DataBatch
<             The current batch of data for forward computation.
< 
<         sparse_row_id_fn : A callback function
<             The function  takes `data_batch` as an input and returns a dict of
<             str -> NDArray. The resulting dict is used for pulling row_sparse
<             parameters from the kvstore, where the str key is the name of the param,
<             and the value is the row id of the param to pull.
<         '''
<         assert self.binded
<         if sparse_row_id_fn is not None:
<             if not self._kvstore or not self._update_on_kvstore:
<                 warnings.warn(UserWarning("Parameters are not updated in the KVStore. "
<                                           "No need to call sparse_row_id_fn."))
<             else:
<                 row_ids = sparse_row_id_fn(data_batch)
<                 assert(isinstance(row_ids, dict)), "Expected dict output from sparse_row_id_fn"
<                 for param_name, row_id in row_ids.items():
<                     param_idx = self._exec_group.param_names.index(param_name)
<                     param_val = self._exec_group.param_arrays[param_idx]
<                     assert(isinstance(param_val, (tuple, list)))
<                     if param_val[0].stype != 'row_sparse':
<                         warnings.warn(UserWarning("%s.stype is not 'row_sparse'. No need to "
<                                                   "perform row_sparse_pull." % param_name))
<                     else:
<                         self._kvstore.row_sparse_pull(param_name, param_val, row_ids=row_id,
<                                                       priority=-param_idx)
---
> # Licensed to the Apache Software Foundation (ASF) under one
> # or more contributor license agreements.  See the NOTICE file
> # distributed with this work for additional information
> # regarding copyright ownership.  The ASF licenses this file
> # to you under the Apache License, Version 2.0 (the
> # "License"); you may not use this file except in compliance
> # with the License.  You may obtain a copy of the License at
> #
> #   http://www.apache.org/licenses/LICENSE-2.0
> #
> # Unless required by applicable law or agreed to in writing,
> # software distributed under the License is distributed on an
> # "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
> # KIND, either express or implied.  See the License for the
> # specific language governing permissions and limitations
> # under the License.
> 
> # pylint: disable=too-many-instance-attributes, too-many-arguments, protected-access, too-many-branches
> # pylint: disable=too-many-public-methods
> """A `Module` implement the `BaseModule` API by wrapping a `Symbol` and one or
> more `Executor` for data parallelization.
> """
> 
> import logging
> import warnings
> 
> from .. import context as ctx
> from .. import optimizer as opt
> from .. import ndarray as nd
> 
> from .executor_group import DataParallelExecutorGroup
> from ..model import _create_kvstore, _initialize_kvstore, _update_params, _update_params_on_kvstore
> from ..model import load_checkpoint
> from ..initializer import Uniform, InitDesc
> from ..io import DataDesc
> from ..ndarray import zeros
> 
> from .base_module import BaseModule, _check_input_names, _parse_data_desc
> 
> class Module(BaseModule):
>     """Module is a basic module that wrap a `Symbol`. It is functionally the same
>     as the `FeedForward` model, except under the module API.
> 
>     Parameters
>     ----------
>     symbol : Symbol
>     data_names : list of str
>         Defaults to `('data')` for a typical model used in image classification.
>     label_names : list of str
>         Defaults to `('softmax_label')` for a typical model used in image
>         classification.
>     logger : Logger
>         Defaults to `logging`.
>     context : Context or list of Context
>         Defaults to ``mx.cpu()``.
>     work_load_list : list of number
>         Default ``None``, indicating uniform workload.
>     fixed_param_names: list of str
>         Default ``None``, indicating no network parameters are fixed.
>     state_names : list of str
>         states are similar to data and label, but not provided by data iterator.
>         Instead they are initialized to 0 and can be set by `set_states()`.
>     group2ctxs : dict of str to context or list of context,
>                  or list of dict of str to context
>         Default is `None`. Mapping the `ctx_group` attribute to the context assignment.
>     compression_params : dict
>         Specifies type of gradient compression and additional arguments depending
>         on the type of compression being used. For example, 2bit compression requires a threshold.
>         Arguments would then be {'type':'2bit', 'threshold':0.5}
>         See mxnet.KVStore.set_gradient_compression method for more details on gradient compression.
>     """
>     def __init__(self, symbol, data_names=('data',), label_names=('softmax_label',),
>                  logger=logging, context=ctx.cpu(), work_load_list=None,
>                  fixed_param_names=None, state_names=None, group2ctxs=None,
>                  compression_params=None):
>         super(Module, self).__init__(logger=logger)
> 
>         if isinstance(context, ctx.Context):
>             context = [context]
>         self._context = context
>         if work_load_list is None:
>             work_load_list = [1] * len(self._context)
>         assert len(work_load_list) == len(self._context)
>         self._work_load_list = work_load_list
> 
>         self._group2ctxs = group2ctxs
> 
>         self._symbol = symbol
> 
>         data_names = list(data_names) if data_names is not None else []
>         label_names = list(label_names) if label_names is not None else []
>         state_names = list(state_names) if state_names is not None else []
>         fixed_param_names = list(fixed_param_names) if fixed_param_names is not None else []
> 
>         _check_input_names(symbol, data_names, "data", True)
>         _check_input_names(symbol, label_names, "label", False)
>         _check_input_names(symbol, state_names, "state", True)
>         _check_input_names(symbol, fixed_param_names, "fixed_param", True)
> 
>         arg_names = symbol.list_arguments()
>         input_names = data_names + label_names + state_names
>         self._param_names = [x for x in arg_names if x not in input_names]
>         self._fixed_param_names = fixed_param_names
>         self._aux_names = symbol.list_auxiliary_states()
>         self._data_names = data_names
>         self._label_names = label_names
>         self._state_names = state_names
>         self._output_names = symbol.list_outputs()
> 
>         self._arg_params = None
>         self._aux_params = None
>         self._params_dirty = False
> 
>         self._compression_params = compression_params
>         self._optimizer = None
>         self._kvstore = None
>         self._update_on_kvstore = None
>         self._updater = None
>         self._preload_opt_states = None
>         self._grad_req = None
> 
>         self._exec_group = None
>         self._data_shapes = None
>         self._label_shapes = None
> 
>     @staticmethod
>     def load(prefix, epoch, load_optimizer_states=False, **kwargs):
>         """Creates a model from previously saved checkpoint.
> 
>         Parameters
>         ----------
>         prefix : str
>             path prefix of saved model files. You should have
>             "prefix-symbol.json", "prefix-xxxx.params", and
>             optionally "prefix-xxxx.states", where xxxx is the
>             epoch number.
>         epoch : int
>             epoch to load.
>         load_optimizer_states : bool
>             whether to load optimizer states. Checkpoint needs
>             to have been made with save_optimizer_states=True.
>         data_names : list of str
>             Default is `('data')` for a typical model used in image classification.
>         label_names : list of str
>             Default is `('softmax_label')` for a typical model used in image
>             classification.
>         logger : Logger
>             Default is `logging`.
>         context : Context or list of Context
>             Default is ``cpu()``.
>         work_load_list : list of number
>             Default ``None``, indicating uniform workload.
>         fixed_param_names: list of str
>             Default ``None``, indicating no network parameters are fixed.
>         """
>         sym, args, auxs = load_checkpoint(prefix, epoch)
>         mod = Module(symbol=sym, **kwargs)
>         mod._arg_params = args
>         mod._aux_params = auxs
>         mod.params_initialized = True
>         if load_optimizer_states:
>             mod._preload_opt_states = '%s-%04d.states'%(prefix, epoch)
>         return mod
> 
>     def save_checkpoint(self, prefix, epoch, save_optimizer_states=False, remove_amp_cast=True):
>         """Saves current progress to checkpoint.
>         Use `mx.callback.module_checkpoint` as `epoch_end_callback` to save during training.
> 
>         Parameters
>         ----------
>         prefix : str
>             The file prefix to checkpoint to.
>         epoch : int
>             The current epoch number.
>         save_optimizer_states : bool
>             Whether to save optimizer states to continue training.
>         """
>         self._symbol.save('%s-symbol.json'%prefix, remove_amp_cast=remove_amp_cast)
>         param_name = '%s-%04d.params' % (prefix, epoch)
>         self.save_params(param_name)
>         logging.info('Saved checkpoint to \"%s\"', param_name)
>         if save_optimizer_states:
>             state_name = '%s-%04d.states' % (prefix, epoch)
>             self.save_optimizer_states(state_name)
>             logging.info('Saved optimizer state to \"%s\"', state_name)
> 
>     def _reset_bind(self):
>         """Internal function to reset binded state."""
>         self.binded = False
>         self._exec_group = None
>         self._data_shapes = None
>         self._label_shapes = None
> 
>     @property
>     def data_names(self):
>         """A list of names for data required by this module."""
>         return self._data_names
> 
>     @property
>     def label_names(self):
>         """A list of names for labels required by this module."""
>         return self._label_names
> 
>     @property
>     def output_names(self):
>         """A list of names for the outputs of this module."""
>         return self._output_names
> 
>     @property
>     def data_shapes(self):
>         """Gets data shapes.
> 
>         Returns
>         -------
>         A list of `(name, shape)` pairs.
>         """
>         assert self.binded
>         return self._data_shapes
> 
>     @property
>     def label_shapes(self):
>         """Gets label shapes.
> 
>         Returns
>         -------
>         A list of `(name, shape)` pairs.
>             The return value could be ``None`` if
>             the module does not need labels, or if the module is not bound for
>             training (in this case, label information is not available).
>         """
>         assert self.binded
>         return self._label_shapes
> 
>     @property
>     def output_shapes(self):
>         """Gets output shapes.
> 
>         Returns
>         -------
>         A list of `(name, shape)` pairs.
>         """
>         assert self.binded
>         return self._exec_group.get_output_shapes()
> 
>     def get_params(self):
>         """Gets current parameters.
> 
>         Returns
>         -------
>         `(arg_params, aux_params)`
>             A pair of dictionaries each mapping parameter names to NDArray values.
>         """
>         assert self.params_initialized
> 
>         if self._params_dirty:
>             self._sync_params_from_devices()
>         return (self._arg_params, self._aux_params)
> 
>     def init_params(self, initializer=Uniform(0.01), arg_params=None, aux_params=None,
>                     allow_missing=False, force_init=False, allow_extra=False):
>         """Initializes the parameters and auxiliary states.
> 
>         Parameters
>         ----------
>         initializer : Initializer
>             Called to initialize parameters if needed.
>         arg_params : dict
>             If not ``None``, should be a dictionary of existing arg_params. Initialization
>             will be copied from that.
>         aux_params : dict
>             If not ``None``, should be a dictionary of existing aux_params. Initialization
>             will be copied from that.
>         allow_missing : bool
>             If ``True``, params could contain missing values, and the initializer will be
>             called to fill those missing params.
>         force_init : bool
>             If ``True``, will force re-initialize even if already initialized.
>         allow_extra : boolean, optional
>             Whether allow extra parameters that are not needed by symbol.
>             If this is True, no error will be thrown when arg_params or aux_params
>             contain extra parameters that is not needed by the executor.
>         """
>         if self.params_initialized and not force_init:
>             warnings.warn("Parameters already initialized and force_init=False. "
>                           "init_params call ignored.", stacklevel=2)
>             return
>         assert self.binded, 'call bind before initializing the parameters'
> 
>         def _impl(name, arr, cache):
>             """Internal helper for parameter initialization"""
>             if cache is not None:
>                 if name in cache:
>                     cache_arr = cache[name]
> 
>                     # just in case the cached array is just the target itself
>                     if cache_arr is not arr:
>                         cache_arr.copyto(arr)
>                 else:
>                     if not allow_missing:
>                         raise RuntimeError("%s is not presented" % name)
>                     if initializer is not None:
>                         initializer(name, arr)
>             else:
>                 initializer(name, arr)
> 
>         attrs = self._symbol.attr_dict()
>         for name, arr in sorted(self._arg_params.items()):
>             desc = InitDesc(name, attrs.get(name, None))
>             _impl(desc, arr, arg_params)
> 
>         for name, arr in sorted(self._aux_params.items()):
>             desc = InitDesc(name, attrs.get(name, None))
>             _impl(desc, arr, aux_params)
> 
>         self.params_initialized = True
>         self._params_dirty = False
> 
>         # copy the initialized parameters to devices
>         self._exec_group.set_params(self._arg_params, self._aux_params,
>                                     allow_extra=allow_extra)
> 
>     def set_params(self, arg_params, aux_params, allow_missing=False, force_init=True,
>                    allow_extra=False):
>         """Assigns parameter and aux state values.
> 
>         Parameters
>         ----------
>         arg_params : dict
>             Dictionary of name to `NDArray`.
>         aux_params : dict
>             Dictionary of name to `NDArray`.
>         allow_missing : bool
>             If ``True``, params could contain missing values, and the initializer will be
>             called to fill those missing params.
>         force_init : bool
>             If ``True``, will force re-initialize even if already initialized.
>         allow_extra : boolean, optional
>             Whether allow extra parameters that are not needed by symbol.
>             If this is True, no error will be thrown when arg_params or aux_params
>             contain extra parameters that is not needed by the executor.
>         Examples
>         --------
>         >>> # An example of setting module parameters.
>         >>> sym, arg_params, aux_params = mx.model.load_checkpoint(model_prefix, n_epoch_load)
>         >>> mod.set_params(arg_params=arg_params, aux_params=aux_params)
>         """
>         if not allow_missing:
>             self.init_params(initializer=None, arg_params=arg_params, aux_params=aux_params,
>                              allow_missing=allow_missing, force_init=force_init,
>                              allow_extra=allow_extra)
>             return
> 
>         if self.params_initialized and not force_init:
>             warnings.warn("Parameters already initialized and force_init=False. "
>                           "set_params call ignored.", stacklevel=2)
>             return
> 
>         self._exec_group.set_params(arg_params, aux_params, allow_extra=allow_extra)
> 
>         # because we didn't update self._arg_params, they are dirty now.
>         self._params_dirty = True
>         self.params_initialized = True
> 
>     def bind(self, data_shapes, label_shapes=None, for_training=True,
>              inputs_need_grad=False, force_rebind=False, shared_module=None,
>              grad_req='write'):
>         """Binds the symbols to construct executors. This is necessary before one
>         can perform computation with the module.
> 
>         Parameters
>         ----------
>         data_shapes : list of (str, tuple)
>             Typically is ``data_iter.provide_data``.
>         label_shapes : list of (str, tuple)
>             Typically is ``data_iter.provide_label``.
>         for_training : bool
>             Default is ``True``. Whether the executors should be bound for training.
>         inputs_need_grad : bool
>             Default is ``False``. Whether the gradients to the input data need to be computed.
>             Typically this is not needed. But this might be needed when implementing composition
>             of modules.
>         force_rebind : bool
>             Default is ``False``. This function does nothing if the executors are already
>             bound. But with this ``True``, the executors will be forced to rebind.
>         shared_module : Module
>             Default is ``None``. This is used in bucketing. When not ``None``, the shared module
>             essentially corresponds to a different bucket -- a module with different symbol
>             but with the same sets of parameters (e.g. unrolled RNNs with different lengths).
>         """
>         # force rebinding is typically used when one want to switch from
>         # training to prediction phase.
>         if force_rebind:
>             self._reset_bind()
> 
>         if self.binded:
>             self.logger.warning('Already bound, ignoring bind()')
>             return
> 
>         self.for_training = for_training
>         self.inputs_need_grad = inputs_need_grad
>         self._grad_req = grad_req
> 
>         if not for_training:
>             assert not inputs_need_grad
>         else:
>             pass
>             # this is not True, as some module might not contains a loss function
>             # that consumes the labels
>             # assert label_shapes is not None
> 
>         self._data_shapes, self._label_shapes = _parse_data_desc(
>             self.data_names, self.label_names, data_shapes, label_shapes)
> 
>         if shared_module is not None:
>             assert isinstance(shared_module, Module) and \
>                     shared_module.binded and shared_module.params_initialized
>             shared_group = shared_module._exec_group
>             assert len(shared_group.execs) >= len(self._context)
>         else:
>             shared_group = None
>         self._exec_group = DataParallelExecutorGroup(self._symbol, self._context,
>                                                      self._work_load_list, self._data_shapes,
>                                                      self._label_shapes, self._param_names,
>                                                      for_training, inputs_need_grad,
>                                                      shared_group, logger=self.logger,
>                                                      fixed_param_names=self._fixed_param_names,
>                                                      grad_req=grad_req, group2ctxs=self._group2ctxs,
>                                                      state_names=self._state_names)
>         self._total_exec_bytes = self._exec_group._total_exec_bytes
>         if shared_module is not None:
>             self.params_initialized = True
>             self._arg_params = shared_module._arg_params
>             self._aux_params = shared_module._aux_params
>         elif self.params_initialized:
>             # if the parameters are already initialized, we are re-binding
>             # so automatically copy the already initialized params
>             self._exec_group.set_params(self._arg_params, self._aux_params)
>         else:
>             assert self._arg_params is None and self._aux_params is None
>             param_arrays = [
>                 zeros(shape=x[0].shape, dtype=x[0].dtype, stype=x[0].stype)
>                 for x in self._exec_group.param_arrays
>             ]
>             self._arg_params = {name:arr for name, arr in zip(self._param_names, param_arrays)}
> 
>             aux_arrays = [
>                 zeros(x[0].shape, dtype=x[0].dtype)
>                 for x in self._exec_group.aux_arrays
>             ]
>             self._aux_params = {name:arr for name, arr in zip(self._aux_names, aux_arrays)}
> 
>         if shared_module is not None and shared_module.optimizer_initialized:
>             self.borrow_optimizer(shared_module)
> 
>         self.binded = True
>         #print(self._arg_params)
>         #while True:
>         #    pass
> 
>     def reshape(self, data_shapes, label_shapes=None):
>         """Reshapes the module for new input shapes.
> 
>         Parameters
>         ----------
>         data_shapes : list of (str, tuple)
>             Typically is ``data_iter.provide_data``.
>         label_shapes : list of (str, tuple)
>             Typically is ``data_iter.provide_label``.
>         """
>         assert self.binded
>         self._data_shapes, self._label_shapes = _parse_data_desc(
>             self.data_names, self.label_names, data_shapes, label_shapes)
> 
>         self._exec_group.reshape(self._data_shapes, self._label_shapes)
> 
>     def init_optimizer(self, kvstore='local', optimizer='sgd',
>                        optimizer_params=(('learning_rate', 0.01),), force_init=False):
>         """Installs and initializes optimizers.
> 
>         Parameters
>         ----------
>         kvstore : str or KVStore
>             Default `'local'`.
>         optimizer : str or Optimizer
>             Default `'sgd'`
>         optimizer_params : dict
>             Default `(('learning_rate', 0.01),)`. The default value is not a dictionary,
>             just to avoid pylint warning of dangerous default values.
>         force_init : bool
>             Default ``False``, indicating whether we should force re-initializing the
>             optimizer in the case an optimizer is already installed.
>         """
>         assert self.binded and self.params_initialized
> 
>         if self.optimizer_initialized and not force_init:
>             self.logger.warning('optimizer already initialized, ignoring...')
>             return
> 
>         if self._params_dirty:
>             self._sync_params_from_devices()
> 
>         (kvstore, update_on_kvstore) = \
>                 _create_kvstore(kvstore, len(self._context), self._arg_params)
> 
>         batch_size = self._exec_group.batch_size
>         if kvstore and 'dist' in kvstore.type and '_sync' in kvstore.type:
>             batch_size *= kvstore.num_workers
>         rescale_grad = 1.0/batch_size
> 
>         idx2name = {}
>         if update_on_kvstore:
>             idx2name.update(enumerate(self._exec_group.param_names))
>         else:
>             for k in range(len(self._context)):
>                 idx2name.update({i*len(self._context)+k: n
>                                  for i, n in enumerate(self._exec_group.param_names)})
>         if isinstance(optimizer, str):
>             optimizer_params = dict(optimizer_params)
>             if 'rescale_grad' not in optimizer_params:
>                 optimizer_params['rescale_grad'] = rescale_grad
>             optimizer = opt.create(optimizer,
>                                    sym=self.symbol, param_idx2name=idx2name,
>                                    **optimizer_params)
>         else:
>             assert isinstance(optimizer, opt.Optimizer)
>             if optimizer.rescale_grad != rescale_grad:
>                 #pylint: disable=no-member
>                 warnings.warn(
>                     "Optimizer created manually outside Module but rescale_grad " +
>                     "is not normalized to 1.0/batch_size/num_workers (%s vs. %s). "%(
>                         optimizer.rescale_grad, rescale_grad) +
>                     "Is this intended?", stacklevel=2)
>             if not optimizer.idx2name:
>                 optimizer.idx2name = idx2name.copy()
> 
>         self._optimizer = optimizer
>         self._kvstore = kvstore
>         self._update_on_kvstore = update_on_kvstore
>         self._updater = None
> 
>         if kvstore:
>             if self._compression_params:
>                 kvstore.set_gradient_compression(self._compression_params)
>             if update_on_kvstore:
>                 kvstore.set_optimizer(self._optimizer)
>             # copy initialized local parameters to kvstore
>             _initialize_kvstore(kvstore=kvstore,
>                                 param_arrays=self._exec_group.param_arrays,
>                                 arg_params=self._arg_params,
>                                 param_names=self._param_names,
>                                 update_on_kvstore=update_on_kvstore)
> 
>         if not update_on_kvstore:
>             self._updater = opt.get_updater(optimizer)
> 
>         self.optimizer_initialized = True
> 
>         if self._preload_opt_states is not None:
>             self.load_optimizer_states(self._preload_opt_states)
>             self._preload_opt_states = None
> 
>     def borrow_optimizer(self, shared_module):
>         """Borrows optimizer from a shared module. Used in bucketing, where exactly the same
>         optimizer (esp. kvstore) is used.
> 
>         Parameters
>         ----------
>         shared_module : Module
>         """
>         assert shared_module.optimizer_initialized
>         self._optimizer = shared_module._optimizer
>         self._kvstore = shared_module._kvstore
>         self._update_on_kvstore = shared_module._update_on_kvstore
>         self._updater = shared_module._updater
>         self.optimizer_initialized = True
> 
>     def forward(self, data_batch, is_train=None):
>         """Forward computation. It supports data batches with different shapes, such as
>         different batch sizes or different image sizes.
>         If reshaping of data batch relates to modification of symbol or module, such as
>         changing image layout ordering or switching from training to predicting, module
>         rebinding is required.
> 
>         See Also
>         ----------
>         :meth:`BaseModule.forward`.
> 
>         Parameters
>         ----------
>         data_batch : DataBatch
>             Could be anything with similar API implemented.
>         is_train : bool
>             Default is ``None``, which means ``is_train`` takes the value of ``self.for_training``.
>         """
>         assert self.binded and self.params_initialized
> 
>         curr_data_shapes = tuple(i.shape for i in self._data_shapes)
>         if isinstance(data_batch, list):
>             assert data_batch is not None, "Encountered empty data batch"
>             new_data_shapes = []
>             for i in range(len(data_batch[0].data)):
>                 shape = data_batch[0].data[i].shape
>                 for db in data_batch:
>                     assert shape == db.data[i].shape, \
>                         "All data batches in a list need to have the same shape"
>                 new_batch_size = len(data_batch) * shape[0]
>                 new_data_shapes.append((new_batch_size,) + shape[1:])
>             new_data_shapes = tuple(new_data_shapes)
>         else:
>             new_data_shapes = tuple(i.shape for i in data_batch.data)
> 
>         if curr_data_shapes != new_data_shapes:
>             if hasattr(data_batch, "provide_data") and data_batch.provide_data:
>                 new_dshape = data_batch.provide_data
>             else:
>                 new_dshape = [DataDesc(i.name, shape, i.dtype, i.layout) \
>                               for i, shape in zip(self._data_shapes, new_data_shapes)]
> 
>             if hasattr(data_batch, "provide_label") and data_batch.provide_label:
>                 new_lshape = data_batch.provide_label
>             elif hasattr(data_batch, "label") and data_batch.label:
>                 new_lshape = [DataDesc(i.name, j.shape, i.dtype, i.layout) \
>                               for i, j in zip(self._label_shapes, data_batch.label)]
>             else:
>                 new_lshape = None
> 
>             self.reshape(new_dshape, new_lshape)
> 
>         self._exec_group.forward(data_batch, is_train)
>         #for i in range(len(self._exec_group.param_arrays)):
>         #    print(i)
>         #    print(self._exec_group.param_arrays[i])
>         #print(len(self._exec_group.param_arrays))
> 
>     def backward(self, out_grads=None):
>         """Backward computation.
> 
>         See Also
>         ----------
>         :meth:`BaseModule.backward`.
> 
>         Parameters
>         ----------
>         out_grads : NDArray or list of NDArray, optional
>             Gradient on the outputs to be propagated back.
>             This parameter is only needed when bind is called
>             on outputs that are not a loss function.
>         """
>         assert self.binded and self.params_initialized
>         self._exec_group.backward(out_grads=out_grads)
> 
>     def update(self):
>         """Updates parameters according to the installed optimizer and the gradients computed
>         in the previous forward-backward batch.
> 
>         When KVStore is used to update parameters for multi-device or multi-machine training,
>         a copy of the parameters are stored in KVStore. Note that for `row_sparse` parameters,
>         this function does update the copy of parameters in KVStore, but doesn't broadcast the
>         updated parameters to all devices / machines. Please call `prepare` to broadcast
>         `row_sparse` parameters with the next batch of data.
> 
>         See Also
>         ----------
>         :meth:`BaseModule.update`.
>         """
>         assert self.binded and self.params_initialized and self.optimizer_initialized
> 
>         self._params_dirty = True
>         if self._update_on_kvstore:
>             _update_params_on_kvstore(self._exec_group.param_arrays,
>                                       self._exec_group.grad_arrays,
>                                       self._kvstore, self._exec_group.param_names)
>         else:
>             _update_params(self._exec_group.param_arrays,
>                            self._exec_group.grad_arrays,
>                            updater=self._updater,
>                            num_device=len(self._context),
>                            kvstore=self._kvstore,
>                            param_names=self._exec_group.param_names)
> 
>     def get_outputs(self, merge_multi_context=True):
>         """Gets outputs of the previous forward computation.
> 
>         If ``merge_multi_context`` is ``True``, it is like ``[out1, out2]``. Otherwise, it
>         is like ``[[out1_dev1, out1_dev2], [out2_dev1, out2_dev2]]``. All the output
>         elements are `NDArray`. When `merge_multi_context` is `False`, those `NDArray`
>         might live on different devices.
> 
>         Parameters
>         ----------
>         merge_multi_context : bool
>             Default is ``True``. In the case when data-parallelism is used, the outputs
>             will be collected from multiple devices. A ``True`` value indicate that we
>             should merge the collected results so that they look like from a single
>             executor.
> 
>         Returns
>         -------
>         list of NDArray or list of list of NDArray
>             Output.
>         """
>         assert self.binded and self.params_initialized
>         return self._exec_group.get_outputs(merge_multi_context=merge_multi_context)
> 
>     def get_input_grads(self, merge_multi_context=True):
>         """Gets the gradients with respect to the inputs of the module.
> 
>         If ``merge_multi_context`` is ``True``, it is like ``[grad1, grad2]``. Otherwise, it
>         is like ``[[grad1_dev1, grad1_dev2], [grad2_dev1, grad2_dev2]]``. All the output
>         elements are `NDArray`.
> 
>         Parameters
>         ----------
>         merge_multi_context : bool
>             Default is ``True``. In the case when data-parallelism is used, the outputs
>             will be collected from multiple devices. A ``True`` value indicate that we
>             should merge the collected results so that they look like from a single
>             executor.
> 
>         Returns
>         -------
>         list of NDArray or list of list of NDArray
>               Input gradients
>         """
>         assert self.binded and self.params_initialized and self.inputs_need_grad
>         return self._exec_group.get_input_grads(merge_multi_context=merge_multi_context)
> 
>     def get_states(self, merge_multi_context=True):
>         """Gets states from all devices.
> 
>         If `merge_multi_context` is ``True``, it is like ``[out1, out2]``. Otherwise, it
>         is like ``[[out1_dev1, out1_dev2], [out2_dev1, out2_dev2]]``. All the output
>         elements are `NDArray`.
> 
>         Parameters
>         ----------
>         merge_multi_context : bool
>             Default is ``True``. In the case when data-parallelism is used, the states
>             will be collected from multiple devices. A ``True`` value indicate that we
>             should merge the collected results so that they look like from a single
>             executor.
> 
>         Returns
>         -------
>         list of NDArray or list of list of NDArray
>             States
>         """
>         assert self.binded and self.params_initialized
>         return self._exec_group.get_states(merge_multi_context=merge_multi_context)
> 
>     def set_states(self, states=None, value=None):
>         """Sets value for states. Only one of the states & value can be specified.
> 
>         Parameters
>         ----------
>         states : list of list of NDArrays
>             source states arrays formatted like ``[[state1_dev1, state1_dev2],
>             [state2_dev1, state2_dev2]]``.
>         value : number
>             a single scalar value for all state arrays.
>         """
>         assert self.binded and self.params_initialized
>         self._exec_group.set_states(states, value)
> 
>     def update_metric(self, eval_metric, labels, pre_sliced=False):
>         """Evaluates and accumulates evaluation metric on outputs of the last forward computation.
> 
>         See Also
>         ----------
>         :meth:`BaseModule.update_metric`.
> 
>         Parameters
>         ----------
>         eval_metric : EvalMetric
>             Evaluation metric to use.
>         labels : list of NDArray if `pre_sliced` parameter is set to `False`,
>             list of lists of NDArray otherwise. Typically `data_batch.label`.
>         pre_sliced: bool
>             Whether the labels are already sliced per device (default: False).
>         """
>         self._exec_group.update_metric(eval_metric, labels, pre_sliced)
> 
>     def _sync_params_from_devices(self):
>         """Synchronizes parameters from devices to CPU. This function should be called after
>         calling `update` that updates the parameters on the devices, before one can read the
>         latest parameters from ``self._arg_params`` and ``self._aux_params``.
> 
>         For row_sparse parameters on devices, ther are pulled from KVStore with all row ids.
> 
>         """
>         self._exec_group.get_params(self._arg_params, self._aux_params)
>         if self._kvstore and self._update_on_kvstore:
>             for param_name, param_val in sorted(self._arg_params.items()):
>                 if param_val.stype == 'row_sparse':
>                     row_ids = nd.arange(0, param_val.shape[0], dtype='int64')
>                     self._kvstore.row_sparse_pull(param_name, param_val, row_ids=row_ids)
>         self._params_dirty = False
> 
>     def save_optimizer_states(self, fname):
>         """Saves optimizer (updater) state to a file.
> 
>         Parameters
>         ----------
>         fname : str
>             Path to output states file.
>         """
>         assert self.optimizer_initialized
> 
>         if self._update_on_kvstore:
>             self._kvstore.save_optimizer_states(fname)
>         else:
>             with open(fname, 'wb') as fout:
>                 fout.write(self._updater.get_states())
> 
>     def load_optimizer_states(self, fname):
>         """Loads optimizer (updater) state from a file.
> 
>         Parameters
>         ----------
>         fname : str
>             Path to input states file.
>         """
>         assert self.optimizer_initialized
> 
>         if self._update_on_kvstore:
>             self._kvstore.load_optimizer_states(fname)
>         else:
>             self._updater.set_states(open(fname, 'rb').read())
> 
>     def install_monitor(self, mon):
>         """Installs monitor on all executors. """
>         assert self.binded
>         self._exec_group.install_monitor(mon)
> 
>     def prepare(self, data_batch, sparse_row_id_fn=None):
>         '''Prepares the module for processing a data batch.
> 
>         Usually involves switching bucket and reshaping.
>         For modules that contain `row_sparse` parameters in KVStore,
>         it prepares the `row_sparse` parameters based on the sparse_row_id_fn.
> 
>         When KVStore is used to update parameters for multi-device or multi-machine training,
>         a copy of the parameters are stored in KVStore. Note that for `row_sparse` parameters,
>         the `update()` updates the copy of parameters in KVStore, but doesn't broadcast
>         the updated parameters to all devices / machines. The `prepare` function is used to
>         broadcast `row_sparse` parameters with the next batch of data.
> 
>         Parameters
>         ----------
>         data_batch : DataBatch
>             The current batch of data for forward computation.
> 
>         sparse_row_id_fn : A callback function
>             The function  takes `data_batch` as an input and returns a dict of
>             str -> NDArray. The resulting dict is used for pulling row_sparse
>             parameters from the kvstore, where the str key is the name of the param,
>             and the value is the row id of the param to pull.
>         '''
>         assert self.binded
>         if sparse_row_id_fn is not None:
>             if not self._kvstore or not self._update_on_kvstore:
>                 warnings.warn(UserWarning("Parameters are not updated in the KVStore. "
>                                           "No need to call sparse_row_id_fn."))
>             else:
>                 row_ids = sparse_row_id_fn(data_batch)
>                 assert(isinstance(row_ids, dict)), "Expected dict output from sparse_row_id_fn"
>                 for param_name, row_id in row_ids.items():
>                     param_idx = self._exec_group.param_names.index(param_name)
>                     param_val = self._exec_group.param_arrays[param_idx]
>                     assert(isinstance(param_val, (tuple, list)))
>                     if param_val[0].stype != 'row_sparse':
>                         warnings.warn(UserWarning("%s.stype is not 'row_sparse'. No need to "
>                                                   "perform row_sparse_pull." % param_name))
>                     else:
>                         self._kvstore.row_sparse_pull(param_name, param_val, row_ids=row_id,
>                                                       priority=-param_idx)
diff -r /home/xxx/diff/README.md /home/xxx/yyy/incubator-mxnet/README.md
1,2c1,7
< # Mxnet Training CNN with multiple virtual CPU synchronous SGD
< Devide the physical CPU to multiple virtual CPU, then doing a synchronous SGD. This method can speed up classification network training on Intel CPU. In addition, this method can also be applied to arm.
---
> <!--- Licensed to the Apache Software Foundation (ASF) under one -->
> <!--- or more contributor license agreements.  See the NOTICE file -->
> <!--- distributed with this work for additional information -->
> <!--- regarding copyright ownership.  The ASF licenses this file -->
> <!--- to you under the Apache License, Version 2.0 (the -->
> <!--- "License"); you may not use this file except in compliance -->
> <!--- with the License.  You may obtain a copy of the License at -->
4c9
< For example, if the CPU has 28 cores, we can divide it into 4 workers each with 7 cores and do a synchronos SGD. We use shared memory IPC instead of Parameter Server to achieve communication between workers, which is faster than PS on single node. We do experiments on INTEL XEON E5-2690 v4 CPU, which can gain an 1.5x speed up when training both Resnet and Mobilenet.
---
> <!---   http://www.apache.org/licenses/LICENSE-2.0 -->
6c11,16
< To run this, you need to copy files above to overwrite the previous ones on MxNet. Total throughput is the sum of all workers' throughput. 
---
> <!--- Unless required by applicable law or agreed to in writing, -->
> <!--- software distributed under the License is distributed on an -->
> <!--- "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->
> <!--- KIND, either express or implied.  See the License for the -->
> <!--- specific language governing permissions and limitations -->
> <!--- under the License. -->
8c18,116
< Read the bash 'run.sh' for more informations.
---
> <div align="center">
>   <a href="https://mxnet.incubator.apache.org/"><img src="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/mxnet_logo_2.png"></a><br>
> </div>
> 
> Apache MXNet (incubating) for Deep Learning
> =====
> | Master         | Docs          | License  |
> | :-------------:|:-------------:|:--------:|
> | [![Build Status](http://jenkins.mxnet-ci.amazon-ml.com/job/incubator-mxnet/job/master/badge/icon)](http://jenkins.mxnet-ci.amazon-ml.com/job/incubator-mxnet/job/master/)  | [![Documentation Status](http://jenkins.mxnet-ci.amazon-ml.com/job/restricted-website-build/badge/icon)](https://mxnet.incubator.apache.org/) | [![GitHub license](http://dmlc.github.io/img/apache2.svg)](./LICENSE) |
> 
> ![banner](https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/banner.png)
> 
> Apache MXNet (incubating) is a deep learning framework designed for both *efficiency* and *flexibility*.
> It allows you to ***mix*** [symbolic and imperative programming](https://mxnet.incubator.apache.org/architecture/index.html#deep-learning-system-design-concepts)
> to ***maximize*** efficiency and productivity.
> At its core, MXNet contains a dynamic dependency scheduler that automatically parallelizes both symbolic and imperative operations on the fly.
> A graph optimization layer on top of that makes symbolic execution fast and memory efficient.
> MXNet is portable and lightweight, scaling effectively to multiple GPUs and multiple machines.
> 
> MXNet is more than a deep learning project. It is a collection of
> [blue prints and guidelines](https://mxnet.incubator.apache.org/architecture/index.html#deep-learning-system-design-concepts) for building
> deep learning systems, and interesting insights of DL systems for hackers.
> 
> Ask Questions
> -------------
> * Please use [discuss.mxnet.io](https://discuss.mxnet.io/) for asking questions.
> * Please use [mxnet/issues](https://github.com/apache/incubator-mxnet/issues) for reporting bugs.
> * [Frequent Asked Questions](https://mxnet.incubator.apache.org/faq/faq.html)
> 
> How to Contribute
> -----------------
> * [Contribute to MXNet](https://mxnet.incubator.apache.org/community/contribute.html)
> 
> What's New
> ----------
> * [Version 1.4.1 Release](https://github.com/apache/incubator-mxnet/releases/tag/1.4.1) - MXNet 1.4.1 Patch Release.
> * [Version 1.4.0 Release](https://github.com/apache/incubator-mxnet/releases/tag/1.4.0) - MXNet 1.4.0 Release.
> * [Version 1.3.1 Release](https://github.com/apache/incubator-mxnet/releases/tag/1.3.1) - MXNet 1.3.1 Patch Release.
> * [Version 1.3.0 Release](https://github.com/apache/incubator-mxnet/releases/tag/1.3.0) - MXNet 1.3.0 Release.
> * [Version 1.2.0 Release](https://github.com/apache/incubator-mxnet/releases/tag/1.2.0) - MXNet 1.2.0 Release.
> * [Version 1.1.0 Release](https://github.com/apache/incubator-mxnet/releases/tag/1.1.0) - MXNet 1.1.0 Release.
> * [Version 1.0.0 Release](https://github.com/apache/incubator-mxnet/releases/tag/1.0.0) - MXNet 1.0.0 Release.
> * [Version 0.12.1 Release](https://github.com/apache/incubator-mxnet/releases/tag/0.12.1) - MXNet 0.12.1 Patch Release.
> * [Version 0.12.0 Release](https://github.com/apache/incubator-mxnet/releases/tag/0.12.0) - MXNet 0.12.0 Release.
> * [Version 0.11.0 Release](https://github.com/apache/incubator-mxnet/releases/tag/0.11.0) - MXNet 0.11.0 Release.
> * [Apache Incubator](http://incubator.apache.org/projects/mxnet.html) - We are now an Apache Incubator project.
> * [Version 0.10.0 Release](https://github.com/dmlc/mxnet/releases/tag/v0.10.0) - MXNet 0.10.0 Release.
> * [Version 0.9.3 Release](./docs/architecture/release_note_0_9.md) - First 0.9 official release.
> * [Version 0.9.1 Release (NNVM refactor)](./docs/architecture/release_note_0_9.md) - NNVM branch is merged into master now. An official release will be made soon.
> * [Version 0.8.0 Release](https://github.com/dmlc/mxnet/releases/tag/v0.8.0)
> * [Updated Image Classification with new Pre-trained Models](./example/image-classification)
> * [Notebooks How to Use MXNet](https://github.com/d2l-ai/d2l-en)
> * [MKLDNN for Faster CPU Performance](./docs/tutorials/mkldnn/MKLDNN_README.md)
> * [MXNet Memory Monger, Training Deeper Nets with Sublinear Memory Cost](https://github.com/dmlc/mxnet-memonger)
> * [Tutorial for NVidia GTC 2016](https://github.com/dmlc/mxnet-gtc-tutorial)
> * [Embedding Torch layers and functions in MXNet](https://mxnet.incubator.apache.org/faq/torch.html)
> * [MXNet.js: Javascript Package for Deep Learning in Browser (without server)
> ](https://github.com/dmlc/mxnet.js/)
> * [Design Note: Design Efficient Deep Learning Data Loading Module](https://mxnet.incubator.apache.org/architecture/note_data_loading.html)
> * [MXNet on Mobile Device](https://mxnet.incubator.apache.org/faq/smart_device.html)
> * [Distributed Training](https://mxnet.incubator.apache.org/faq/multi_devices.html)
> * [Guide to Creating New Operators (Layers)](https://mxnet.incubator.apache.org/faq/new_op.html)
> * [Go binding for inference](https://github.com/songtianyi/go-mxnet-predictor)
> * [Amalgamation and Go Binding for Predictors](https://github.com/jdeng/gomxnet/) - Outdated
> * [Large Scale Image Classification](https://github.com/apache/incubator-mxnet/tree/master/example/image-classification)
> 
> Contents
> --------
> * [Documentation](https://mxnet.incubator.apache.org/) and  [Tutorials](https://mxnet.incubator.apache.org/tutorials/)
> * [Design Notes](https://mxnet.incubator.apache.org/architecture/index.html)
> * [Code Examples](https://github.com/apache/incubator-mxnet/tree/master/example)
> * [Installation](https://mxnet.incubator.apache.org/install/index.html)
> * [Pretrained Models](http://mxnet.incubator.apache.org/api/python/gluon/model_zoo.html)
> 
> Features
> --------
> * Design notes providing useful insights that can re-used by other DL projects
> * Flexible configuration for arbitrary computation graph
> * Mix and match imperative and symbolic programming to maximize flexibility and efficiency
> * Lightweight, memory efficient and portable to smart devices
> * Scales up to multi GPUs and distributed setting with auto parallelism
> * Support for [Python](https://github.com/apache/incubator-mxnet/tree/master/python), [Scala](https://github.com/apache/incubator-mxnet/tree/master/scala-package), [C++](https://github.com/apache/incubator-mxnet/tree/master/cpp-package), [Java](https://github.com/apache/incubator-mxnet/tree/master/scala-package), [Clojure](https://github.com/apache/incubator-mxnet/tree/master/contrib/clojure-package), [R](https://github.com/apache/incubator-mxnet/tree/master/R-package), [Go](https://github.com/jdeng/gomxnet/), [Javascript](https://github.com/dmlc/mxnet.js/), [Perl](https://github.com/apache/incubator-mxnet/tree/master/perl-package), [Matlab](https://github.com/apache/incubator-mxnet/tree/master/matlab), and [Julia](https://github.com/apache/incubator-mxnet/tree/master/julia)
> * Cloud-friendly and directly compatible with S3, HDFS, and Azure
> 
> License
> -------
> Licensed under an [Apache-2.0](https://github.com/apache/incubator-mxnet/blob/master/LICENSE) license.
> 
> Reference Paper
> ---------------
> 
> Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao,
> Bing Xu, Chiyuan Zhang, and Zheng Zhang.
> [MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems](https://github.com/dmlc/web-data/raw/master/mxnet/paper/mxnet-learningsys.pdf).
> In Neural Information Processing Systems, Workshop on Machine Learning Systems, 2015
> 
> History
> -------
> MXNet emerged from a collaboration by the authors of [cxxnet](https://github.com/dmlc/cxxnet), [minerva](https://github.com/dmlc/minerva), and [purine2](https://github.com/purine/purine2). The project reflects what we have learned from the past projects. MXNet combines aspects of each of these projects to achieve flexibility, speed, and memory efficiency.
Only in /home/xxx/diff/: run.sh
diff -r /home/xxx/diff/src/ndarray/ndarray.cc /home/xxx/yyy/incubator-mxnet/src/ndarray/ndarray.cc
98a99,105
> void NDArray::SetShapeFromChunk() {
>   if (Imperative::Get()->is_np_shape() ||
>       !(ptr_->storage_shape.ndim() == 1 && ptr_->storage_shape[0] == 0)) {
>     shape_ = ptr_->storage_shape;
>   }
> }
> 
110,112d116
<   return;
< //  if (!this->is_shared)
<   //  this->shandle.dptr = this->true_handle;
318c322
<   if (shape_.ndim() > 1) {
---
>   if (shape_.ndim() > 1 || Imperative::Get()->is_np_shape()) {
401d404
<   //LOG(INFO) << storage_type << " st " << kDefaultStorage;
431c434
<   // If the memory already uses the speciified layout, don't do anything.
---
>   // If the memory already uses the specified layout, don't do anything.
468,481d470
< void NDArray::SetSharedMem(const mkldnn::memory::primitive_desc &pd, void* handle){
<   ptr_->mkl_mem_ = std::make_shared< MKLDNNMemory>(pd, handle, 1);
<   return;
< }
< 
< int NDArray::TimesVisit() const{
<   return ptr_->mkl_mem_->TimesVisit();
< }
< 
< void NDArray::Visit() const{
<   ptr_->mkl_mem_->Visit();
<   return;
< }
< 
646d634
< 
650,651d637
<   //LOG(INFO) << " is   " << IsView();
<   if (ptr_ != nullptr && ptr_->mkl_mem_!=nullptr && (ptr_->mkl_mem_->TimesVisit() > 0)) return  ptr_->mkl_mem_->GetRaw();
680d665
<     //LOG(INFO) <<" new ";
1230c1215,1218
<   if (from.shape().Size() == 0U) return;
---
>   // zero-size array, no need to copy
>   if (from.shape().Size() == 0U) {
>     return;
>   }
1750c1738
<            " to scope of the code of loading the ndarray.";
---
>            " to scope the code of loading the ndarray.";
1889a1878,1881
>   // zero-size array, no need to copy
>   if (size == 0U) {
>     return;
>   }
2020a2013,2016
>   // zero-size array, no need to copy
>   if (size == 0U) {
>     return;
>   }
2116a2113
> .add_alias("_npi_copyto")
diff -r /home/xxx/diff/src/operator/nn/mkldnn/mkldnn_convolution.cc /home/xxx/yyy/incubator-mxnet/src/operator/nn/mkldnn/mkldnn_convolution.cc
29,34d28
< #define FILENAME "/dev/null"
< 
< #include <unistd.h>
< #include <sys/types.h>
< #include <sys/ipc.h>
< #include <sys/shm.h>
39,40d32
< #include <string>
< #include <stdlib.h>
396d387
< 
402c393
<   if (weight.IsView() && weight.IsMKLDNNData()){
---
>   if (weight.IsView() && weight.IsMKLDNNData())
404d394
<   }
418c408
<       weight_mem = GetWeights(weight, fwd->fwd_pd.weights_primitive_desc(),
---
>     weight_mem = GetWeights(weight, fwd->fwd_pd.weights_primitive_desc(),
423d412
<     //LOG(INFO) << weight.IsDefaultData();
435d423
<     //weight_mem = weight.GetMKLDNNData();
451,505d438
< //  LOG(INFO)<< weight.TimesVisit();
< /*
<   if (weight.TimesVisit() == 0){
<     key_t shm_key = MKLDNNStream::Get()->OpNum();
<     int shm_size = weight_mem->get_primitive_desc().get_size();
<     int shm_id = shmget(shm_key, 10000000, 0644 | IPC_CREAT);
<     if(shm_id == -1){
<       perror("shmget");
<       exit(EXIT_FAILURE);
<     }
<     void* d = (void*)shmat(shm_id, NULL, 0);
<     memcpy(d, weight_mem->get_data_handle(), shm_size);
<     weight_mem->set_data_handle(d);
<   }
<   weight.Visit();    
< */
< /*
<   if (data.TimesVisit() == 0){
<     LOG(INFO) << data.TimesVisit();
<     key_t shm_key = MKLDNNStream::Get()->OpNum();
<     int shm_size = data_mem->get_primitive_desc().get_size();
<     int shm_id = shmget(1, 10000000, 0644 | IPC_CREAT);
<     if(shm_id == -1){
<       perror("shmget");
<       exit(EXIT_FAILURE);
<     }
<     void* d = (void*)shmat(shm_id, NULL, 0);
<     //memcpy(d, data_mem->get_data_handle(), shm_size);
<     data_mem->set_data_handle(d);
<     }
<   data.Visit();
<   */                                          
<  /* 
<   if (data.TimesVisit() == 0){
<     LOG(INFO) << data.TimesVisit();
<     key_t shm_key = MKLDNNStream::Get()->OpNum();
<     int shm_size = data_mem->get_primitive_desc().get_size();
<     //int shm_id = shmget(shm_key, shm_size, 0644 | IPC_CREAT);
<     int shm_id = shmget(1, 10000000, 0644 | IPC_CREAT);
<     if(shm_id == -1){
<       perror("shmget");
<       exit(EXIT_FAILURE);
<     }
<     void* d = (void*)shmat(shm_id, 0, 0);
<     memcpy(d, data_mem->get_data_handle(), shm_size);
<     data_mem->set_data_handle(d);
<   }
< 
< 
<     data.Visit();
< */
< //  for (int i = 0; i < 10; i++)
<   //   LOG(INFO) << weight_mem->get_primitive_desc().get_size() <<  "    " << *(float *)(weight_mem->get_data_handle()+i*4);
< //  LOG(INFO) << "xx";
< 
508c441
<   
---
> 
577,579c510,512
< void SetWeightNewMem(const mkldnn::memory &data,
<                      const mkldnn::memory &out_grad,
<                      const mkldnn::memory &in_grad_weight) {
---
>   void SetWeightNewMem(const mkldnn::memory &data,
>                        const mkldnn::memory &out_grad,
>                        const mkldnn::memory &in_grad_weight) {
694,711d626
<  //LOG(INFO)<< weight.TimesVisit();
< //  weight.Visit();
< /*  if (!weight.IsShared()){
<   const mkldnn::memory *weight_mem;
<     weight_mem = weight.GetMKLDNNData();
<     key_t shm_key = MKLDNNStream::Get()->OpNum();
<     int shm_size = weight_mem->get_primitive_desc().get_size();
<     int shm_id = shmget(1, 100000000, 0644 | IPC_CREAT);
<     int sum_size = MKLDNNStream::Get()->ShareMemSize(shm_size);
<     if(shm_id == -1){
<       perror("shmget");
<       exit(EXIT_FAILURE);
<     }
<     void* d = (void*)shmat(shm_id, NULL, 0);
<     //memcpy(d + sum_size, weight_mem->get_data_handle(), shm_size);
<     weight.SetSharedMem(weight_mem->get_primitive_desc(), d);
<   }
< */
737c652
<   if (req[conv::kWeight]) {
---
>   if (req[conv::kWeight] || req[conv::kBias]) {
750d664
<     mkldnn_output_t in_grad_bias;
753c667
<                               *in_grad_weight.second);
---
>           *in_grad_weight.second);
756c670
<       in_grad_bias = CreateMKLDNNMem(
---
>       auto in_grad_bias = CreateMKLDNNMem(
760c674
<                               *in_grad_weight.second, *in_grad_bias.second);
---
>           *in_grad_weight.second, *in_grad_bias.second);
diff -r /home/xxx/diff/src/operator/nn/mkldnn/mkldnn_convolution-inl.h /home/xxx/yyy/incubator-mxnet/src/operator/nn/mkldnn/mkldnn_convolution-inl.h
73,79d72
< struct MKLDNNPostActParam {
<   mkldnn::algorithm alg = mkldnn::algorithm::algorithm_undef;
<   float scale = 1.f;
<   float alpha = 0.f;
<   float beta = 1.f;
< };
< 
85,86c78,79
<   MKLDNNPostActParam act_param;
<   MKLDNNPostActParam postsum_act_param;
---
>   MKLDNNPostEltwiseParam act_param;
>   MKLDNNPostEltwiseParam postsum_act_param;
diff -r /home/xxx/diff/src/operator/nn/mkldnn/mkldnn_fully_connected.cc /home/xxx/yyy/incubator-mxnet/src/operator/nn/mkldnn/mkldnn_fully_connected.cc
29,33d28
< #include <unistd.h>
< #include <sys/types.h>
< #include <sys/ipc.h>
< #include <sys/shm.h>
< 
52,56c47,51
<   if (full_param.mkldnn_param.with_relu) {
<     const float scale = 1.0f;
<     const float alpha = 0.0f;
<     const float beta = 1.0f;
<     ops.append_eltwise(scale, eltwise_relu, alpha, beta);
---
>   if (full_param.mkldnn_param.with_eltwise) {
>     ops.append_eltwise(full_param.eltwise_param.scale,
>                        full_param.eltwise_param.alg,
>                        full_param.eltwise_param.alpha,
>                        full_param.eltwise_param.beta);
251c246
<   //LOG(INFO) << " x";
---
> 
271,290d265
< /*
<   if (weight.TimesVisit() == 0){
<     key_t shm_key = MKLDNNStream::Get()->OpNum();
<     int shm_size = weight_mem->get_primitive_desc().get_size();
<     //LOG(INFO) << shm_key<< "  " <<shm_size <<"  " << data_mem->get_primitive_desc().get_size();
<     int shm_id = shmget(1, 10000000, 0644 | IPC_CREAT);
<     //        //int sum_size = MKLDNNStream::Get()->ShareMemSize(shm_size);
<     if(shm_id == -1){
<       perror("shmget");
<       exit(EXIT_FAILURE);
<     }
<     void* d = (void*)shmat(shm_id, NULL, 0);
<     memcpy(d, weight_mem->get_data_handle(), shm_size);
<     LOG(INFO) << shm_key << "  " << shm_size;
<     weight_mem->set_data_handle(d);
<     //weight.SetSharedMem(weight_mem->get_primitive_desc(), d);
<   }
<   weight.Visit();
< */
< //  LOG(INFO) << weight.TimesVisit();  
diff -r /home/xxx/diff/src/operator/nn/mkldnn/mkldnn_fully_connected-inl.h /home/xxx/yyy/incubator-mxnet/src/operator/nn/mkldnn/mkldnn_fully_connected-inl.h
43c43
<   bool with_relu;
---
>   bool with_eltwise;
52,53c52,53
<     DMLC_DECLARE_FIELD(with_relu).set_default(false)
<     .describe("Whether there's a post relu after FullyConnected operator");
---
>     DMLC_DECLARE_FIELD(with_eltwise).set_default(false)
>     .describe("Whether there's a post elemwise after FullyConnected operator");
69a70
>   MKLDNNPostEltwiseParam eltwise_param;
88d88
< 
diff -r /home/xxx/diff/src/operator/tensor/indexing_op.cc /home/xxx/yyy/incubator-mxnet/src/operator/tensor/indexing_op.cc
26,29d25
< #include <unistd.h>
< #include <sys/types.h>
< #include <sys/ipc.h>
< #include <sys/shm.h>
85d80
<   //weight.dptr_ = NULL;
88,89c83
< //  auto x = weight.dptr;
<   //LOG(INFO)<< " xx ";
---
> 
94c88
<       Tensor<cpu, 2, DType>  wmat = weight.get<cpu, 2, DType>(s);
---
>       Tensor<cpu, 2, DType> wmat = weight.get<cpu, 2, DType>(s);
109,128d102
<   //LOG(INFO) << " xx";
<   //
< /*
<   if (!weight.isshared()){
<       LOG(INFO) << " x" << weight.shape().Size();
<      //int shm_key = weight.shape().Size() % 100;
<      int shm_key = weight.shape().Size() % 100;
<      //if (weight.shape()[0] > 30000)
<      //   shm_key = 52; 
<      int shm_id = shmget(1, 50000000, 0644 | IPC_CREAT);
<      if(shm_id == -1){
<        perror("shmget");
<        exit(EXIT_FAILURE);
<      }
<      void* d = (void*)shmat(shm_id, NULL, 0);
<      memcpy(d, weight.data().dptr_, weight.shape().Size());
<      weight.shareTB(d);
<   }
< */
< 
494a469
> .add_alias("_npx_embedding")
547c522
< .set_num_outputs(1)
---
> .set_num_outputs(3)
552a528,531
> .set_attr<nnvm::FListOutputNames>("FListOutputNames",
>   [](const NodeAttrs& attrs) {
>     return std::vector<std::string>{"output", "min_output", "max_output"};
>   })
792a772
> .add_alias("_npx_one_hot")
841a822
> .add_alias("_npi_gather_nd")
1034a1016
> .add_alias("_npi_scatter_set_nd")
diff -r /home/xxx/diff/src/operator/tensor/indexing_op.h /home/xxx/yyy/incubator-mxnet/src/operator/tensor/indexing_op.h
57c57
< enum EmbeddingOpOutputs {kOut};
---
> enum EmbeddingOpOutputs {kOut, kMin, kMax};
159d158
< 
160a160,161
>   out_attrs->push_back(mxnet::TShape(1, 1));
>   out_attrs->push_back(mxnet::TShape(1, 1));
170c171
<   CHECK_GE(out_type->size(), 1U);
---
>   CHECK_GE(out_type->size(), 3U);
186a188,191
>   int dtype_out_min = 0;
>   int dtype_out_max = 0;
>   out_type->push_back(dtype_out_min);
>   out_type->push_back(dtype_out_max);
197c202
<   CHECK_EQ(out_attrs->size(), 1U);
---
>   CHECK_EQ(out_attrs->size(), 3U);
200a206,207
>   int& out_stype_min = out_attrs->at(embedding::kMin);
>   int& out_stype_max = out_attrs->at(embedding::kMax);
205a213,216
>     dispatched = storage_type_assign(&out_stype_min, kDefaultStorage,
>                                      dispatch_mode, DispatchMode::kFCompute);
>     dispatched = storage_type_assign(&out_stype_max, kDefaultStorage,
>                                      dispatch_mode, DispatchMode::kFCompute);
455c466
<   CHECK_EQ(outputs.size(), 1U);
---
>   CHECK_EQ(outputs.size(), 3U);
461d471
<   
463a474,477
>   float output_neg_min = -2.345f;
>   float output_pos_max = 2.345f;
>   outputs[embedding::kMin].dptr<float>()[0] = output_neg_min;
>   outputs[embedding::kMax].dptr<float>()[0] = output_pos_max;
477,488d490
< /*
< 
<   if (weight.IsView() && weight.IsMKLDNNData()){
<     weight = weight.Reorder2Default();
<   }
<   weight.Reorder2DefaultAsync();
< 
<    LOG(INFO) << " xx";
<   //weight.MKLDNNDataReorderAsync(fwd->fwd_pd.weights_primitive_desc());
< 
<   LOG(INFO) << (weight.TimesVisit());
<   //weight.Visit();*/
Only in /home/xxx/diff/example: image_classification
diff -r /home/xxx/diff/include/mxnet/ndarray.h /home/xxx/yyy/incubator-mxnet/include/mxnet/ndarray.h
193,197c193
<   void SetShapeFromChunk() {
<     if (!(ptr_->storage_shape.ndim() == 1 && ptr_->storage_shape[0] == 0)) {
<       shape_ = ptr_->storage_shape;
<     }
<   }
---
>   void SetShapeFromChunk();
200c196
<    * reshape or slice). If an array is a view and the the data is stored in
---
>    * reshape or slice). If an array is a view and the data is stored in
287,296d282
< 
<   const void shareTB(void *handle) const{
<     ptr_->shandle.dptr = handle;
<     ptr_->is_shared = true;
<   }
<   
<   const bool isshared() const{
<     return ptr_->is_shared;
<   }
<   
749,751d734
<   int TimesVisit() const;
<   void Visit() const;
<   void SetSharedMem(const mkldnn::memory::primitive_desc &pd, void* handle);
753,754c736
<   
< /*
---
>   /*
864,867d845
<     bool is_shared = false;
< 
< //    void* true_handle = NULL;
<     
889c867
<     Chunk() : static_data(true), delay_alloc(false), is_shared(false),
---
>     Chunk() : static_data(true), delay_alloc(false),
895c873
<         : static_data(false), delay_alloc(true), is_shared(false), ctx(ctx_),
---
>         : static_data(false), delay_alloc(true), ctx(ctx_),
910,911c888,889
<         : static_data(true), delay_alloc(false), is_shared(false),
<           storage_ref_(Storage::_GetSharedRef()), 
---
>         : static_data(true), delay_alloc(false),
>           storage_ref_(Storage::_GetSharedRef()),
929c907
<         : static_data(false), delay_alloc(false), is_shared(false),
---
>         : static_data(false), delay_alloc(false),
945c923
<         : static_data(false), delay_alloc(delay_alloc_), /*is_shared(false),*/ storage_type(storage_type_),
---
>         : static_data(false), delay_alloc(delay_alloc_), storage_type(storage_type_),
965c943
<         : static_data(true), delay_alloc(false), /*is_shared(false),*/ storage_type(storage_type_),
---
>         : static_data(true), delay_alloc(false), storage_type(storage_type_),
1074d1051
<     bool IsShared();
diff -r /home/xxx/diff/python/mxnet/model.py /home/xxx/yyy/incubator-mxnet/python/mxnet/model.py
1,1051c1,1036
< # Licensed to the Apache Software Foundation (ASF) under one
< # or more contributor license agreements.  See the NOTICE file
< # distributed with this work for additional information
< # regarding copyright ownership.  The ASF licenses this file
< # to you under the Apache License, Version 2.0 (the
< # "License"); you may not use this file except in compliance
< # with the License.  You may obtain a copy of the License at
< #
< #   http://www.apache.org/licenses/LICENSE-2.0
< #
< # Unless required by applicable law or agreed to in writing,
< # software distributed under the License is distributed on an
< # "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
< # KIND, either express or implied.  See the License for the
< # specific language governing permissions and limitations
< # under the License.
< 
< # pylint: disable=fixme, invalid-name, too-many-arguments, too-many-locals, too-many-lines
< # pylint: disable=too-many-branches, too-many-statements
< """MXNet model module"""
< from __future__ import absolute_import, print_function
< 
< import os
< import time
< import logging
< import warnings
< from collections import namedtuple
< import numpy as np
< import posix_ipc
< import mmap
< from . import io
< from . import ndarray as nd
< from . import symbol as sym
< from . import optimizer as opt
< from . import metric
< from . import kvstore as kvs
< from .context import Context, cpu
< from .initializer import Uniform
< from .optimizer import get_updater
< from .executor_manager import DataParallelExecutorManager, _check_arguments, _load_data
< from .io import DataDesc
< from .base import mx_real_t
< import mxnet as mx
< import logging as logger
< 
< BASE_ESTIMATOR = object
< 
< try:
<     from sklearn.base import BaseEstimator
<     BASE_ESTIMATOR = BaseEstimator
< except ImportError:
<     SKLEARN_INSTALLED = False
< 
< # Parameter to pass to batch_end_callback
< BatchEndParam = namedtuple('BatchEndParams',
<                            ['epoch',
<                             'nbatch',
<                             'eval_metric',
<                             'locals'])
< 
< def _create_kvstore(kvstore, num_device, arg_params):
<     """Create kvstore
<     This function select and create a proper kvstore if given the kvstore type.
< 
<     Parameters
<     ----------
<     kvstore : KVStore or str
<         The kvstore.
<     num_device : int
<         The number of devices
<     arg_params : dict of str to `NDArray`.
<         Model parameter, dict of name to `NDArray` of net's weights.
<     """
<     update_on_kvstore = True
<     if kvstore is None:
<         kv = None
<     elif isinstance(kvstore, kvs.KVStore):
<         kv = kvstore
<     elif isinstance(kvstore, str):
<         # create kvstore using the string type
<         if num_device is 1 and 'dist' not in kvstore:
<             # no need to use kv for single device and single machine
<             kv = None
<         else:
<             kv = kvs.create(kvstore)
<             if kvstore == 'local':
<             # automatically select a proper local
<                 max_size = max(np.prod(param.shape) for param in
<                                arg_params.values())
<                 if max_size > 1024 * 1024 * 16:
<                     update_on_kvstore = False
<     else:
<         raise TypeError('kvstore must be KVStore, str or None')
< 
<     if kv is None:
<         update_on_kvstore = False
< 
<     return (kv, update_on_kvstore)
< 
< def _initialize_kvstore(kvstore, param_arrays, arg_params, param_names, update_on_kvstore):
<     """Initialize kvstore"""
<     for idx, param_on_devs in enumerate(param_arrays):
<         name = param_names[idx]
<         kvstore.init(name, arg_params[name])
< 
<         if update_on_kvstore:
<             kvstore.pull(name, param_on_devs, priority=-idx)
< 
< def _update_params_on_kvstore_nccl(param_arrays, grad_arrays, kvstore, param_names):
<     """Perform update of param_arrays from grad_arrays on NCCL kvstore."""
<     valid_indices = [index for index, grad_list in
<                      enumerate(grad_arrays) if grad_list[0] is not None]
<     valid_grad_arrays = [grad_arrays[i] for i in valid_indices]
<     valid_param_arrays = [param_arrays[i] for i in valid_indices]
<     valid_param_names = [param_names[i] for i in valid_indices]
<     size = len(valid_grad_arrays)
<     start = 0
<     # Use aggregation by default only with NCCL
<     default_batch = 16
<     batch = int(os.getenv('MXNET_UPDATE_AGGREGATION_SIZE', default_batch))
<     while start < size:
<         end = start + batch if start + batch < size else size
<         # push gradient, priority is negative index
<         kvstore.push(valid_param_names[start:end], valid_grad_arrays[start:end], priority=-start)
<         # pull back the weights
<         kvstore.pull(valid_param_names[start:end], valid_param_arrays[start:end], priority=-start)
<         start = end
< 
< import time
< from mxnet.ndarray import ndarray as nnd
< #from ndarray import _new_from_shared_mem
< 
< def _update_params_on_kvstore(param_arrays, grad_arrays, kvstore, param_names):
<     """Perform update of param_arrays from grad_arrays on kvstore."""
<     for index, pair in enumerate(zip(param_arrays, grad_arrays)):
<         arg_list, grad_list = pair
<         if grad_list[0] is None:
<             continue
<         name = param_names[index]
<         # push gradient, priority is negative index
<         kvstore.push(name, grad_list, priority=-index)
<         # pull back the weights
<         kvstore.pull(name, arg_list, priority=-index)
< 
< def _init_shnd(param_arrays, grad_arrays, nd, is_chief):
< 
<     l = len(grad_arrays)
<     size = 24 * 2 * l
<     print(is_chief)
<     for i, pair in enumerate(zip(param_arrays, grad_arrays)):
<         arg_list, grad_list = pair
<         a,b,c,d = grad_list[0]._to_shared_mem()
< 
<         shm = posix_ipc.SharedMemory('/home/grad', posix_ipc.O_CREAT, size=size)
<         buf = mmap.mmap(shm.fd, size)
<         shape = (l, 2)
<         mx_shm_key = np.ndarray(shape, int, buf, order = 'C')
<         if is_chief == 0:
<             mx_shm_key[i][0] = a
<             mx_shm_key[i][1] = b
<         y = nnd._new_from_shared_mem(mx_shm_key[i][0], mx_shm_key[i][1], grad_list[0].shape, np.float32)
<         nd.append(nnd.NDArray(y))
<         if is_chief == 0:
<             nd[i].zeros_like()
< 
<     if is_chief == 0:
<         size = 12 * 24
<         shm = posix_ipc.SharedMemory('/home/sync', posix_ipc.O_CREAT, size=size)
<         buf = mmap.mmap(shm.fd, size)
<         shape = 12
<         sync = np.ndarray(shape, int, buf, order = 'C')
<         for i in range(shape):
<             sync[i] = 0
< 
< def _update_params(param_arrays, grad_arrays, work_id, updater, num_device,
<                    kvstore=None, param_names=None, nd = None):
<     """Perform update of param_arrays from grad_arrays not on kvstore."""
<     for i, pair in enumerate(zip(param_arrays, grad_arrays)):
<         arg_list, grad_list = pair
<         if grad_list[0] is None:
<             continue
<         nd[i] += grad_list[0]
< 
<     #sync by shared_memory
<     size = 12 * 24
<     shm = posix_ipc.SharedMemory('/home/sync', posix_ipc.O_CREAT, size = size)
<     buf = mmap.mmap(shm.fd, size)
<     shape = 12
<     sync = np.ndarray(shape, int, buf, order = 'C')
<     sync[work_id] += 1
< 
<     while (sync[0] != sync[1]) or (sync[0] != sync[3]) or (sync[0] != sync[2]):
<         pass
< 
<     for i, pair in enumerate(zip(param_arrays, grad_arrays)):
<         arg_list, grad_list = pair
<         if grad_list[0] is None:
<             continue
<         index = i
<         updater(index*num_device, nd[i], arg_list[0])
< 
<     sync[work_id + 4] += 1
<     while (sync[4] != sync[5]) or (sync[4] != sync[6]) or (sync[4] != sync[7]):
<         pass
< 
<     for i, pair in enumerate(zip(param_arrays, grad_arrays)):
<         arg_list, grad_list = pair
<         if grad_list[0] is None:
<             continue
<         nd[i].zeros_like()
<     sync[work_id + 8] += 1
< 
<     while (sync[8] != sync[9]) or (sync[8] != sync[10]) or (sync[8] != sync[11]):
<         pass
< 
< def _multiple_callbacks(callbacks, *args, **kwargs):
<     """Sends args and kwargs to any configured callbacks.
<     This handles the cases where the 'callbacks' variable
<     is ``None``, a single function, or a list.
<     """
<     if isinstance(callbacks, list):
<         for cb in callbacks:
<             cb(*args, **kwargs)
<         return
<     if callbacks:
<         callbacks(*args, **kwargs)
< 
< 
< def _train_multi_device(symbol, ctx, arg_names, param_names, aux_names,
<                         arg_params, aux_params,
<                         begin_epoch, end_epoch, epoch_size, optimizer,
<                         kvstore, update_on_kvstore,
<                         train_data, eval_data=None, eval_metric=None,
<                         epoch_end_callback=None, batch_end_callback=None,
<                         logger=None, work_load_list=None, monitor=None,
<                         eval_end_callback=None,
<                         eval_batch_end_callback=None, sym_gen=None):
<     """Internal training function on multiple devices.
<     This function will also work for single device as well.
< 
<     Parameters
<     ----------
<     symbol : Symbol
<         The network configuration.
<     ctx : list of Context
<         The training devices.
<     arg_names: list of str
<         Name of all arguments of the network.
<     param_names: list of str
<         Name of all trainable parameters of the network.
<     aux_names: list of str
<         Name of all auxiliary states of the network.
<     arg_params : dict of str to NDArray
<         Model parameter, dict of name to NDArray of net's weights.
<     aux_params : dict of str to NDArray
<         Model parameter, dict of name to NDArray of net's auxiliary states.
<     begin_epoch : int
<         The begining training epoch.
<     end_epoch : int
<         The end training epoch.
<     epoch_size : int, optional
<         Number of batches in a epoch. In default, it is set to
<         ``ceil(num_train_examples / batch_size)``.
<     optimizer : Optimizer
<         The optimization algorithm
<     train_data : DataIter
<         Training data iterator.
<     eval_data : DataIter
<         Validation data iterator.
<     eval_metric : EvalMetric
<         An evaluation function or a list of evaluation functions.
<     epoch_end_callback : callable(epoch, symbol, arg_params, aux_states)
<         A callback that is invoked at end of each epoch.
<         This can be used to checkpoint model each epoch.
<     batch_end_callback : callable(BatchEndParams)
<         A callback that is invoked at end of each batch.
<         This can be used to measure speed, get result from evaluation metric. etc.
<     kvstore : KVStore
<         The KVStore.
<     update_on_kvstore : bool
<         Whether or not perform weight updating on kvstore.
<     logger : logging logger
<         When not specified, default logger will be used.
<     work_load_list : list of float or int, optional
<         The list of work load for different devices,
<         in the same order as ``ctx``.
<     monitor : Monitor, optional
<         Monitor installed to executor,
<         for monitoring outputs, weights, and gradients for debugging.
<     Notes
<     -----
<     - This function will inplace update the NDArrays in `arg_params` and `aux_states`.
<     """
<     if logger is None:
<         logger = logging
<     executor_manager = DataParallelExecutorManager(symbol=symbol,
<                                                    sym_gen=sym_gen,
<                                                    ctx=ctx,
<                                                    train_data=train_data,
<                                                    param_names=param_names,
<                                                    arg_names=arg_names,
<                                                    aux_names=aux_names,
<                                                    work_load_list=work_load_list,
<                                                    logger=logger)
<     if monitor:
<         executor_manager.install_monitor(monitor)
< 
<     executor_manager.set_params(arg_params, aux_params)
< 
<     if not update_on_kvstore:
<         updater = get_updater(optimizer)
<     else:
<         kvstore.set_optimizer(optimizer)
< 
<     if kvstore:
<         _initialize_kvstore(kvstore=kvstore,
<                             param_arrays=executor_manager.param_arrays,
<                             arg_params=arg_params,
<                             param_names=executor_manager.param_names,
<                             update_on_kvstore=update_on_kvstore)
< 
<     # Now start training
<     train_data.reset()
<     for epoch in range(begin_epoch, end_epoch):
<         # Training phase
<         tic = time.time()
<         eval_metric.reset()
<         nbatch = 0
<         # Iterate over training data.
<         while True:
<             do_reset = True
<             for data_batch in train_data:
<                 executor_manager.load_data_batch(data_batch)
< 
<                 if monitor is not None:
<                     monitor.tic()
< 
<                 executor_manager.forward(is_train=True)
<                 executor_manager.backward()
< 
<                 if update_on_kvstore:
<                     if 'nccl' in kvstore.type:
<                         _update_params_on_kvstore_nccl(executor_manager.param_arrays,
<                                                        executor_manager.grad_arrays,
<                                                        kvstore, executor_manager.param_names)
<                     else:
<                         _update_params_on_kvstore(executor_manager.param_arrays,
<                                                   executor_manager.grad_arrays,
<                                                   kvstore, executor_manager.param_names)
<                 else:
<                     print('fsfs')
<                     _update_params(executor_manager.param_arrays,
<                                    executor_manager.grad_arrays,
<                                    updater=updater,
<                                    num_device=len(ctx),
<                                    kvstore=kvstore,
<                                    param_names=executor_manager.param_names)
< 
<                 if monitor is not None:
<                     monitor.toc_print()
< 
<                 # evaluate at end, so we can lazy copy
<                 executor_manager.update_metric(eval_metric, data_batch.label)
< 
<                 nbatch += 1
<                 # batch callback (for print purpose)
<                 if batch_end_callback is not None:
<                     batch_end_params = BatchEndParam(epoch=epoch,
<                                                      nbatch=nbatch,
<                                                      eval_metric=eval_metric,
<                                                      locals=locals())
<                     _multiple_callbacks(batch_end_callback, batch_end_params)
< 
<                 # this epoch is done possibly earlier
<                 if epoch_size is not None and nbatch >= epoch_size:
<                     do_reset = False
<                     break
< 
<             if do_reset:
<                 logger.info('Epoch[%d] Resetting Data Iterator', epoch)
<                 train_data.reset()
< 
<             # this epoch is done
<             if epoch_size is None or nbatch >= epoch_size:
<                 break
< 
<         toc = time.time()
<         logger.info('Epoch[%d] Time cost=%.3f', epoch, (toc - tic))
< 
<         if epoch_end_callback or epoch + 1 == end_epoch:
<             executor_manager.copy_to(arg_params, aux_params)
< 
<         _multiple_callbacks(epoch_end_callback, epoch, symbol, arg_params, aux_params)
< 
<         # evaluation
<         if eval_data:
<             eval_metric.reset()
<             eval_data.reset()
<             total_num_batch = 0
<             for i, eval_batch in enumerate(eval_data):
<                 executor_manager.load_data_batch(eval_batch)
<                 executor_manager.forward(is_train=False)
<                 executor_manager.update_metric(eval_metric, eval_batch.label)
<                 if eval_batch_end_callback is not None:
<                     batch_end_params = BatchEndParam(epoch=epoch,
<                                                      nbatch=i,
<                                                      eval_metric=eval_metric,
<                                                      locals=locals())
<                     _multiple_callbacks(eval_batch_end_callback, batch_end_params)
<                 total_num_batch += 1
<             if eval_end_callback is not None:
<                 eval_end_params = BatchEndParam(epoch=epoch,
<                                                 nbatch=total_num_batch,
<                                                 eval_metric=eval_metric,
<                                                 locals=locals())
<                 _multiple_callbacks(eval_end_callback, eval_end_params)
<             eval_data.reset()
<     # end of all epochs
<     return
< 
< 
< def save_checkpoint(prefix, epoch, symbol, arg_params, aux_params):
<     """Checkpoint the model data into file.
< 
<     Parameters
<     ----------
<     prefix : str
<         Prefix of model name.
<     epoch : int
<         The epoch number of the model.
<     symbol : Symbol
<         The input Symbol.
<     arg_params : dict of str to NDArray
<         Model parameter, dict of name to NDArray of net's weights.
<     aux_params : dict of str to NDArray
<         Model parameter, dict of name to NDArray of net's auxiliary states.
<     Notes
<     -----
<     - ``prefix-symbol.json`` will be saved for symbol.
<     - ``prefix-epoch.params`` will be saved for parameters.
<     """
<     if symbol is not None:
<         symbol.save('%s-symbol.json' % prefix)
< 
<     save_dict = {('arg:%s' % k) : v.as_in_context(cpu()) for k, v in arg_params.items()}
<     save_dict.update({('aux:%s' % k) : v.as_in_context(cpu()) for k, v in aux_params.items()})
<     param_name = '%s-%04d.params' % (prefix, epoch)
<     nd.save(param_name, save_dict)
<     logging.info('Saved checkpoint to \"%s\"', param_name)
< 
< 
< def load_checkpoint(prefix, epoch):
<     """Load model checkpoint from file.
< 
<     Parameters
<     ----------
<     prefix : str
<         Prefix of model name.
<     epoch : int
<         Epoch number of model we would like to load.
< 
<     Returns
<     -------
<     symbol : Symbol
<         The symbol configuration of computation network.
<     arg_params : dict of str to NDArray
<         Model parameter, dict of name to NDArray of net's weights.
<     aux_params : dict of str to NDArray
<         Model parameter, dict of name to NDArray of net's auxiliary states.
< 
<     Notes
<     -----
<     - Symbol will be loaded from ``prefix-symbol.json``.
<     - Parameters will be loaded from ``prefix-epoch.params``.
<     """
<     symbol = sym.load('%s-symbol.json' % prefix)
<     save_dict = nd.load('%s-%04d.params' % (prefix, epoch))
<     arg_params = {}
<     aux_params = {}
<     for k, v in save_dict.items():
<         tp, name = k.split(':', 1)
<         if tp == 'arg':
<             arg_params[name] = v
<         if tp == 'aux':
<             aux_params[name] = v
<     return (symbol, arg_params, aux_params)
< 
< from .callback import LogValidationMetricsCallback # pylint: disable=wrong-import-position
< 
< class FeedForward(BASE_ESTIMATOR):
<     """Model class of MXNet for training and predicting feedforward nets.
<     This class is designed for a single-data single output supervised network.
< 
<     Parameters
<     ----------
<     symbol : Symbol
<         The symbol configuration of computation network.
<     ctx : Context or list of Context, optional
<         The device context of training and prediction.
<         To use multi GPU training, pass in a list of gpu contexts.
<     num_epoch : int, optional
<         Training parameter, number of training epochs(epochs).
<     epoch_size : int, optional
<         Number of batches in a epoch. In default, it is set to
<         ``ceil(num_train_examples / batch_size)``.
<     optimizer : str or Optimizer, optional
<         Training parameter, name or optimizer object for training.
<     initializer : initializer function, optional
<         Training parameter, the initialization scheme used.
<     numpy_batch_size : int, optional
<         The batch size of training data.
<         Only needed when input array is numpy.
<     arg_params : dict of str to NDArray, optional
<         Model parameter, dict of name to NDArray of net's weights.
<     aux_params : dict of str to NDArray, optional
<         Model parameter, dict of name to NDArray of net's auxiliary states.
<     allow_extra_params : boolean, optional
<         Whether allow extra parameters that are not needed by symbol
<         to be passed by aux_params and ``arg_params``.
<         If this is True, no error will be thrown when ``aux_params`` and ``arg_params``
<         contain more parameters than needed.
<     begin_epoch : int, optional
<         The begining training epoch.
<     kwargs : dict
<         The additional keyword arguments passed to optimizer.
<     """
<     def __init__(self, symbol, ctx=None,
<                  num_epoch=None, epoch_size=None, optimizer='sgd',
<                  initializer=Uniform(0.01),
<                  numpy_batch_size=128,
<                  arg_params=None, aux_params=None,
<                  allow_extra_params=False,
<                  begin_epoch=0,
<                  **kwargs):
<         warnings.warn(
<             '\033[91mmxnet.model.FeedForward has been deprecated. ' + \
<             'Please use mxnet.mod.Module instead.\033[0m',
<             DeprecationWarning, stacklevel=2)
< 
<         if isinstance(symbol, sym.Symbol):
<             self.symbol = symbol
<             self.sym_gen = None
<         else:
<             assert(callable(symbol))
<             self.symbol = None
<             self.sym_gen = symbol
< 
<         # model parameters
<         self.arg_params = arg_params
<         self.aux_params = aux_params
<         self.allow_extra_params = allow_extra_params
< 
<         self.argument_checked = False
<         if self.sym_gen is None:
<             self._check_arguments()
< 
<         # basic configuration
<         if ctx is None:
<             ctx = [cpu()]
<         elif isinstance(ctx, Context):
<             ctx = [ctx]
<         self.ctx = ctx
<         # training parameters
<         self.num_epoch = num_epoch
<         self.epoch_size = epoch_size
<         self.kwargs = kwargs.copy()
<         self.optimizer = optimizer
<         self.initializer = initializer
<         self.numpy_batch_size = numpy_batch_size
<         # internal helper state
<         self._pred_exec = None
<         self.begin_epoch = begin_epoch
< 
<     def _check_arguments(self):
<         """verify the argument of the default symbol and user provided parameters"""
<         if self.argument_checked:
<             return
< 
<         assert(self.symbol is not None)
<         self.argument_checked = True
< 
<         # check if symbol contain duplicated names.
<         _check_arguments(self.symbol)
<         # rematch parameters to delete useless ones
<         if self.allow_extra_params:
<             if self.arg_params:
<                 arg_names = set(self.symbol.list_arguments())
<                 self.arg_params = {k : v for k, v in self.arg_params.items()
<                                    if k in arg_names}
<             if self.aux_params:
<                 aux_names = set(self.symbol.list_auxiliary_states())
<                 self.aux_params = {k : v for k, v in self.aux_params.items()
<                                    if k in aux_names}
< 
< 
<     @staticmethod
<     def _is_data_arg(name):
<         """Check if name is a data argument."""
<         return name.endswith('data') or name.endswith('label')
< 
<     def _init_params(self, inputs, overwrite=False):
<         """Initialize weight parameters and auxiliary states."""
<         inputs = [x if isinstance(x, DataDesc) else DataDesc(*x) for x in inputs]
<         input_shapes = {item.name: item.shape for item in inputs}
<         arg_shapes, _, aux_shapes = self.symbol.infer_shape(**input_shapes)
<         assert arg_shapes is not None
<         input_dtypes = {item.name: item.dtype for item in inputs}
<         arg_dtypes, _, aux_dtypes = self.symbol.infer_type(**input_dtypes)
<         assert arg_dtypes is not None
< 
<         arg_names = self.symbol.list_arguments()
<         input_names = input_shapes.keys()
<         param_names = [key for key in arg_names if key not in input_names]
<         aux_names = self.symbol.list_auxiliary_states()
< 
<         param_name_attrs = [x for x in zip(arg_names, arg_shapes, arg_dtypes)
<                             if x[0] in param_names]
<         arg_params = {k : nd.zeros(shape=s, dtype=t)
<                       for k, s, t in param_name_attrs}
<         aux_name_attrs = [x for x in zip(aux_names, aux_shapes, aux_dtypes)
<                           if x[0] in aux_names]
<         aux_params = {k : nd.zeros(shape=s, dtype=t)
<                       for k, s, t in aux_name_attrs}
< 
<         for k, v in arg_params.items():
<             if self.arg_params and k in self.arg_params and (not overwrite):
<                 arg_params[k][:] = self.arg_params[k][:]
<             else:
<                 self.initializer(k, v)
< 
<         for k, v in aux_params.items():
<             if self.aux_params and k in self.aux_params and (not overwrite):
<                 aux_params[k][:] = self.aux_params[k][:]
<             else:
<                 self.initializer(k, v)
< 
<         self.arg_params = arg_params
<         self.aux_params = aux_params
<         return (arg_names, list(param_names), aux_names)
< 
<     def __getstate__(self):
<         this = self.__dict__.copy()
<         this['_pred_exec'] = None
<         return this
< 
<     def __setstate__(self, state):
<         self.__dict__.update(state)
< 
<     def _init_predictor(self, input_shapes, type_dict=None):
<         """Initialize the predictor module for running prediction."""
<         shapes = {name: self.arg_params[name].shape for name in self.arg_params}
<         shapes.update(dict(input_shapes))
<         if self._pred_exec is not None:
<             arg_shapes, _, _ = self.symbol.infer_shape(**shapes)
<             assert arg_shapes is not None, "Incomplete input shapes"
<             pred_shapes = [x.shape for x in self._pred_exec.arg_arrays]
<             if arg_shapes == pred_shapes:
<                 return
<         # for now only use the first device
<         pred_exec = self.symbol.simple_bind(
<             self.ctx[0], grad_req='null', type_dict=type_dict, **shapes)
<         pred_exec.copy_params_from(self.arg_params, self.aux_params)
< 
<         _check_arguments(self.symbol)
<         self._pred_exec = pred_exec
< 
<     def _init_iter(self, X, y, is_train):
<         """Initialize the iterator given input."""
<         if isinstance(X, (np.ndarray, nd.NDArray)):
<             if y is None:
<                 if is_train:
<                     raise ValueError('y must be specified when X is numpy.ndarray')
<                 else:
<                     y = np.zeros(X.shape[0])
<             if not isinstance(y, (np.ndarray, nd.NDArray)):
<                 raise TypeError('y must be ndarray when X is numpy.ndarray')
<             if X.shape[0] != y.shape[0]:
<                 raise ValueError("The numbers of data points and labels not equal")
<             if y.ndim == 2 and y.shape[1] == 1:
<                 y = y.flatten()
<             if y.ndim != 1:
<                 raise ValueError("Label must be 1D or 2D (with 2nd dimension being 1)")
<             if is_train:
<                 return io.NDArrayIter(X, y, min(X.shape[0], self.numpy_batch_size),
<                                       shuffle=is_train, last_batch_handle='roll_over')
<             else:
<                 return io.NDArrayIter(X, y, min(X.shape[0], self.numpy_batch_size), shuffle=False)
<         if not isinstance(X, io.DataIter):
<             raise TypeError('X must be DataIter, NDArray or numpy.ndarray')
<         return X
< 
<     def _init_eval_iter(self, eval_data):
<         """Initialize the iterator given eval_data."""
<         if eval_data is None:
<             return eval_data
<         if isinstance(eval_data, (tuple, list)) and len(eval_data) == 2:
<             if eval_data[0] is not None:
<                 if eval_data[1] is None and isinstance(eval_data[0], io.DataIter):
<                     return eval_data[0]
<                 input_data = (np.array(eval_data[0]) if isinstance(eval_data[0], list)
<                               else eval_data[0])
<                 input_label = (np.array(eval_data[1]) if isinstance(eval_data[1], list)
<                                else eval_data[1])
<                 return self._init_iter(input_data, input_label, is_train=True)
<             else:
<                 raise ValueError("Eval data is NONE")
<         if not isinstance(eval_data, io.DataIter):
<             raise TypeError('Eval data must be DataIter, or ' \
<                             'NDArray/numpy.ndarray/list pair (i.e. tuple/list of length 2)')
<         return eval_data
< 
<     def predict(self, X, num_batch=None, return_data=False, reset=True):
<         """Run the prediction, always only use one device.
< 
<         Parameters
<         ----------
<         X : mxnet.DataIter
<         num_batch : int or None
<             The number of batch to run. Go though all batches if ``None``.
<         Returns
<         -------
<         y : numpy.ndarray or a list of numpy.ndarray if the network has multiple outputs.
<             The predicted value of the output.
<         """
<         X = self._init_iter(X, None, is_train=False)
< 
<         if reset:
<             X.reset()
<         data_shapes = X.provide_data
<         data_names = [x[0] for x in data_shapes]
<         type_dict = dict((key, value.dtype) for (key, value) in self.arg_params.items())
<         for x in X.provide_data:
<             if isinstance(x, DataDesc):
<                 type_dict[x.name] = x.dtype
<             else:
<                 type_dict[x[0]] = mx_real_t
< 
<         self._init_predictor(data_shapes, type_dict)
<         batch_size = X.batch_size
<         data_arrays = [self._pred_exec.arg_dict[name] for name in data_names]
<         output_list = [[] for _ in range(len(self._pred_exec.outputs))]
<         if return_data:
<             data_list = [[] for _ in X.provide_data]
<             label_list = [[] for _ in X.provide_label]
< 
<         i = 0
<         for batch in X:
< 
<             _load_data(batch, data_arrays)
<             self._pred_exec.forward(is_train=False)
<             padded = batch.pad
<             real_size = batch_size - padded
< 
<             for o_list, o_nd in zip(output_list, self._pred_exec.outputs):
<                 o_list.append(o_nd[0:real_size].asnumpy())
< 
<             if return_data:
<                 for j, x in enumerate(batch.data):
<                     data_list[j].append(x[0:real_size].asnumpy())
<                 for j, x in enumerate(batch.label):
<                     label_list[j].append(x[0:real_size].asnumpy())
<             i += 1
<             if num_batch is not None and i == num_batch:
<                 break
< 
<         outputs = [np.concatenate(x) for x in output_list]
<         if len(outputs) == 1:
<             outputs = outputs[0]
< 
<         if return_data:
<             data = [np.concatenate(x) for x in data_list]
<             label = [np.concatenate(x) for x in label_list]
<             if len(data) == 1:
<                 data = data[0]
<             if len(label) == 1:
<                 label = label[0]
<             return outputs, data, label
<         else:
<             return outputs
< 
<     def score(self, X, eval_metric='acc', num_batch=None, batch_end_callback=None, reset=True):
<         """Run the model given an input and calculate the score
<         as assessed by an evaluation metric.
< 
<         Parameters
<         ----------
<         X : mxnet.DataIter
<         eval_metric : metric.metric
<             The metric for calculating score.
<         num_batch : int or None
<             The number of batches to run. Go though all batches if ``None``.
<         Returns
<         -------
<         s : float
<             The final score.
<         """
<         # setup metric
<         if not isinstance(eval_metric, metric.EvalMetric):
<             eval_metric = metric.create(eval_metric)
< 
<         X = self._init_iter(X, None, is_train=False)
<         if reset:
<             X.reset()
< 
<         data_shapes = X.provide_data
<         data_names = [x[0] for x in data_shapes]
<         type_dict = dict((key, value.dtype) for (key, value) in self.arg_params.items())
<         for x in X.provide_data:
<             if isinstance(x, DataDesc):
<                 type_dict[x.name] = x.dtype
<             else:
<                 type_dict[x[0]] = mx_real_t
< 
<         self._init_predictor(data_shapes, type_dict)
<         data_arrays = [self._pred_exec.arg_dict[name] for name in data_names]
< 
<         for i, batch in enumerate(X):
<             if num_batch is not None and i == num_batch:
<                 break
<             _load_data(batch, data_arrays)
<             self._pred_exec.forward(is_train=False)
<             eval_metric.update(batch.label, self._pred_exec.outputs)
< 
<             if batch_end_callback is not None:
<                 batch_end_params = BatchEndParam(epoch=0,
<                                                  nbatch=i,
<                                                  eval_metric=eval_metric,
<                                                  locals=locals())
<                 _multiple_callbacks(batch_end_callback, batch_end_params)
<         return eval_metric.get()[1]
< 
<     def fit(self, X, y=None, eval_data=None, eval_metric='acc',
<             epoch_end_callback=None, batch_end_callback=None, kvstore='local', logger=None,
<             work_load_list=None, monitor=None, eval_end_callback=LogValidationMetricsCallback(),
<             eval_batch_end_callback=None):
<         """Fit the model.
< 
<         Parameters
<         ----------
<         X : DataIter, or numpy.ndarray/NDArray
<             Training data. If `X` is a `DataIter`, the name or (if name not available)
<             the position of its outputs should match the corresponding variable
<             names defined in the symbolic graph.
<         y : numpy.ndarray/NDArray, optional
<             Training set label.
<             If X is ``numpy.ndarray`` or `NDArray`, `y` is required to be set.
<             While y can be 1D or 2D (with 2nd dimension as 1), its first dimension must be
<             the same as `X`, i.e. the number of data points and labels should be equal.
<         eval_data : DataIter or numpy.ndarray/list/NDArray pair
<             If eval_data is numpy.ndarray/list/NDArray pair,
<             it should be ``(valid_data, valid_label)``.
<         eval_metric : metric.EvalMetric or str or callable
<             The evaluation metric. This could be the name of evaluation metric
<             or a custom evaluation function that returns statistics
<             based on a minibatch.
<         epoch_end_callback : callable(epoch, symbol, arg_params, aux_states)
<             A callback that is invoked at end of each epoch.
<             This can be used to checkpoint model each epoch.
<         batch_end_callback: callable(epoch)
<             A callback that is invoked at end of each batch for purposes of printing.
<         kvstore: KVStore or str, optional
<            The KVStore or a string kvstore type: 'local', 'dist_sync', 'dist_async'
<            In default uses 'local', often no need to change for single machiine.
<         logger : logging logger, optional
<             When not specified, default logger will be used.
<         work_load_list : float or int, optional
<             The list of work load for different devices,
<             in the same order as `ctx`.
< 
<         Note
<         ----
<         KVStore behavior
<         - 'local', multi-devices on a single machine, will automatically choose best type.
<         - 'dist_sync', multiple machines communicating via BSP.
<         - 'dist_async', multiple machines with asynchronous communication.
<         """
< 
<         data = self._init_iter(X, y, is_train=True)
<         eval_data = self._init_eval_iter(eval_data)
< 
<         if self.sym_gen:
<             self.symbol = self.sym_gen(data.default_bucket_key) # pylint: disable=no-member
<             self._check_arguments()
<         self.kwargs["sym"] = self.symbol
< 
<         arg_names, param_names, aux_names = \
<                 self._init_params(data.provide_data+data.provide_label)
< 
<         # setup metric
<         if not isinstance(eval_metric, metric.EvalMetric):
<             eval_metric = metric.create(eval_metric)
< 
<         # create kvstore
<         (kvstore, update_on_kvstore) = _create_kvstore(
<             kvstore, len(self.ctx), self.arg_params)
< 
<         param_idx2name = {}
<         if update_on_kvstore:
<             param_idx2name.update(enumerate(param_names))
<         else:
<             for i, n in enumerate(param_names):
<                 for k in range(len(self.ctx)):
<                     param_idx2name[i*len(self.ctx)+k] = n
<         self.kwargs["param_idx2name"] = param_idx2name
< 
<         # init optmizer
<         if isinstance(self.optimizer, str):
<             batch_size = data.batch_size
<             if kvstore and 'dist' in kvstore.type and '_async' not in kvstore.type:
<                 batch_size *= kvstore.num_workers
<             optimizer = opt.create(self.optimizer,
<                                    rescale_grad=(1.0/batch_size),
<                                    **(self.kwargs))
<         elif isinstance(self.optimizer, opt.Optimizer):
<             optimizer = self.optimizer
< 
<         # do training
<         _train_multi_device(self.symbol, self.ctx, arg_names, param_names, aux_names,
<                             self.arg_params, self.aux_params,
<                             begin_epoch=self.begin_epoch, end_epoch=self.num_epoch,
<                             epoch_size=self.epoch_size,
<                             optimizer=optimizer,
<                             train_data=data, eval_data=eval_data,
<                             eval_metric=eval_metric,
<                             epoch_end_callback=epoch_end_callback,
<                             batch_end_callback=batch_end_callback,
<                             kvstore=kvstore, update_on_kvstore=update_on_kvstore,
<                             logger=logger, work_load_list=work_load_list, monitor=monitor,
<                             eval_end_callback=eval_end_callback,
<                             eval_batch_end_callback=eval_batch_end_callback,
<                             sym_gen=self.sym_gen)
< 
< 
<     def save(self, prefix, epoch=None):
<         """Checkpoint the model checkpoint into file.
<         You can also use `pickle` to do the job if you only work on Python.
<         The advantage of `load` and `save` (as compared to `pickle`) is that
<         the resulting file can be loaded from other MXNet language bindings.
<         One can also directly `load`/`save` from/to cloud storage(S3, HDFS)
< 
<         Parameters
<         ----------
<         prefix : str
<             Prefix of model name.
< 
<         Notes
<         -----
<         - ``prefix-symbol.json`` will be saved for symbol.
<         - ``prefix-epoch.params`` will be saved for parameters.
<         """
<         if epoch is None:
<             epoch = self.num_epoch
<         assert epoch is not None
<         save_checkpoint(prefix, epoch, self.symbol, self.arg_params, self.aux_params)
< 
<     @staticmethod
<     def load(prefix, epoch, ctx=None, **kwargs):
<         """Load model checkpoint from file.
< 
<         Parameters
<         ----------
<         prefix : str
<             Prefix of model name.
<         epoch : int
<             epoch number of model we would like to load.
<         ctx : Context or list of Context, optional
<             The device context of training and prediction.
<         kwargs : dict
<             Other parameters for model, including `num_epoch`, optimizer and `numpy_batch_size`.
< 
<         Returns
<         -------
<         model : FeedForward
<             The loaded model that can be used for prediction.
< 
<         Notes
<         -----
<         - ``prefix-symbol.json`` will be saved for symbol.
<         - ``prefix-epoch.params`` will be saved for parameters.
<         """
<         symbol, arg_params, aux_params = load_checkpoint(prefix, epoch)
<         return FeedForward(symbol, ctx=ctx,
<                            arg_params=arg_params, aux_params=aux_params,
<                            begin_epoch=epoch,
<                            **kwargs)
< 
<     @staticmethod
<     def create(symbol, X, y=None, ctx=None,
<                num_epoch=None, epoch_size=None, optimizer='sgd', initializer=Uniform(0.01),
<                eval_data=None, eval_metric='acc',
<                epoch_end_callback=None, batch_end_callback=None,
<                kvstore='local', logger=None, work_load_list=None,
<                eval_end_callback=LogValidationMetricsCallback(),
<                eval_batch_end_callback=None, **kwargs):
<         """Functional style to create a model.
<         This function is more consistent with functional
<         languages such as R, where mutation is not allowed.
< 
<         Parameters
<         ----------
<         symbol : Symbol
<             The symbol configuration of a computation network.
<         X : DataIter
<             Training data.
<         y : numpy.ndarray, optional
<             If `X` is a ``numpy.ndarray``, `y` must be set.
<         ctx : Context or list of Context, optional
<             The device context of training and prediction.
<             To use multi-GPU training, pass in a list of GPU contexts.
<         num_epoch : int, optional
<             The number of training epochs(epochs).
<         epoch_size : int, optional
<             Number of batches in a epoch. In default, it is set to
<             ``ceil(num_train_examples / batch_size)``.
<         optimizer : str or Optimizer, optional
<             The name of the chosen optimizer, or an optimizer object, used for training.
<         initializer : initializer function, optional
<             The initialization scheme used.
<         eval_data : DataIter or numpy.ndarray pair
<             If `eval_set` is ``numpy.ndarray`` pair, it should
<             be (`valid_data`, `valid_label`).
<         eval_metric : metric.EvalMetric or str or callable
<             The evaluation metric. Can be the name of an evaluation metric
<             or a custom evaluation function that returns statistics
<             based on a minibatch.
<         epoch_end_callback : callable(epoch, symbol, arg_params, aux_states)
<             A callback that is invoked at end of each epoch.
<             This can be used to checkpoint model each epoch.
<         batch_end_callback: callable(epoch)
<             A callback that is invoked at end of each batch for print purposes.
<         kvstore: KVStore or str, optional
<            The KVStore or a string kvstore type: 'local', 'dist_sync', 'dis_async'.
<            Defaults to 'local', often no need to change for single machine.
<         logger : logging logger, optional
<             When not specified, default logger will be used.
<         work_load_list : list of float or int, optional
<             The list of work load for different devices,
<             in the same order as `ctx`.
<         """
<         model = FeedForward(symbol, ctx=ctx, num_epoch=num_epoch,
<                             epoch_size=epoch_size,
<                             optimizer=optimizer, initializer=initializer, **kwargs)
<         model.fit(X, y, eval_data=eval_data, eval_metric=eval_metric,
<                   epoch_end_callback=epoch_end_callback,
<                   batch_end_callback=batch_end_callback,
<                   kvstore=kvstore,
<                   logger=logger,
<                   work_load_list=work_load_list,
<                   eval_end_callback=eval_end_callback,
<                   eval_batch_end_callback=eval_batch_end_callback)
<         return model
---
> # Licensed to the Apache Software Foundation (ASF) under one
> # or more contributor license agreements.  See the NOTICE file
> # distributed with this work for additional information
> # regarding copyright ownership.  The ASF licenses this file
> # to you under the Apache License, Version 2.0 (the
> # "License"); you may not use this file except in compliance
> # with the License.  You may obtain a copy of the License at
> #
> #   http://www.apache.org/licenses/LICENSE-2.0
> #
> # Unless required by applicable law or agreed to in writing,
> # software distributed under the License is distributed on an
> # "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
> # KIND, either express or implied.  See the License for the
> # specific language governing permissions and limitations
> # under the License.
> 
> # pylint: disable=fixme, invalid-name, too-many-arguments, too-many-locals, too-many-lines
> # pylint: disable=too-many-branches, too-many-statements
> """MXNet model module"""
> from __future__ import absolute_import, print_function
> 
> import os
> import time
> import logging
> import warnings
> from collections import namedtuple
> import numpy as np
> 
> from . import io
> from . import ndarray as nd
> from . import symbol as sym
> from . import optimizer as opt
> from . import metric
> from . import kvstore as kvs
> from .context import Context, cpu
> from .initializer import Uniform
> from .optimizer import get_updater
> from .executor_manager import DataParallelExecutorManager, _check_arguments, _load_data
> from .io import DataDesc
> from .base import mx_real_t
> 
> BASE_ESTIMATOR = object
> 
> try:
>     from sklearn.base import BaseEstimator
>     BASE_ESTIMATOR = BaseEstimator
> except ImportError:
>     SKLEARN_INSTALLED = False
> 
> # Parameter to pass to batch_end_callback
> BatchEndParam = namedtuple('BatchEndParams',
>                            ['epoch',
>                             'nbatch',
>                             'eval_metric',
>                             'locals'])
> 
> def _create_sparse_kvstore(kvstore):
>     """Create kvstore assuming some parameters' storage types are row_sparse.
> 
>     Parameters
>     ----------
>     kvstore : KVStore or str
>         The kvstore.
> 
>     Returns
>     -------
>     kvstore : KVStore
>     update_on_kvstore : bool. Always True.
>     """
>     # always update on kvstore
>     update_on_kvstore = True
>     if isinstance(kvstore, kvs.KVStore):
>         kv = kvstore
>     elif isinstance(kvstore, str):
>         kv = kvs.create(kvstore)
>     else:
>         raise TypeError("Cannot create '%s' KVStore with row_sparse parameters. "
>                         "The type must be KVStore or str." % kvstore)
>     return (kv, update_on_kvstore)
> 
> def _create_kvstore(kvstore, num_device, arg_params):
>     """Create kvstore
>     This function select and create a proper kvstore if given the kvstore type.
> 
>     Parameters
>     ----------
>     kvstore : KVStore or str
>         The kvstore.
>     num_device : int
>         The number of devices
>     arg_params : dict of str to `NDArray`.
>         Model parameter, dict of name to `NDArray` of net's weights.
>     """
>     update_on_kvstore = bool(int(os.getenv('MXNET_UPDATE_ON_KVSTORE', "1")))
>     if kvstore is None:
>         kv = None
>     elif isinstance(kvstore, kvs.KVStore):
>         kv = kvstore
>     elif isinstance(kvstore, str):
>         # create kvstore using the string type
>         if num_device == 1 and 'dist' not in kvstore:
>             # no need to use kv for single device and single machine
>             kv = None
>         else:
>             kv = kvs.create(kvstore)
>             if kvstore == 'local':
>             # automatically select a proper local
>                 max_size = max(np.prod(param.shape) for param in
>                                arg_params.values())
>                 if max_size > 1024 * 1024 * 16:
>                     update_on_kvstore = False
>     else:
>         raise TypeError('kvstore must be KVStore, str or None')
> 
>     if kv is None:
>         update_on_kvstore = False
> 
>     return (kv, update_on_kvstore)
> 
> def _initialize_kvstore(kvstore, param_arrays, arg_params, param_names, update_on_kvstore):
>     """Initialize kvstore"""
>     for idx, param_on_devs in enumerate(param_arrays):
>         name = param_names[idx]
>         kvstore.init(name, arg_params[name])
> 
>         if update_on_kvstore:
>             kvstore.pull(name, param_on_devs, priority=-idx)
> 
> def _update_params_on_kvstore_nccl(param_arrays, grad_arrays, kvstore, param_names):
>     """Perform update of param_arrays from grad_arrays on NCCL kvstore."""
>     valid_indices = [index for index, grad_list in
>                      enumerate(grad_arrays) if grad_list[0] is not None]
>     valid_grad_arrays = [grad_arrays[i] for i in valid_indices]
>     valid_param_arrays = [param_arrays[i] for i in valid_indices]
>     valid_param_names = [param_names[i] for i in valid_indices]
>     size = len(valid_grad_arrays)
>     start = 0
>     # Use aggregation by default only with NCCL
>     default_batch = '16'
>     batch = int(os.getenv('MXNET_UPDATE_AGGREGATION_SIZE', default_batch))
>     while start < size:
>         end = start + batch if start + batch < size else size
>         # push gradient, priority is negative index
>         kvstore.push(valid_param_names[start:end], valid_grad_arrays[start:end], priority=-start)
>         # pull back the weights
>         kvstore.pull(valid_param_names[start:end], valid_param_arrays[start:end], priority=-start)
>         start = end
> 
> def _update_params_on_kvstore(param_arrays, grad_arrays, kvstore, param_names):
>     """Perform update of param_arrays from grad_arrays on kvstore."""
>     for index, pair in enumerate(zip(param_arrays, grad_arrays)):
>         arg_list, grad_list = pair
>         if grad_list[0] is None:
>             continue
>         name = param_names[index]
>         # push gradient, priority is negative index
>         kvstore.push(name, grad_list, priority=-index)
>         # pull back the weights
>         kvstore.pull(name, arg_list, priority=-index)
> 
> def _update_params(param_arrays, grad_arrays, updater, num_device,
>                    kvstore=None, param_names=None):
>     """Perform update of param_arrays from grad_arrays not on kvstore."""
>     updates = [[] for _ in range(num_device)]
>     for i, pair in enumerate(zip(param_arrays, grad_arrays)):
>         arg_list, grad_list = pair
>         if grad_list[0] is None:
>             continue
>         index = i
>         if kvstore:
>             name = param_names[index]
>             # push gradient, priority is negative index
>             kvstore.push(name, grad_list, priority=-index)
>             # pull back the sum gradients, to the same locations.
>             kvstore.pull(name, grad_list, priority=-index)
>         for k, p in enumerate(zip(arg_list, grad_list)):
>             # faked an index here, to make optimizer create diff
>             # state for the same index but on diff devs, TODO(mli)
>             # use a better solution later
>             w, g = p
>             updates[k].append((index*num_device+k, g, w))
>     for dev_updates in updates:
>         # update params if param_arrays and grad_arrays are not empty
>         if dev_updates:
>             i, w, g = zip(*dev_updates)
>             updater(i, w, g)
> 
> 
> def _multiple_callbacks(callbacks, *args, **kwargs):
>     """Sends args and kwargs to any configured callbacks.
>     This handles the cases where the 'callbacks' variable
>     is ``None``, a single function, or a list.
>     """
>     if isinstance(callbacks, list):
>         for cb in callbacks:
>             cb(*args, **kwargs)
>         return
>     if callbacks:
>         callbacks(*args, **kwargs)
> 
> 
> def _train_multi_device(symbol, ctx, arg_names, param_names, aux_names,
>                         arg_params, aux_params,
>                         begin_epoch, end_epoch, epoch_size, optimizer,
>                         kvstore, update_on_kvstore,
>                         train_data, eval_data=None, eval_metric=None,
>                         epoch_end_callback=None, batch_end_callback=None,
>                         logger=None, work_load_list=None, monitor=None,
>                         eval_end_callback=None,
>                         eval_batch_end_callback=None, sym_gen=None):
>     """Internal training function on multiple devices.
>     This function will also work for single device as well.
> 
>     Parameters
>     ----------
>     symbol : Symbol
>         The network configuration.
>     ctx : list of Context
>         The training devices.
>     arg_names: list of str
>         Name of all arguments of the network.
>     param_names: list of str
>         Name of all trainable parameters of the network.
>     aux_names: list of str
>         Name of all auxiliary states of the network.
>     arg_params : dict of str to NDArray
>         Model parameter, dict of name to NDArray of net's weights.
>     aux_params : dict of str to NDArray
>         Model parameter, dict of name to NDArray of net's auxiliary states.
>     begin_epoch : int
>         The begining training epoch.
>     end_epoch : int
>         The end training epoch.
>     epoch_size : int, optional
>         Number of batches in a epoch. In default, it is set to
>         ``ceil(num_train_examples / batch_size)``.
>     optimizer : Optimizer
>         The optimization algorithm
>     train_data : DataIter
>         Training data iterator.
>     eval_data : DataIter
>         Validation data iterator.
>     eval_metric : EvalMetric
>         An evaluation function or a list of evaluation functions.
>     epoch_end_callback : callable(epoch, symbol, arg_params, aux_states)
>         A callback that is invoked at end of each epoch.
>         This can be used to checkpoint model each epoch.
>     batch_end_callback : callable(BatchEndParams)
>         A callback that is invoked at end of each batch.
>         This can be used to measure speed, get result from evaluation metric. etc.
>     kvstore : KVStore
>         The KVStore.
>     update_on_kvstore : bool
>         Whether or not perform weight updating on kvstore.
>     logger : logging logger
>         When not specified, default logger will be used.
>     work_load_list : list of float or int, optional
>         The list of work load for different devices,
>         in the same order as ``ctx``.
>     monitor : Monitor, optional
>         Monitor installed to executor,
>         for monitoring outputs, weights, and gradients for debugging.
>     Notes
>     -----
>     - This function will inplace update the NDArrays in `arg_params` and `aux_states`.
>     """
>     if logger is None:
>         logger = logging
>     executor_manager = DataParallelExecutorManager(symbol=symbol,
>                                                    sym_gen=sym_gen,
>                                                    ctx=ctx,
>                                                    train_data=train_data,
>                                                    param_names=param_names,
>                                                    arg_names=arg_names,
>                                                    aux_names=aux_names,
>                                                    work_load_list=work_load_list,
>                                                    logger=logger)
>     if monitor:
>         executor_manager.install_monitor(monitor)
> 
>     executor_manager.set_params(arg_params, aux_params)
> 
>     if not update_on_kvstore:
>         updater = get_updater(optimizer)
>     else:
>         kvstore.set_optimizer(optimizer)
> 
>     if kvstore:
>         _initialize_kvstore(kvstore=kvstore,
>                             param_arrays=executor_manager.param_arrays,
>                             arg_params=arg_params,
>                             param_names=executor_manager.param_names,
>                             update_on_kvstore=update_on_kvstore)
> 
>     # Now start training
>     train_data.reset()
>     for epoch in range(begin_epoch, end_epoch):
>         # Training phase
>         tic = time.time()
>         eval_metric.reset()
>         nbatch = 0
>         # Iterate over training data.
>         while True:
>             do_reset = True
>             for data_batch in train_data:
>                 executor_manager.load_data_batch(data_batch)
> 
>                 if monitor is not None:
>                     monitor.tic()
> 
>                 executor_manager.forward(is_train=True)
>                 executor_manager.backward()
> 
>                 if update_on_kvstore:
>                     if 'nccl' in kvstore.type:
>                         _update_params_on_kvstore_nccl(executor_manager.param_arrays,
>                                                        executor_manager.grad_arrays,
>                                                        kvstore, executor_manager.param_names)
>                     else:
>                         _update_params_on_kvstore(executor_manager.param_arrays,
>                                                   executor_manager.grad_arrays,
>                                                   kvstore, executor_manager.param_names)
>                 else:
>                     _update_params(executor_manager.param_arrays,
>                                    executor_manager.grad_arrays,
>                                    updater=updater,
>                                    num_device=len(ctx),
>                                    kvstore=kvstore,
>                                    param_names=executor_manager.param_names)
> 
>                 if monitor is not None:
>                     monitor.toc_print()
> 
>                 # evaluate at end, so we can lazy copy
>                 executor_manager.update_metric(eval_metric, data_batch.label)
> 
>                 nbatch += 1
>                 # batch callback (for print purpose)
>                 if batch_end_callback is not None:
>                     batch_end_params = BatchEndParam(epoch=epoch,
>                                                      nbatch=nbatch,
>                                                      eval_metric=eval_metric,
>                                                      locals=locals())
>                     _multiple_callbacks(batch_end_callback, batch_end_params)
> 
>                 # this epoch is done possibly earlier
>                 if epoch_size is not None and nbatch >= epoch_size:
>                     do_reset = False
>                     break
> 
>             if do_reset:
>                 logger.info('Epoch[%d] Resetting Data Iterator', epoch)
>                 train_data.reset()
> 
>             # this epoch is done
>             if epoch_size is None or nbatch >= epoch_size:
>                 break
> 
>         toc = time.time()
>         logger.info('Epoch[%d] Time cost=%.3f', epoch, (toc - tic))
> 
>         if epoch_end_callback or epoch + 1 == end_epoch:
>             executor_manager.copy_to(arg_params, aux_params)
> 
>         _multiple_callbacks(epoch_end_callback, epoch, symbol, arg_params, aux_params)
> 
>         # evaluation
>         if eval_data:
>             eval_metric.reset()
>             eval_data.reset()
>             total_num_batch = 0
>             for i, eval_batch in enumerate(eval_data):
>                 executor_manager.load_data_batch(eval_batch)
>                 executor_manager.forward(is_train=False)
>                 executor_manager.update_metric(eval_metric, eval_batch.label)
>                 if eval_batch_end_callback is not None:
>                     batch_end_params = BatchEndParam(epoch=epoch,
>                                                      nbatch=i,
>                                                      eval_metric=eval_metric,
>                                                      locals=locals())
>                     _multiple_callbacks(eval_batch_end_callback, batch_end_params)
>                 total_num_batch += 1
>             if eval_end_callback is not None:
>                 eval_end_params = BatchEndParam(epoch=epoch,
>                                                 nbatch=total_num_batch,
>                                                 eval_metric=eval_metric,
>                                                 locals=locals())
>                 _multiple_callbacks(eval_end_callback, eval_end_params)
>             eval_data.reset()
>     # end of all epochs
> 
> 
> def save_checkpoint(prefix, epoch, symbol, arg_params, aux_params, remove_amp_cast=True):
>     """Checkpoint the model data into file.
> 
>     Parameters
>     ----------
>     prefix : str
>         Prefix of model name.
>     epoch : int
>         The epoch number of the model.
>     symbol : Symbol
>         The input Symbol.
>     arg_params : dict of str to NDArray
>         Model parameter, dict of name to NDArray of net's weights.
>     aux_params : dict of str to NDArray
>         Model parameter, dict of name to NDArray of net's auxiliary states.
>     remove_amp_cast : bool, optional
>         Whether to remove the amp_cast and amp_multicast operators, before saving the model.
>     Notes
>     -----
>     - ``prefix-symbol.json`` will be saved for symbol.
>     - ``prefix-epoch.params`` will be saved for parameters.
>     """
>     if symbol is not None:
>         symbol.save('%s-symbol.json' % prefix, remove_amp_cast=remove_amp_cast)
> 
>     save_dict = {('arg:%s' % k) : v.as_in_context(cpu()) for k, v in arg_params.items()}
>     save_dict.update({('aux:%s' % k) : v.as_in_context(cpu()) for k, v in aux_params.items()})
>     param_name = '%s-%04d.params' % (prefix, epoch)
>     nd.save(param_name, save_dict)
>     logging.info('Saved checkpoint to \"%s\"', param_name)
> 
> 
> def load_params(prefix, epoch):
>     """Load params from a file
>     """
>     save_dict = nd.load("%s-%04d.params" % (prefix, epoch))
>     arg_params = {}
>     aux_params = {}
>     if not save_dict:
>         logging.warning("Params file '%s' is empty", '%s-%04d.params' % (prefix, epoch))
>     for k, v in save_dict.items():
>         tp, name = k.split(":", 1)
>         if tp == "arg":
>             arg_params[name] = v
>         if tp == "aux":
>             aux_params[name] = v
>     return (arg_params, aux_params)
> 
> def load_checkpoint(prefix, epoch):
>     """Load model checkpoint from file.
> 
>     Parameters
>     ----------
>     prefix : str
>         Prefix of model name.
>     epoch : int
>         Epoch number of model we would like to load.
> 
>     Returns
>     -------
>     symbol : Symbol
>         The symbol configuration of computation network.
>     arg_params : dict of str to NDArray
>         Model parameter, dict of name to NDArray of net's weights.
>     aux_params : dict of str to NDArray
>         Model parameter, dict of name to NDArray of net's auxiliary states.
> 
>     Notes
>     -----
>     - Symbol will be loaded from ``prefix-symbol.json``.
>     - Parameters will be loaded from ``prefix-epoch.params``.
>     """
>     symbol = sym.load('%s-symbol.json' % prefix)
>     arg_params, aux_params = load_params(prefix, epoch)
>     return (symbol, arg_params, aux_params)
> 
> from .callback import LogValidationMetricsCallback # pylint: disable=wrong-import-position
> 
> class FeedForward(BASE_ESTIMATOR):
>     """Model class of MXNet for training and predicting feedforward nets.
>     This class is designed for a single-data single output supervised network.
> 
>     Parameters
>     ----------
>     symbol : Symbol
>         The symbol configuration of computation network.
>     ctx : Context or list of Context, optional
>         The device context of training and prediction.
>         To use multi GPU training, pass in a list of gpu contexts.
>     num_epoch : int, optional
>         Training parameter, number of training epochs(epochs).
>     epoch_size : int, optional
>         Number of batches in a epoch. In default, it is set to
>         ``ceil(num_train_examples / batch_size)``.
>     optimizer : str or Optimizer, optional
>         Training parameter, name or optimizer object for training.
>     initializer : initializer function, optional
>         Training parameter, the initialization scheme used.
>     numpy_batch_size : int, optional
>         The batch size of training data.
>         Only needed when input array is numpy.
>     arg_params : dict of str to NDArray, optional
>         Model parameter, dict of name to NDArray of net's weights.
>     aux_params : dict of str to NDArray, optional
>         Model parameter, dict of name to NDArray of net's auxiliary states.
>     allow_extra_params : boolean, optional
>         Whether allow extra parameters that are not needed by symbol
>         to be passed by aux_params and ``arg_params``.
>         If this is True, no error will be thrown when ``aux_params`` and ``arg_params``
>         contain more parameters than needed.
>     begin_epoch : int, optional
>         The begining training epoch.
>     kwargs : dict
>         The additional keyword arguments passed to optimizer.
>     """
>     def __init__(self, symbol, ctx=None,
>                  num_epoch=None, epoch_size=None, optimizer='sgd',
>                  initializer=Uniform(0.01),
>                  numpy_batch_size=128,
>                  arg_params=None, aux_params=None,
>                  allow_extra_params=False,
>                  begin_epoch=0,
>                  **kwargs):
>         warnings.warn(
>             '\033[91mmxnet.model.FeedForward has been deprecated. ' + \
>             'Please use mxnet.mod.Module instead.\033[0m',
>             DeprecationWarning, stacklevel=2)
> 
>         if isinstance(symbol, sym.Symbol):
>             self.symbol = symbol
>             self.sym_gen = None
>         else:
>             assert(callable(symbol))
>             self.symbol = None
>             self.sym_gen = symbol
> 
>         # model parameters
>         self.arg_params = arg_params
>         self.aux_params = aux_params
>         self.allow_extra_params = allow_extra_params
> 
>         self.argument_checked = False
>         if self.sym_gen is None:
>             self._check_arguments()
> 
>         # basic configuration
>         if ctx is None:
>             ctx = [cpu()]
>         elif isinstance(ctx, Context):
>             ctx = [ctx]
>         self.ctx = ctx
>         # training parameters
>         self.num_epoch = num_epoch
>         self.epoch_size = epoch_size
>         self.kwargs = kwargs.copy()
>         self.optimizer = optimizer
>         self.initializer = initializer
>         self.numpy_batch_size = numpy_batch_size
>         # internal helper state
>         self._pred_exec = None
>         self.begin_epoch = begin_epoch
> 
>     def _check_arguments(self):
>         """verify the argument of the default symbol and user provided parameters"""
>         if self.argument_checked:
>             return
> 
>         assert(self.symbol is not None)
>         self.argument_checked = True
> 
>         # check if symbol contain duplicated names.
>         _check_arguments(self.symbol)
>         # rematch parameters to delete useless ones
>         if self.allow_extra_params:
>             if self.arg_params:
>                 arg_names = set(self.symbol.list_arguments())
>                 self.arg_params = {k : v for k, v in self.arg_params.items()
>                                    if k in arg_names}
>             if self.aux_params:
>                 aux_names = set(self.symbol.list_auxiliary_states())
>                 self.aux_params = {k : v for k, v in self.aux_params.items()
>                                    if k in aux_names}
> 
> 
>     @staticmethod
>     def _is_data_arg(name):
>         """Check if name is a data argument."""
>         return name.endswith('data') or name.endswith('label')
> 
>     def _init_params(self, inputs, overwrite=False):
>         """Initialize weight parameters and auxiliary states."""
>         inputs = [x if isinstance(x, DataDesc) else DataDesc(*x) for x in inputs]
>         input_shapes = {item.name: item.shape for item in inputs}
>         arg_shapes, _, aux_shapes = self.symbol.infer_shape(**input_shapes)
>         assert arg_shapes is not None
>         input_dtypes = {item.name: item.dtype for item in inputs}
>         arg_dtypes, _, aux_dtypes = self.symbol.infer_type(**input_dtypes)
>         assert arg_dtypes is not None
> 
>         arg_names = self.symbol.list_arguments()
>         input_names = input_shapes.keys()
>         param_names = [key for key in arg_names if key not in input_names]
>         aux_names = self.symbol.list_auxiliary_states()
> 
>         param_name_attrs = [x for x in zip(arg_names, arg_shapes, arg_dtypes)
>                             if x[0] in param_names]
>         arg_params = {k : nd.zeros(shape=s, dtype=t)
>                       for k, s, t in param_name_attrs}
>         aux_name_attrs = [x for x in zip(aux_names, aux_shapes, aux_dtypes)
>                           if x[0] in aux_names]
>         aux_params = {k : nd.zeros(shape=s, dtype=t)
>                       for k, s, t in aux_name_attrs}
> 
>         for k, v in arg_params.items():
>             if self.arg_params and k in self.arg_params and (not overwrite):
>                 arg_params[k][:] = self.arg_params[k][:]
>             else:
>                 self.initializer(k, v)
> 
>         for k, v in aux_params.items():
>             if self.aux_params and k in self.aux_params and (not overwrite):
>                 aux_params[k][:] = self.aux_params[k][:]
>             else:
>                 self.initializer(k, v)
> 
>         self.arg_params = arg_params
>         self.aux_params = aux_params
>         return (arg_names, list(param_names), aux_names)
> 
>     def __getstate__(self):
>         this = self.__dict__.copy()
>         this['_pred_exec'] = None
>         return this
> 
>     def __setstate__(self, state):
>         self.__dict__.update(state)
> 
>     def _init_predictor(self, input_shapes, type_dict=None):
>         """Initialize the predictor module for running prediction."""
>         shapes = {name: self.arg_params[name].shape for name in self.arg_params}
>         shapes.update(dict(input_shapes))
>         if self._pred_exec is not None:
>             arg_shapes, _, _ = self.symbol.infer_shape(**shapes)
>             assert arg_shapes is not None, "Incomplete input shapes"
>             pred_shapes = [x.shape for x in self._pred_exec.arg_arrays]
>             if arg_shapes == pred_shapes:
>                 return
>         # for now only use the first device
>         pred_exec = self.symbol.simple_bind(
>             self.ctx[0], grad_req='null', type_dict=type_dict, **shapes)
>         pred_exec.copy_params_from(self.arg_params, self.aux_params)
> 
>         _check_arguments(self.symbol)
>         self._pred_exec = pred_exec
> 
>     def _init_iter(self, X, y, is_train):
>         """Initialize the iterator given input."""
>         if isinstance(X, (np.ndarray, nd.NDArray)):
>             if y is None:
>                 if is_train:
>                     raise ValueError('y must be specified when X is numpy.ndarray')
>                 y = np.zeros(X.shape[0])
>             if not isinstance(y, (np.ndarray, nd.NDArray)):
>                 raise TypeError('y must be ndarray when X is numpy.ndarray')
>             if X.shape[0] != y.shape[0]:
>                 raise ValueError("The numbers of data points and labels not equal")
>             if y.ndim == 2 and y.shape[1] == 1:
>                 y = y.flatten()
>             if y.ndim != 1:
>                 raise ValueError("Label must be 1D or 2D (with 2nd dimension being 1)")
>             if is_train:
>                 return io.NDArrayIter(X, y, min(X.shape[0], self.numpy_batch_size),
>                                       shuffle=is_train, last_batch_handle='roll_over')
>             else:
>                 return io.NDArrayIter(X, y, min(X.shape[0], self.numpy_batch_size), shuffle=False)
>         if not isinstance(X, io.DataIter):
>             raise TypeError('X must be DataIter, NDArray or numpy.ndarray')
>         return X
> 
>     def _init_eval_iter(self, eval_data):
>         """Initialize the iterator given eval_data."""
>         if eval_data is None:
>             return eval_data
>         if isinstance(eval_data, (tuple, list)) and len(eval_data) == 2:
>             if eval_data[0] is not None:
>                 if eval_data[1] is None and isinstance(eval_data[0], io.DataIter):
>                     return eval_data[0]
>                 input_data = (np.array(eval_data[0]) if isinstance(eval_data[0], list)
>                               else eval_data[0])
>                 input_label = (np.array(eval_data[1]) if isinstance(eval_data[1], list)
>                                else eval_data[1])
>                 return self._init_iter(input_data, input_label, is_train=True)
>             else:
>                 raise ValueError("Eval data is NONE")
>         if not isinstance(eval_data, io.DataIter):
>             raise TypeError('Eval data must be DataIter, or ' \
>                             'NDArray/numpy.ndarray/list pair (i.e. tuple/list of length 2)')
>         return eval_data
> 
>     def predict(self, X, num_batch=None, return_data=False, reset=True):
>         """Run the prediction, always only use one device.
> 
>         Parameters
>         ----------
>         X : mxnet.DataIter
>         num_batch : int or None
>             The number of batch to run. Go though all batches if ``None``.
>         Returns
>         -------
>         y : numpy.ndarray or a list of numpy.ndarray if the network has multiple outputs.
>             The predicted value of the output.
>         """
>         X = self._init_iter(X, None, is_train=False)
> 
>         if reset:
>             X.reset()
>         data_shapes = X.provide_data
>         data_names = [x[0] for x in data_shapes]
>         type_dict = dict((key, value.dtype) for (key, value) in self.arg_params.items())
>         for x in X.provide_data:
>             if isinstance(x, DataDesc):
>                 type_dict[x.name] = x.dtype
>             else:
>                 type_dict[x[0]] = mx_real_t
> 
>         self._init_predictor(data_shapes, type_dict)
>         batch_size = X.batch_size
>         data_arrays = [self._pred_exec.arg_dict[name] for name in data_names]
>         output_list = [[] for _ in range(len(self._pred_exec.outputs))]
>         if return_data:
>             data_list = [[] for _ in X.provide_data]
>             label_list = [[] for _ in X.provide_label]
> 
>         i = 0
>         for batch in X:
> 
>             _load_data(batch, data_arrays)
>             self._pred_exec.forward(is_train=False)
>             padded = batch.pad
>             real_size = batch_size - padded
> 
>             for o_list, o_nd in zip(output_list, self._pred_exec.outputs):
>                 o_list.append(o_nd[0:real_size].asnumpy())
> 
>             if return_data:
>                 for j, x in enumerate(batch.data):
>                     data_list[j].append(x[0:real_size].asnumpy())
>                 for j, x in enumerate(batch.label):
>                     label_list[j].append(x[0:real_size].asnumpy())
>             i += 1
>             if num_batch is not None and i == num_batch:
>                 break
> 
>         outputs = [np.concatenate(x) for x in output_list]
>         if len(outputs) == 1:
>             outputs = outputs[0]
> 
>         if return_data:
>             data = [np.concatenate(x) for x in data_list]
>             label = [np.concatenate(x) for x in label_list]
>             if len(data) == 1:
>                 data = data[0]
>             if len(label) == 1:
>                 label = label[0]
>             return outputs, data, label
>         else:
>             return outputs
> 
>     def score(self, X, eval_metric='acc', num_batch=None, batch_end_callback=None, reset=True):
>         """Run the model given an input and calculate the score
>         as assessed by an evaluation metric.
> 
>         Parameters
>         ----------
>         X : mxnet.DataIter
>         eval_metric : metric.metric
>             The metric for calculating score.
>         num_batch : int or None
>             The number of batches to run. Go though all batches if ``None``.
>         Returns
>         -------
>         s : float
>             The final score.
>         """
>         # setup metric
>         if not isinstance(eval_metric, metric.EvalMetric):
>             eval_metric = metric.create(eval_metric)
> 
>         X = self._init_iter(X, None, is_train=False)
>         if reset:
>             X.reset()
> 
>         data_shapes = X.provide_data
>         data_names = [x[0] for x in data_shapes]
>         type_dict = dict((key, value.dtype) for (key, value) in self.arg_params.items())
>         for x in X.provide_data:
>             if isinstance(x, DataDesc):
>                 type_dict[x.name] = x.dtype
>             else:
>                 type_dict[x[0]] = mx_real_t
> 
>         self._init_predictor(data_shapes, type_dict)
>         data_arrays = [self._pred_exec.arg_dict[name] for name in data_names]
> 
>         for i, batch in enumerate(X):
>             if num_batch is not None and i == num_batch:
>                 break
>             _load_data(batch, data_arrays)
>             self._pred_exec.forward(is_train=False)
>             eval_metric.update(batch.label, self._pred_exec.outputs)
> 
>             if batch_end_callback is not None:
>                 batch_end_params = BatchEndParam(epoch=0,
>                                                  nbatch=i,
>                                                  eval_metric=eval_metric,
>                                                  locals=locals())
>                 _multiple_callbacks(batch_end_callback, batch_end_params)
>         return eval_metric.get()[1]
> 
>     def fit(self, X, y=None, eval_data=None, eval_metric='acc',
>             epoch_end_callback=None, batch_end_callback=None, kvstore='local', logger=None,
>             work_load_list=None, monitor=None, eval_end_callback=LogValidationMetricsCallback(),
>             eval_batch_end_callback=None):
>         """Fit the model.
> 
>         Parameters
>         ----------
>         X : DataIter, or numpy.ndarray/NDArray
>             Training data. If `X` is a `DataIter`, the name or (if name not available)
>             the position of its outputs should match the corresponding variable
>             names defined in the symbolic graph.
>         y : numpy.ndarray/NDArray, optional
>             Training set label.
>             If X is ``numpy.ndarray`` or `NDArray`, `y` is required to be set.
>             While y can be 1D or 2D (with 2nd dimension as 1), its first dimension must be
>             the same as `X`, i.e. the number of data points and labels should be equal.
>         eval_data : DataIter or numpy.ndarray/list/NDArray pair
>             If eval_data is numpy.ndarray/list/NDArray pair,
>             it should be ``(valid_data, valid_label)``.
>         eval_metric : metric.EvalMetric or str or callable
>             The evaluation metric. This could be the name of evaluation metric
>             or a custom evaluation function that returns statistics
>             based on a minibatch.
>         epoch_end_callback : callable(epoch, symbol, arg_params, aux_states)
>             A callback that is invoked at end of each epoch.
>             This can be used to checkpoint model each epoch.
>         batch_end_callback: callable(epoch)
>             A callback that is invoked at end of each batch for purposes of printing.
>         kvstore: KVStore or str, optional
>            The KVStore or a string kvstore type: 'local', 'dist_sync', 'dist_async'
>            In default uses 'local', often no need to change for single machiine.
>         logger : logging logger, optional
>             When not specified, default logger will be used.
>         work_load_list : float or int, optional
>             The list of work load for different devices,
>             in the same order as `ctx`.
> 
>         Note
>         ----
>         KVStore behavior
>         - 'local', multi-devices on a single machine, will automatically choose best type.
>         - 'dist_sync', multiple machines communicating via BSP.
>         - 'dist_async', multiple machines with asynchronous communication.
>         """
> 
>         data = self._init_iter(X, y, is_train=True)
>         eval_data = self._init_eval_iter(eval_data)
> 
>         if self.sym_gen:
>             self.symbol = self.sym_gen(data.default_bucket_key) # pylint: disable=no-member
>             self._check_arguments()
>         self.kwargs["sym"] = self.symbol
> 
>         arg_names, param_names, aux_names = \
>                 self._init_params(data.provide_data+data.provide_label)
> 
>         # setup metric
>         if not isinstance(eval_metric, metric.EvalMetric):
>             eval_metric = metric.create(eval_metric)
> 
>         # create kvstore
>         (kvstore, update_on_kvstore) = _create_kvstore(
>             kvstore, len(self.ctx), self.arg_params)
> 
>         param_idx2name = {}
>         if update_on_kvstore:
>             param_idx2name.update(enumerate(param_names))
>         else:
>             for i, n in enumerate(param_names):
>                 for k in range(len(self.ctx)):
>                     param_idx2name[i*len(self.ctx)+k] = n
>         self.kwargs["param_idx2name"] = param_idx2name
> 
>         # init optmizer
>         if isinstance(self.optimizer, str):
>             batch_size = data.batch_size
>             if kvstore and 'dist' in kvstore.type and '_async' not in kvstore.type:
>                 batch_size *= kvstore.num_workers
>             optimizer = opt.create(self.optimizer,
>                                    rescale_grad=(1.0/batch_size),
>                                    **(self.kwargs))
>         elif isinstance(self.optimizer, opt.Optimizer):
>             if not optimizer.idx2name:
>                 optimizer.idx2name = param_idx2name.copy()
>             optimizer = self.optimizer
> 
>         # do training
>         _train_multi_device(self.symbol, self.ctx, arg_names, param_names, aux_names,
>                             self.arg_params, self.aux_params,
>                             begin_epoch=self.begin_epoch, end_epoch=self.num_epoch,
>                             epoch_size=self.epoch_size,
>                             optimizer=optimizer,
>                             train_data=data, eval_data=eval_data,
>                             eval_metric=eval_metric,
>                             epoch_end_callback=epoch_end_callback,
>                             batch_end_callback=batch_end_callback,
>                             kvstore=kvstore, update_on_kvstore=update_on_kvstore,
>                             logger=logger, work_load_list=work_load_list, monitor=monitor,
>                             eval_end_callback=eval_end_callback,
>                             eval_batch_end_callback=eval_batch_end_callback,
>                             sym_gen=self.sym_gen)
> 
> 
>     def save(self, prefix, epoch=None, remove_amp_cast=True):
>         """Checkpoint the model checkpoint into file.
>         You can also use `pickle` to do the job if you only work on Python.
>         The advantage of `load` and `save` (as compared to `pickle`) is that
>         the resulting file can be loaded from other MXNet language bindings.
>         One can also directly `load`/`save` from/to cloud storage(S3, HDFS)
> 
>         Parameters
>         ----------
>         prefix : str
>             Prefix of model name.
>         remove_amp_cast : bool, optional
>             Whether to remove the amp_cast and amp_multicast operators, before saving the model.
> 
>         Notes
>         -----
>         - ``prefix-symbol.json`` will be saved for symbol.
>         - ``prefix-epoch.params`` will be saved for parameters.
>         """
>         if epoch is None:
>             epoch = self.num_epoch
>         assert epoch is not None
>         save_checkpoint(prefix, epoch, self.symbol, self.arg_params, self.aux_params, remove_amp_cast=remove_amp_cast)
> 
>     @staticmethod
>     def load(prefix, epoch, ctx=None, **kwargs):
>         """Load model checkpoint from file.
> 
>         Parameters
>         ----------
>         prefix : str
>             Prefix of model name.
>         epoch : int
>             epoch number of model we would like to load.
>         ctx : Context or list of Context, optional
>             The device context of training and prediction.
>         kwargs : dict
>             Other parameters for model, including `num_epoch`, optimizer and `numpy_batch_size`.
> 
>         Returns
>         -------
>         model : FeedForward
>             The loaded model that can be used for prediction.
> 
>         Notes
>         -----
>         - ``prefix-symbol.json`` will be saved for symbol.
>         - ``prefix-epoch.params`` will be saved for parameters.
>         """
>         symbol, arg_params, aux_params = load_checkpoint(prefix, epoch)
>         return FeedForward(symbol, ctx=ctx,
>                            arg_params=arg_params, aux_params=aux_params,
>                            begin_epoch=epoch,
>                            **kwargs)
> 
>     @staticmethod
>     def create(symbol, X, y=None, ctx=None,
>                num_epoch=None, epoch_size=None, optimizer='sgd', initializer=Uniform(0.01),
>                eval_data=None, eval_metric='acc',
>                epoch_end_callback=None, batch_end_callback=None,
>                kvstore='local', logger=None, work_load_list=None,
>                eval_end_callback=LogValidationMetricsCallback(),
>                eval_batch_end_callback=None, **kwargs):
>         """Functional style to create a model.
>         This function is more consistent with functional
>         languages such as R, where mutation is not allowed.
> 
>         Parameters
>         ----------
>         symbol : Symbol
>             The symbol configuration of a computation network.
>         X : DataIter
>             Training data.
>         y : numpy.ndarray, optional
>             If `X` is a ``numpy.ndarray``, `y` must be set.
>         ctx : Context or list of Context, optional
>             The device context of training and prediction.
>             To use multi-GPU training, pass in a list of GPU contexts.
>         num_epoch : int, optional
>             The number of training epochs(epochs).
>         epoch_size : int, optional
>             Number of batches in a epoch. In default, it is set to
>             ``ceil(num_train_examples / batch_size)``.
>         optimizer : str or Optimizer, optional
>             The name of the chosen optimizer, or an optimizer object, used for training.
>         initializer : initializer function, optional
>             The initialization scheme used.
>         eval_data : DataIter or numpy.ndarray pair
>             If `eval_set` is ``numpy.ndarray`` pair, it should
>             be (`valid_data`, `valid_label`).
>         eval_metric : metric.EvalMetric or str or callable
>             The evaluation metric. Can be the name of an evaluation metric
>             or a custom evaluation function that returns statistics
>             based on a minibatch.
>         epoch_end_callback : callable(epoch, symbol, arg_params, aux_states)
>             A callback that is invoked at end of each epoch.
>             This can be used to checkpoint model each epoch.
>         batch_end_callback: callable(epoch)
>             A callback that is invoked at end of each batch for print purposes.
>         kvstore: KVStore or str, optional
>            The KVStore or a string kvstore type: 'local', 'dist_sync', 'dis_async'.
>            Defaults to 'local', often no need to change for single machine.
>         logger : logging logger, optional
>             When not specified, default logger will be used.
>         work_load_list : list of float or int, optional
>             The list of work load for different devices,
>             in the same order as `ctx`.
>         """
>         model = FeedForward(symbol, ctx=ctx, num_epoch=num_epoch,
>                             epoch_size=epoch_size,
>                             optimizer=optimizer, initializer=initializer, **kwargs)
>         model.fit(X, y, eval_data=eval_data, eval_metric=eval_metric,
>                   epoch_end_callback=epoch_end_callback,
>                   batch_end_callback=batch_end_callback,
>                   kvstore=kvstore,
>                   logger=logger,
>                   work_load_list=work_load_list,
>                   eval_end_callback=eval_end_callback,
>                   eval_batch_end_callback=eval_batch_end_callback)
>         return model
diff -r /home/xxx/diff/python/mxnet/module/module.py /home/xxx/yyy/incubator-mxnet/python/mxnet/module/module.py
1,857c1,876
< # Licensed to the Apache Software Foundation (ASF) under one
< # or more contributor license agreements.  See the NOTICE file
< # distributed with this work for additional information
< # regarding copyright ownership.  The ASF licenses this file
< # to you under the Apache License, Version 2.0 (the
< # "License"); you may not use this file except in compliance
< # with the License.  You may obtain a copy of the License at
< #
< #   http://www.apache.org/licenses/LICENSE-2.0
< #
< # Unless required by applicable law or agreed to in writing,
< # software distributed under the License is distributed on an
< # "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
< # KIND, either express or implied.  See the License for the
< # specific language governing permissions and limitations
< # under the License.
< 
< # pylint: disable=too-many-instance-attributes, too-many-arguments, protected-access, too-many-branches
< # pylint: disable=too-many-public-methods
< """A `Module` implement the `BaseModule` API by wrapping a `Symbol` and one or
< more `Executor` for data parallelization.
< """
< 
< import logging
< import warnings
< 
< from .. import context as ctx
< from .. import optimizer as opt
< from .. import ndarray as nd
< 
< from .executor_group import DataParallelExecutorGroup
< from ..model import _create_kvstore, _initialize_kvstore, _update_params, _update_params_on_kvstore, _init_shnd
< from ..model import load_checkpoint
< from ..initializer import Uniform, InitDesc
< from ..io import DataDesc
< from ..ndarray import zeros
< 
< from .base_module import BaseModule, _check_input_names, _parse_data_desc
< 
< class Module(BaseModule):
<     """Module is a basic module that wrap a `Symbol`. It is functionally the same
<     as the `FeedForward` model, except under the module API.
< 
<     Parameters
<     ----------
<     symbol : Symbol
<     data_names : list of str
<         Defaults to `('data')` for a typical model used in image classification.
<     label_names : list of str
<         Defaults to `('softmax_label')` for a typical model used in image
<         classification.
<     logger : Logger
<         Defaults to `logging`.
<     context : Context or list of Context
<         Defaults to ``mx.cpu()``.
<     work_load_list : list of number
<         Default ``None``, indicating uniform workload.
<     fixed_param_names: list of str
<         Default ``None``, indicating no network parameters are fixed.
<     state_names : list of str
<         states are similar to data and label, but not provided by data iterator.
<         Instead they are initialized to 0 and can be set by `set_states()`.
<     group2ctxs : dict of str to context or list of context,
<                  or list of dict of str to context
<         Default is `None`. Mapping the `ctx_group` attribute to the context assignment.
<     compression_params : dict
<         Specifies type of gradient compression and additional arguments depending
<         on the type of compression being used. For example, 2bit compression requires a threshold.
<         Arguments would then be {'type':'2bit', 'threshold':0.5}
<         See mxnet.KVStore.set_gradient_compression method for more details on gradient compression.
<     """
<     def __init__(self, symbol, data_names=('data',), label_names=('softmax_label',),
<                  logger=logging, context=ctx.cpu(), work_load_list=None,
<                  fixed_param_names=None, state_names=None, group2ctxs=None,
<                  compression_params=None):
<         super(Module, self).__init__(logger=logger)
< 
<         if isinstance(context, ctx.Context):
<             context = [context]
<         self._context = context
<         if work_load_list is None:
<             work_load_list = [1] * len(self._context)
<         assert len(work_load_list) == len(self._context)
<         self._work_load_list = work_load_list
< 
<         self._group2ctxs = group2ctxs
< 
<         self._symbol = symbol
< 
<         data_names = list(data_names) if data_names is not None else []
<         label_names = list(label_names) if label_names is not None else []
<         state_names = list(state_names) if state_names is not None else []
<         fixed_param_names = list(fixed_param_names) if fixed_param_names is not None else []
< 
<         _check_input_names(symbol, data_names, "data", True)
<         _check_input_names(symbol, label_names, "label", False)
<         _check_input_names(symbol, state_names, "state", True)
<         _check_input_names(symbol, fixed_param_names, "fixed_param", True)
< 
<         arg_names = symbol.list_arguments()
<         input_names = data_names + label_names + state_names
<         self._param_names = [x for x in arg_names if x not in input_names]
<         self._fixed_param_names = fixed_param_names
<         self._aux_names = symbol.list_auxiliary_states()
<         self._data_names = data_names
<         self._label_names = label_names
<         self._state_names = state_names
<         self._output_names = symbol.list_outputs()
< 
<         self._arg_params = None
<         self._aux_params = None
<         self._params_dirty = False
< 
<         self._compression_params = compression_params
<         self._optimizer = None
<         self._kvstore = None
<         self._update_on_kvstore = None
<         self._updater = None
<         self._preload_opt_states = None
<         self._grad_req = None
< 
<         self._exec_group = None
<         self._data_shapes = None
<         self._label_shapes = None
<         self.shnd = []
< 
<     @staticmethod
<     def load(prefix, epoch, load_optimizer_states=False, **kwargs):
<         """Creates a model from previously saved checkpoint.
< 
<         Parameters
<         ----------
<         prefix : str
<             path prefix of saved model files. You should have
<             "prefix-symbol.json", "prefix-xxxx.params", and
<             optionally "prefix-xxxx.states", where xxxx is the
<             epoch number.
<         epoch : int
<             epoch to load.
<         load_optimizer_states : bool
<             whether to load optimizer states. Checkpoint needs
<             to have been made with save_optimizer_states=True.
<         data_names : list of str
<             Default is `('data')` for a typical model used in image classification.
<         label_names : list of str
<             Default is `('softmax_label')` for a typical model used in image
<             classification.
<         logger : Logger
<             Default is `logging`.
<         context : Context or list of Context
<             Default is ``cpu()``.
<         work_load_list : list of number
<             Default ``None``, indicating uniform workload.
<         fixed_param_names: list of str
<             Default ``None``, indicating no network parameters are fixed.
<         """
<         sym, args, auxs = load_checkpoint(prefix, epoch)
<         mod = Module(symbol=sym, **kwargs)
<         mod._arg_params = args
<         mod._aux_params = auxs
<         mod.params_initialized = True
<         if load_optimizer_states:
<             mod._preload_opt_states = '%s-%04d.states'%(prefix, epoch)
<         return mod
< 
<     def save_checkpoint(self, prefix, epoch, save_optimizer_states=False):
<         """Saves current progress to checkpoint.
<         Use `mx.callback.module_checkpoint` as `epoch_end_callback` to save during training.
< 
<         Parameters
<         ----------
<         prefix : str
<             The file prefix to checkpoint to.
<         epoch : int
<             The current epoch number.
<         save_optimizer_states : bool
<             Whether to save optimizer states to continue training.
<         """
<         self._symbol.save('%s-symbol.json'%prefix)
<         param_name = '%s-%04d.params' % (prefix, epoch)
<         self.save_params(param_name)
<         logging.info('Saved checkpoint to \"%s\"', param_name)
<         if save_optimizer_states:
<             state_name = '%s-%04d.states' % (prefix, epoch)
<             self.save_optimizer_states(state_name)
<             logging.info('Saved optimizer state to \"%s\"', state_name)
< 
<     def _reset_bind(self):
<         """Internal function to reset binded state."""
<         self.binded = False
<         self._exec_group = None
<         self._data_shapes = None
<         self._label_shapes = None
< 
<     @property
<     def data_names(self):
<         """A list of names for data required by this module."""
<         return self._data_names
< 
<     @property
<     def label_names(self):
<         """A list of names for labels required by this module."""
<         return self._label_names
< 
<     @property
<     def output_names(self):
<         """A list of names for the outputs of this module."""
<         return self._output_names
< 
<     @property
<     def data_shapes(self):
<         """Gets data shapes.
< 
<         Returns
<         -------
<         A list of `(name, shape)` pairs.
<         """
<         assert self.binded
<         return self._data_shapes
< 
<     @property
<     def label_shapes(self):
<         """Gets label shapes.
< 
<         Returns
<         -------
<         A list of `(name, shape)` pairs.
<             The return value could be ``None`` if
<             the module does not need labels, or if the module is not bound for
<             training (in this case, label information is not available).
<         """
<         assert self.binded
<         return self._label_shapes
< 
<     @property
<     def output_shapes(self):
<         """Gets output shapes.
< 
<         Returns
<         -------
<         A list of `(name, shape)` pairs.
<         """
<         assert self.binded
<         return self._exec_group.get_output_shapes()
< 
<     def get_params(self):
<         """Gets current parameters.
< 
<         Returns
<         -------
<         `(arg_params, aux_params)`
<             A pair of dictionaries each mapping parameter names to NDArray values.
<         """
<         assert self.binded and self.params_initialized
< 
<         if self._params_dirty:
<             self._sync_params_from_devices()
<         return (self._arg_params, self._aux_params)
< 
<     def init_params(self, initializer=Uniform(0.01), arg_params=None, aux_params=None,
<                     allow_missing=False, force_init=False, allow_extra=False):
<         """Initializes the parameters and auxiliary states.
< 
<         Parameters
<         ----------
<         initializer : Initializer
<             Called to initialize parameters if needed.
<         arg_params : dict
<             If not ``None``, should be a dictionary of existing arg_params. Initialization
<             will be copied from that.
<         aux_params : dict
<             If not ``None``, should be a dictionary of existing aux_params. Initialization
<             will be copied from that.
<         allow_missing : bool
<             If ``True``, params could contain missing values, and the initializer will be
<             called to fill those missing params.
<         force_init : bool
<             If ``True``, will force re-initialize even if already initialized.
<         allow_extra : boolean, optional
<             Whether allow extra parameters that are not needed by symbol.
<             If this is True, no error will be thrown when arg_params or aux_params
<             contain extra parameters that is not needed by the executor.
<         """
<         if self.params_initialized and not force_init:
<             warnings.warn("Parameters already initialized and force_init=False. "
<                           "init_params call ignored.", stacklevel=2)
<             return
<         assert self.binded, 'call bind before initializing the parameters'
< 
<         def _impl(name, arr, cache):
<             """Internal helper for parameter initialization"""
<             if cache is not None:
<                 if name in cache:
<                     cache_arr = cache[name]
< 
<                     # just in case the cached array is just the target itself
<                     if cache_arr is not arr:
<                         cache_arr.copyto(arr)
<                 else:
<                     if not allow_missing:
<                         raise RuntimeError("%s is not presented" % name)
<                     if initializer is not None:
<                         initializer(name, arr)
<             else:
<                 initializer(name, arr)
< 
<         attrs = self._symbol.attr_dict()
<         for name, arr in sorted(self._arg_params.items()):
<             desc = InitDesc(name, attrs.get(name, None))
<             _impl(desc, arr, arg_params)
< 
<         for name, arr in sorted(self._aux_params.items()):
<             desc = InitDesc(name, attrs.get(name, None))
<             _impl(desc, arr, aux_params)
< 
<         self.params_initialized = True
<         self._params_dirty = False
< 
<         # copy the initialized parameters to devices
<         self._exec_group.set_params(self._arg_params, self._aux_params,
<                                     allow_extra=allow_extra)
< 
<     def set_params(self, arg_params, aux_params, allow_missing=False, force_init=True,
<                    allow_extra=False):
<         """Assigns parameter and aux state values.
< 
<         Parameters
<         ----------
<         arg_params : dict
<             Dictionary of name to `NDArray`.
<         aux_params : dict
<             Dictionary of name to `NDArray`.
<         allow_missing : bool
<             If ``True``, params could contain missing values, and the initializer will be
<             called to fill those missing params.
<         force_init : bool
<             If ``True``, will force re-initialize even if already initialized.
<         allow_extra : boolean, optional
<             Whether allow extra parameters that are not needed by symbol.
<             If this is True, no error will be thrown when arg_params or aux_params
<             contain extra parameters that is not needed by the executor.
<         Examples
<         --------
<         >>> # An example of setting module parameters.
<         >>> sym, arg_params, aux_params = mx.model.load_checkpoint(model_prefix, n_epoch_load)
<         >>> mod.set_params(arg_params=arg_params, aux_params=aux_params)
<         """
<         if not allow_missing:
<             self.init_params(initializer=None, arg_params=arg_params, aux_params=aux_params,
<                              allow_missing=allow_missing, force_init=force_init,
<                              allow_extra=allow_extra)
<             return
< 
<         if self.params_initialized and not force_init:
<             warnings.warn("Parameters already initialized and force_init=False. "
<                           "set_params call ignored.", stacklevel=2)
<             return
< 
<         self._exec_group.set_params(arg_params, aux_params, allow_extra=allow_extra)
< 
<         # because we didn't update self._arg_params, they are dirty now.
<         self._params_dirty = True
<         self.params_initialized = True
< 
<     def bind(self, data_shapes, label_shapes=None, for_training=True,
<              inputs_need_grad=False, force_rebind=False, shared_module=None,
<              grad_req='write'):
<         """Binds the symbols to construct executors. This is necessary before one
<         can perform computation with the module.
< 
<         Parameters
<         ----------
<         data_shapes : list of (str, tuple)
<             Typically is ``data_iter.provide_data``.
<         label_shapes : list of (str, tuple)
<             Typically is ``data_iter.provide_label``.
<         for_training : bool
<             Default is ``True``. Whether the executors should be bound for training.
<         inputs_need_grad : bool
<             Default is ``False``. Whether the gradients to the input data need to be computed.
<             Typically this is not needed. But this might be needed when implementing composition
<             of modules.
<         force_rebind : bool
<             Default is ``False``. This function does nothing if the executors are already
<             bound. But with this ``True``, the executors will be forced to rebind.
<         shared_module : Module
<             Default is ``None``. This is used in bucketing. When not ``None``, the shared module
<             essentially corresponds to a different bucket -- a module with different symbol
<             but with the same sets of parameters (e.g. unrolled RNNs with different lengths).
<         """
<         # force rebinding is typically used when one want to switch from
<         # training to prediction phase.
<         if force_rebind:
<             self._reset_bind()
< 
<         if self.binded:
<             self.logger.warning('Already bound, ignoring bind()')
<             return
< 
<         self.for_training = for_training
<         self.inputs_need_grad = inputs_need_grad
<         self.binded = True
<         self._grad_req = grad_req
< 
<         if not for_training:
<             assert not inputs_need_grad
<         else:
<             pass
<             # this is not True, as some module might not contains a loss function
<             # that consumes the labels
<             # assert label_shapes is not None
< 
<         self._data_shapes, self._label_shapes = _parse_data_desc(
<             self.data_names, self.label_names, data_shapes, label_shapes)
< 
<         if shared_module is not None:
<             assert isinstance(shared_module, Module) and \
<                     shared_module.binded and shared_module.params_initialized
<             shared_group = shared_module._exec_group
<             assert len(shared_group.execs) >= len(self._context)
<         else:
<             shared_group = None
< 
<         self._exec_group = DataParallelExecutorGroup(self._symbol, self._context,
<                                                      self._work_load_list, self._data_shapes,
<                                                      self._label_shapes, self._param_names,
<                                                      for_training, inputs_need_grad,
<                                                      shared_group, logger=self.logger,
<                                                      fixed_param_names=self._fixed_param_names,
<                                                      grad_req=grad_req, group2ctxs=self._group2ctxs,
<                                                      state_names=self._state_names)
<         self._total_exec_bytes = self._exec_group._total_exec_bytes
<         if shared_module is not None:
<             self.params_initialized = True
<             self._arg_params = shared_module._arg_params
<             self._aux_params = shared_module._aux_params
<         elif self.params_initialized:
<             # if the parameters are already initialized, we are re-binding
<             # so automatically copy the already initialized params
<             self._exec_group.set_params(self._arg_params, self._aux_params)
<         else:
<             assert self._arg_params is None and self._aux_params is None
<             param_arrays = [
<                 zeros(shape=x[0].shape, dtype=x[0].dtype, stype=x[0].stype)
<                 for x in self._exec_group.param_arrays
<             ]
<             self._arg_params = {name:arr for name, arr in zip(self._param_names, param_arrays)}
< 
<             aux_arrays = [
<                 zeros(x[0].shape, dtype=x[0].dtype)
<                 for x in self._exec_group.aux_arrays
<             ]
<             self._aux_params = {name:arr for name, arr in zip(self._aux_names, aux_arrays)}
< 
<         if shared_module is not None and shared_module.optimizer_initialized:
<             self.borrow_optimizer(shared_module)
< 
<     def reshape(self, data_shapes, label_shapes=None):
<         """Reshapes the module for new input shapes.
< 
<         Parameters
<         ----------
<         data_shapes : list of (str, tuple)
<             Typically is ``data_iter.provide_data``.
<         label_shapes : list of (str, tuple)
<             Typically is ``data_iter.provide_label``.
<         """
<         assert self.binded
<         self._data_shapes, self._label_shapes = _parse_data_desc(
<             self.data_names, self.label_names, data_shapes, label_shapes)
< 
<         self._exec_group.reshape(self._data_shapes, self._label_shapes)
< 
<     def init_optimizer(self, kvstore='local', optimizer='sgd',
<                        optimizer_params=(('learning_rate', 0.01),), force_init=False):
<         """Installs and initializes optimizers.
< 
<         Parameters
<         ----------
<         kvstore : str or KVStore
<             Default `'local'`.
<         optimizer : str or Optimizer
<             Default `'sgd'`
<         optimizer_params : dict
<             Default `(('learning_rate', 0.01),)`. The default value is not a dictionary,
<             just to avoid pylint warning of dangerous default values.
<         force_init : bool
<             Default ``False``, indicating whether we should force re-initializing the
<             optimizer in the case an optimizer is already installed.
<         """
<         assert self.binded and self.params_initialized
< 
<         if self.optimizer_initialized and not force_init:
<             self.logger.warning('optimizer already initialized, ignoring...')
<             return
< 
<         if self._params_dirty:
<             self._sync_params_from_devices()
< 
<         (kvstore, update_on_kvstore) = \
<                 _create_kvstore(kvstore, len(self._context), self._arg_params)
< 
<         batch_size = self._exec_group.batch_size
<         if kvstore and 'dist' in kvstore.type and '_sync' in kvstore.type:
<             batch_size *= kvstore.num_workers
<         rescale_grad = 1.0/batch_size
< 
<         if isinstance(optimizer, str):
<             idx2name = {}
<             if update_on_kvstore:
<                 idx2name.update(enumerate(self._exec_group.param_names))
<             else:
<                 for k in range(len(self._context)):
<                     idx2name.update({i*len(self._context)+k: n
<                                      for i, n in enumerate(self._exec_group.param_names)})
<             optimizer_params = dict(optimizer_params)
<             if 'rescale_grad' not in optimizer_params:
<                 optimizer_params['rescale_grad'] = rescale_grad
<             optimizer = opt.create(optimizer,
<                                    sym=self.symbol, param_idx2name=idx2name,
<                                    **optimizer_params)
<         else:
<             assert isinstance(optimizer, opt.Optimizer)
<             if optimizer.rescale_grad != rescale_grad:
<                 #pylint: disable=no-member
<                 warnings.warn(
<                     "Optimizer created manually outside Module but rescale_grad " +
<                     "is not normalized to 1.0/batch_size/num_workers (%s vs. %s). "%(
<                         optimizer.rescale_grad, rescale_grad) +
<                     "Is this intended?", stacklevel=2)
< 
<         self._optimizer = optimizer
<         self._kvstore = kvstore
<         self._update_on_kvstore = update_on_kvstore
<         self._updater = None
< 
<         if kvstore:
<             if self._compression_params:
<                 kvstore.set_gradient_compression(self._compression_params)
<             if update_on_kvstore:
<                 kvstore.set_optimizer(self._optimizer)
<             # copy initialized local parameters to kvstore
<             _initialize_kvstore(kvstore=kvstore,
<                                 param_arrays=self._exec_group.param_arrays,
<                                 arg_params=self._arg_params,
<                                 param_names=self._param_names,
<                                 update_on_kvstore=update_on_kvstore)
< 
<         if not update_on_kvstore:
<             self._updater = opt.get_updater(optimizer)
< 
<         self.optimizer_initialized = True
< 
<         if self._preload_opt_states is not None:
<             self.load_optimizer_states(self._preload_opt_states)
<             self._preload_opt_states = None
< 
<     def borrow_optimizer(self, shared_module):
<         """Borrows optimizer from a shared module. Used in bucketing, where exactly the same
<         optimizer (esp. kvstore) is used.
< 
<         Parameters
<         ----------
<         shared_module : Module
<         """
<         assert shared_module.optimizer_initialized
<         self._optimizer = shared_module._optimizer
<         self._kvstore = shared_module._kvstore
<         self._update_on_kvstore = shared_module._update_on_kvstore
<         self._updater = shared_module._updater
<         self.optimizer_initialized = True
< 
<     def forward(self, data_batch, is_train=None):
<         """Forward computation. It supports data batches with different shapes, such as
<         different batch sizes or different image sizes.
<         If reshaping of data batch relates to modification of symbol or module, such as
<         changing image layout ordering or switching from training to predicting, module
<         rebinding is required.
< 
<         See Also
<         ----------
<         :meth:`BaseModule.forward`.
< 
<         Parameters
<         ----------
<         data_batch : DataBatch
<             Could be anything with similar API implemented.
<         is_train : bool
<             Default is ``None``, which means ``is_train`` takes the value of ``self.for_training``.
<         """
<         assert self.binded and self.params_initialized
< 
<         curr_data_shapes = tuple(i.shape for i in self._data_shapes)
<         new_data_shapes = tuple(i.shape for i in data_batch.data)
< 
<         if curr_data_shapes != new_data_shapes:
<             if hasattr(data_batch, "provide_data") and data_batch.provide_data:
<                 new_dshape = data_batch.provide_data
<             else:
<                 new_dshape = [DataDesc(i.name, shape, i.dtype, i.layout) \
<                               for i, shape in zip(self._data_shapes, new_data_shapes)]
< 
<             if hasattr(data_batch, "provide_label") and data_batch.provide_label:
<                 new_lshape = data_batch.provide_label
<             elif hasattr(data_batch, "label") and data_batch.label:
<                 new_lshape = [DataDesc(i.name, j.shape, i.dtype, i.layout) \
<                               for i, j in zip(self._label_shapes, data_batch.label)]
<             else:
<                 new_lshape = None
< 
<             self.reshape(new_dshape, new_lshape)
< 
<         self._exec_group.forward(data_batch, is_train)
< 
<     def backward(self, out_grads=None):
<         """Backward computation.
< 
<         See Also
<         ----------
<         :meth:`BaseModule.backward`.
< 
<         Parameters
<         ----------
<         out_grads : NDArray or list of NDArray, optional
<             Gradient on the outputs to be propagated back.
<             This parameter is only needed when bind is called
<             on outputs that are not a loss function.
<         """
<         assert self.binded and self.params_initialized
<         self._exec_group.backward(out_grads=out_grads)
< 
<     def update(self, work_id):
<         """Updates parameters according to the installed optimizer and the gradients computed
<         in the previous forward-backward batch.
< 
<         When KVStore is used to update parameters for multi-device or multi-machine training,
<         a copy of the parameters are stored in KVStore. Note that for `row_sparse` parameters,
<         this function does update the copy of parameters in KVStore, but doesn't broadcast the
<         updated parameters to all devices / machines. Please call `prepare` to broadcast
<         `row_sparse` parameters with the next batch of data.
< 
<         See Also
<         ----------
<         :meth:`BaseModule.update`.
<         """
<         assert self.binded and self.params_initialized and self.optimizer_initialized
< 
<         self._params_dirty = True
<         if self._update_on_kvstore:
<             _update_params_on_kvstore(self._exec_group.param_arrays,
<                                       self._exec_group.grad_arrays,
<                                       self._kvstore, self._exec_group.param_names)
<         else:
<             if len(self.shnd)==0 :
<                 _init_shnd(self._exec_group.param_arrays,
<                         self._exec_group.grad_arrays, self.shnd, work_id)
<          #   print(self.shnd)
<             _update_params(self._exec_group.param_arrays,
<                            self._exec_group.grad_arrays, work_id,
<                            updater=self._updater,
<                            num_device=len(self._context),
<                            kvstore=self._kvstore,
<                            param_names=self._exec_group.param_names, nd=self.shnd)
< 
<     def get_outputs(self, merge_multi_context=True):
<         """Gets outputs of the previous forward computation.
< 
<         If ``merge_multi_context`` is ``True``, it is like ``[out1, out2]``. Otherwise, it
<         is like ``[[out1_dev1, out1_dev2], [out2_dev1, out2_dev2]]``. All the output
<         elements are `NDArray`. When `merge_multi_context` is `False`, those `NDArray`
<         might live on different devices.
< 
<         Parameters
<         ----------
<         merge_multi_context : bool
<             Default is ``True``. In the case when data-parallelism is used, the outputs
<             will be collected from multiple devices. A ``True`` value indicate that we
<             should merge the collected results so that they look like from a single
<             executor.
< 
<         Returns
<         -------
<         list of NDArray or list of list of NDArray
<             Output.
<         """
<         assert self.binded and self.params_initialized
<         return self._exec_group.get_outputs(merge_multi_context=merge_multi_context)
< 
<     def get_input_grads(self, merge_multi_context=True):
<         """Gets the gradients with respect to the inputs of the module.
< 
<         If ``merge_multi_context`` is ``True``, it is like ``[grad1, grad2]``. Otherwise, it
<         is like ``[[grad1_dev1, grad1_dev2], [grad2_dev1, grad2_dev2]]``. All the output
<         elements are `NDArray`.
< 
<         Parameters
<         ----------
<         merge_multi_context : bool
<             Default is ``True``. In the case when data-parallelism is used, the outputs
<             will be collected from multiple devices. A ``True`` value indicate that we
<             should merge the collected results so that they look like from a single
<             executor.
< 
<         Returns
<         -------
<         list of NDArray or list of list of NDArray
<               Input gradients
<         """
<         assert self.binded and self.params_initialized and self.inputs_need_grad
<         return self._exec_group.get_input_grads(merge_multi_context=merge_multi_context)
< 
<     def get_states(self, merge_multi_context=True):
<         """Gets states from all devices.
< 
<         If `merge_multi_context` is ``True``, it is like ``[out1, out2]``. Otherwise, it
<         is like ``[[out1_dev1, out1_dev2], [out2_dev1, out2_dev2]]``. All the output
<         elements are `NDArray`.
< 
<         Parameters
<         ----------
<         merge_multi_context : bool
<             Default is ``True``. In the case when data-parallelism is used, the states
<             will be collected from multiple devices. A ``True`` value indicate that we
<             should merge the collected results so that they look like from a single
<             executor.
< 
<         Returns
<         -------
<         list of NDArray or list of list of NDArray
<             States
<         """
<         assert self.binded and self.params_initialized
<         return self._exec_group.get_states(merge_multi_context=merge_multi_context)
< 
<     def set_states(self, states=None, value=None):
<         """Sets value for states. Only one of the states & value can be specified.
< 
<         Parameters
<         ----------
<         states : list of list of NDArrays
<             source states arrays formatted like ``[[state1_dev1, state1_dev2],
<             [state2_dev1, state2_dev2]]``.
<         value : number
<             a single scalar value for all state arrays.
<         """
<         assert self.binded and self.params_initialized
<         self._exec_group.set_states(states, value)
< 
<     def update_metric(self, eval_metric, labels):
<         """Evaluates and accumulates evaluation metric on outputs of the last forward computation.
< 
<         See Also
<         ----------
<         :meth:`BaseModule.update_metric`.
< 
<         Parameters
<         ----------
<         eval_metric : EvalMetric
<         labels : list of NDArray
<             Typically ``data_batch.label``.
<         """
<         self._exec_group.update_metric(eval_metric, labels)
< 
<     def _sync_params_from_devices(self):
<         """Synchronizes parameters from devices to CPU. This function should be called after
<         calling `update` that updates the parameters on the devices, before one can read the
<         latest parameters from ``self._arg_params`` and ``self._aux_params``.
< 
<         For row_sparse parameters on devices, ther are pulled from KVStore with all row ids.
< 
<         """
<         self._exec_group.get_params(self._arg_params, self._aux_params)
<         if self._kvstore and self._update_on_kvstore:
<             for param_name, param_val in sorted(self._arg_params.items()):
<                 if param_val.stype == 'row_sparse':
<                     row_ids = nd.arange(0, param_val.shape[0], dtype='int64')
<                     self._kvstore.row_sparse_pull(param_name, param_val, row_ids=row_ids)
<         self._params_dirty = False
< 
<     def save_optimizer_states(self, fname):
<         """Saves optimizer (updater) state to a file.
< 
<         Parameters
<         ----------
<         fname : str
<             Path to output states file.
<         """
<         assert self.optimizer_initialized
< 
<         if self._update_on_kvstore:
<             self._kvstore.save_optimizer_states(fname)
<         else:
<             with open(fname, 'wb') as fout:
<                 fout.write(self._updater.get_states())
< 
<     def load_optimizer_states(self, fname):
<         """Loads optimizer (updater) state from a file.
< 
<         Parameters
<         ----------
<         fname : str
<             Path to input states file.
<         """
<         assert self.optimizer_initialized
< 
<         if self._update_on_kvstore:
<             self._kvstore.load_optimizer_states(fname)
<         else:
<             self._updater.set_states(open(fname, 'rb').read())
< 
<     def install_monitor(self, mon):
<         """Installs monitor on all executors. """
<         assert self.binded
<         self._exec_group.install_monitor(mon)
< 
<     def prepare(self, data_batch, sparse_row_id_fn=None):
<         '''Prepares the module for processing a data batch.
< 
<         Usually involves switching bucket and reshaping.
<         For modules that contain `row_sparse` parameters in KVStore,
<         it prepares the `row_sparse` parameters based on the sparse_row_id_fn.
< 
<         When KVStore is used to update parameters for multi-device or multi-machine training,
<         a copy of the parameters are stored in KVStore. Note that for `row_sparse` parameters,
<         the `update()` updates the copy of parameters in KVStore, but doesn't broadcast
<         the updated parameters to all devices / machines. The `prepare` function is used to
<         broadcast `row_sparse` parameters with the next batch of data.
< 
<         Parameters
<         ----------
<         data_batch : DataBatch
<             The current batch of data for forward computation.
< 
<         sparse_row_id_fn : A callback function
<             The function  takes `data_batch` as an input and returns a dict of
<             str -> NDArray. The resulting dict is used for pulling row_sparse
<             parameters from the kvstore, where the str key is the name of the param,
<             and the value is the row id of the param to pull.
<         '''
<         assert self.binded
<         if sparse_row_id_fn is not None:
<             if not self._kvstore or not self._update_on_kvstore:
<                 warnings.warn(UserWarning("Parameters are not updated in the KVStore. "
<                                           "No need to call sparse_row_id_fn."))
<             else:
<                 row_ids = sparse_row_id_fn(data_batch)
<                 assert(isinstance(row_ids, dict)), "Expected dict output from sparse_row_id_fn"
<                 for param_name, row_id in row_ids.items():
<                     param_idx = self._exec_group.param_names.index(param_name)
<                     param_val = self._exec_group.param_arrays[param_idx]
<                     assert(isinstance(param_val, (tuple, list)))
<                     if param_val[0].stype != 'row_sparse':
<                         warnings.warn(UserWarning("%s.stype is not 'row_sparse'. No need to "
<                                                   "perform row_sparse_pull." % param_name))
<                     else:
<                         self._kvstore.row_sparse_pull(param_name, param_val, row_ids=row_id,
<                                                       priority=-param_idx)
---
> # Licensed to the Apache Software Foundation (ASF) under one
> # or more contributor license agreements.  See the NOTICE file
> # distributed with this work for additional information
> # regarding copyright ownership.  The ASF licenses this file
> # to you under the Apache License, Version 2.0 (the
> # "License"); you may not use this file except in compliance
> # with the License.  You may obtain a copy of the License at
> #
> #   http://www.apache.org/licenses/LICENSE-2.0
> #
> # Unless required by applicable law or agreed to in writing,
> # software distributed under the License is distributed on an
> # "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
> # KIND, either express or implied.  See the License for the
> # specific language governing permissions and limitations
> # under the License.
> 
> # pylint: disable=too-many-instance-attributes, too-many-arguments, protected-access, too-many-branches
> # pylint: disable=too-many-public-methods
> """A `Module` implement the `BaseModule` API by wrapping a `Symbol` and one or
> more `Executor` for data parallelization.
> """
> 
> import logging
> import warnings
> 
> from .. import context as ctx
> from .. import optimizer as opt
> from .. import ndarray as nd
> 
> from .executor_group import DataParallelExecutorGroup
> from ..model import _create_kvstore, _initialize_kvstore, _update_params, _update_params_on_kvstore
> from ..model import load_checkpoint
> from ..initializer import Uniform, InitDesc
> from ..io import DataDesc
> from ..ndarray import zeros
> 
> from .base_module import BaseModule, _check_input_names, _parse_data_desc
> 
> class Module(BaseModule):
>     """Module is a basic module that wrap a `Symbol`. It is functionally the same
>     as the `FeedForward` model, except under the module API.
> 
>     Parameters
>     ----------
>     symbol : Symbol
>     data_names : list of str
>         Defaults to `('data')` for a typical model used in image classification.
>     label_names : list of str
>         Defaults to `('softmax_label')` for a typical model used in image
>         classification.
>     logger : Logger
>         Defaults to `logging`.
>     context : Context or list of Context
>         Defaults to ``mx.cpu()``.
>     work_load_list : list of number
>         Default ``None``, indicating uniform workload.
>     fixed_param_names: list of str
>         Default ``None``, indicating no network parameters are fixed.
>     state_names : list of str
>         states are similar to data and label, but not provided by data iterator.
>         Instead they are initialized to 0 and can be set by `set_states()`.
>     group2ctxs : dict of str to context or list of context,
>                  or list of dict of str to context
>         Default is `None`. Mapping the `ctx_group` attribute to the context assignment.
>     compression_params : dict
>         Specifies type of gradient compression and additional arguments depending
>         on the type of compression being used. For example, 2bit compression requires a threshold.
>         Arguments would then be {'type':'2bit', 'threshold':0.5}
>         See mxnet.KVStore.set_gradient_compression method for more details on gradient compression.
>     """
>     def __init__(self, symbol, data_names=('data',), label_names=('softmax_label',),
>                  logger=logging, context=ctx.cpu(), work_load_list=None,
>                  fixed_param_names=None, state_names=None, group2ctxs=None,
>                  compression_params=None):
>         super(Module, self).__init__(logger=logger)
> 
>         if isinstance(context, ctx.Context):
>             context = [context]
>         self._context = context
>         if work_load_list is None:
>             work_load_list = [1] * len(self._context)
>         assert len(work_load_list) == len(self._context)
>         self._work_load_list = work_load_list
> 
>         self._group2ctxs = group2ctxs
> 
>         self._symbol = symbol
> 
>         data_names = list(data_names) if data_names is not None else []
>         label_names = list(label_names) if label_names is not None else []
>         state_names = list(state_names) if state_names is not None else []
>         fixed_param_names = list(fixed_param_names) if fixed_param_names is not None else []
> 
>         _check_input_names(symbol, data_names, "data", True)
>         _check_input_names(symbol, label_names, "label", False)
>         _check_input_names(symbol, state_names, "state", True)
>         _check_input_names(symbol, fixed_param_names, "fixed_param", True)
> 
>         arg_names = symbol.list_arguments()
>         input_names = data_names + label_names + state_names
>         self._param_names = [x for x in arg_names if x not in input_names]
>         self._fixed_param_names = fixed_param_names
>         self._aux_names = symbol.list_auxiliary_states()
>         self._data_names = data_names
>         self._label_names = label_names
>         self._state_names = state_names
>         self._output_names = symbol.list_outputs()
> 
>         self._arg_params = None
>         self._aux_params = None
>         self._params_dirty = False
> 
>         self._compression_params = compression_params
>         self._optimizer = None
>         self._kvstore = None
>         self._update_on_kvstore = None
>         self._updater = None
>         self._preload_opt_states = None
>         self._grad_req = None
> 
>         self._exec_group = None
>         self._data_shapes = None
>         self._label_shapes = None
> 
>     @staticmethod
>     def load(prefix, epoch, load_optimizer_states=False, **kwargs):
>         """Creates a model from previously saved checkpoint.
> 
>         Parameters
>         ----------
>         prefix : str
>             path prefix of saved model files. You should have
>             "prefix-symbol.json", "prefix-xxxx.params", and
>             optionally "prefix-xxxx.states", where xxxx is the
>             epoch number.
>         epoch : int
>             epoch to load.
>         load_optimizer_states : bool
>             whether to load optimizer states. Checkpoint needs
>             to have been made with save_optimizer_states=True.
>         data_names : list of str
>             Default is `('data')` for a typical model used in image classification.
>         label_names : list of str
>             Default is `('softmax_label')` for a typical model used in image
>             classification.
>         logger : Logger
>             Default is `logging`.
>         context : Context or list of Context
>             Default is ``cpu()``.
>         work_load_list : list of number
>             Default ``None``, indicating uniform workload.
>         fixed_param_names: list of str
>             Default ``None``, indicating no network parameters are fixed.
>         """
>         sym, args, auxs = load_checkpoint(prefix, epoch)
>         mod = Module(symbol=sym, **kwargs)
>         mod._arg_params = args
>         mod._aux_params = auxs
>         mod.params_initialized = True
>         if load_optimizer_states:
>             mod._preload_opt_states = '%s-%04d.states'%(prefix, epoch)
>         return mod
> 
>     def save_checkpoint(self, prefix, epoch, save_optimizer_states=False, remove_amp_cast=True):
>         """Saves current progress to checkpoint.
>         Use `mx.callback.module_checkpoint` as `epoch_end_callback` to save during training.
> 
>         Parameters
>         ----------
>         prefix : str
>             The file prefix to checkpoint to.
>         epoch : int
>             The current epoch number.
>         save_optimizer_states : bool
>             Whether to save optimizer states to continue training.
>         """
>         self._symbol.save('%s-symbol.json'%prefix, remove_amp_cast=remove_amp_cast)
>         param_name = '%s-%04d.params' % (prefix, epoch)
>         self.save_params(param_name)
>         logging.info('Saved checkpoint to \"%s\"', param_name)
>         if save_optimizer_states:
>             state_name = '%s-%04d.states' % (prefix, epoch)
>             self.save_optimizer_states(state_name)
>             logging.info('Saved optimizer state to \"%s\"', state_name)
> 
>     def _reset_bind(self):
>         """Internal function to reset binded state."""
>         self.binded = False
>         self._exec_group = None
>         self._data_shapes = None
>         self._label_shapes = None
> 
>     @property
>     def data_names(self):
>         """A list of names for data required by this module."""
>         return self._data_names
> 
>     @property
>     def label_names(self):
>         """A list of names for labels required by this module."""
>         return self._label_names
> 
>     @property
>     def output_names(self):
>         """A list of names for the outputs of this module."""
>         return self._output_names
> 
>     @property
>     def data_shapes(self):
>         """Gets data shapes.
> 
>         Returns
>         -------
>         A list of `(name, shape)` pairs.
>         """
>         assert self.binded
>         return self._data_shapes
> 
>     @property
>     def label_shapes(self):
>         """Gets label shapes.
> 
>         Returns
>         -------
>         A list of `(name, shape)` pairs.
>             The return value could be ``None`` if
>             the module does not need labels, or if the module is not bound for
>             training (in this case, label information is not available).
>         """
>         assert self.binded
>         return self._label_shapes
> 
>     @property
>     def output_shapes(self):
>         """Gets output shapes.
> 
>         Returns
>         -------
>         A list of `(name, shape)` pairs.
>         """
>         assert self.binded
>         return self._exec_group.get_output_shapes()
> 
>     def get_params(self):
>         """Gets current parameters.
> 
>         Returns
>         -------
>         `(arg_params, aux_params)`
>             A pair of dictionaries each mapping parameter names to NDArray values.
>         """
>         assert self.params_initialized
> 
>         if self._params_dirty:
>             self._sync_params_from_devices()
>         return (self._arg_params, self._aux_params)
> 
>     def init_params(self, initializer=Uniform(0.01), arg_params=None, aux_params=None,
>                     allow_missing=False, force_init=False, allow_extra=False):
>         """Initializes the parameters and auxiliary states.
> 
>         Parameters
>         ----------
>         initializer : Initializer
>             Called to initialize parameters if needed.
>         arg_params : dict
>             If not ``None``, should be a dictionary of existing arg_params. Initialization
>             will be copied from that.
>         aux_params : dict
>             If not ``None``, should be a dictionary of existing aux_params. Initialization
>             will be copied from that.
>         allow_missing : bool
>             If ``True``, params could contain missing values, and the initializer will be
>             called to fill those missing params.
>         force_init : bool
>             If ``True``, will force re-initialize even if already initialized.
>         allow_extra : boolean, optional
>             Whether allow extra parameters that are not needed by symbol.
>             If this is True, no error will be thrown when arg_params or aux_params
>             contain extra parameters that is not needed by the executor.
>         """
>         if self.params_initialized and not force_init:
>             warnings.warn("Parameters already initialized and force_init=False. "
>                           "init_params call ignored.", stacklevel=2)
>             return
>         assert self.binded, 'call bind before initializing the parameters'
> 
>         def _impl(name, arr, cache):
>             """Internal helper for parameter initialization"""
>             if cache is not None:
>                 if name in cache:
>                     cache_arr = cache[name]
> 
>                     # just in case the cached array is just the target itself
>                     if cache_arr is not arr:
>                         cache_arr.copyto(arr)
>                 else:
>                     if not allow_missing:
>                         raise RuntimeError("%s is not presented" % name)
>                     if initializer is not None:
>                         initializer(name, arr)
>             else:
>                 initializer(name, arr)
> 
>         attrs = self._symbol.attr_dict()
>         for name, arr in sorted(self._arg_params.items()):
>             desc = InitDesc(name, attrs.get(name, None))
>             _impl(desc, arr, arg_params)
> 
>         for name, arr in sorted(self._aux_params.items()):
>             desc = InitDesc(name, attrs.get(name, None))
>             _impl(desc, arr, aux_params)
> 
>         self.params_initialized = True
>         self._params_dirty = False
> 
>         # copy the initialized parameters to devices
>         self._exec_group.set_params(self._arg_params, self._aux_params,
>                                     allow_extra=allow_extra)
> 
>     def set_params(self, arg_params, aux_params, allow_missing=False, force_init=True,
>                    allow_extra=False):
>         """Assigns parameter and aux state values.
> 
>         Parameters
>         ----------
>         arg_params : dict
>             Dictionary of name to `NDArray`.
>         aux_params : dict
>             Dictionary of name to `NDArray`.
>         allow_missing : bool
>             If ``True``, params could contain missing values, and the initializer will be
>             called to fill those missing params.
>         force_init : bool
>             If ``True``, will force re-initialize even if already initialized.
>         allow_extra : boolean, optional
>             Whether allow extra parameters that are not needed by symbol.
>             If this is True, no error will be thrown when arg_params or aux_params
>             contain extra parameters that is not needed by the executor.
>         Examples
>         --------
>         >>> # An example of setting module parameters.
>         >>> sym, arg_params, aux_params = mx.model.load_checkpoint(model_prefix, n_epoch_load)
>         >>> mod.set_params(arg_params=arg_params, aux_params=aux_params)
>         """
>         if not allow_missing:
>             self.init_params(initializer=None, arg_params=arg_params, aux_params=aux_params,
>                              allow_missing=allow_missing, force_init=force_init,
>                              allow_extra=allow_extra)
>             return
> 
>         if self.params_initialized and not force_init:
>             warnings.warn("Parameters already initialized and force_init=False. "
>                           "set_params call ignored.", stacklevel=2)
>             return
> 
>         self._exec_group.set_params(arg_params, aux_params, allow_extra=allow_extra)
> 
>         # because we didn't update self._arg_params, they are dirty now.
>         self._params_dirty = True
>         self.params_initialized = True
> 
>     def bind(self, data_shapes, label_shapes=None, for_training=True,
>              inputs_need_grad=False, force_rebind=False, shared_module=None,
>              grad_req='write'):
>         """Binds the symbols to construct executors. This is necessary before one
>         can perform computation with the module.
> 
>         Parameters
>         ----------
>         data_shapes : list of (str, tuple)
>             Typically is ``data_iter.provide_data``.
>         label_shapes : list of (str, tuple)
>             Typically is ``data_iter.provide_label``.
>         for_training : bool
>             Default is ``True``. Whether the executors should be bound for training.
>         inputs_need_grad : bool
>             Default is ``False``. Whether the gradients to the input data need to be computed.
>             Typically this is not needed. But this might be needed when implementing composition
>             of modules.
>         force_rebind : bool
>             Default is ``False``. This function does nothing if the executors are already
>             bound. But with this ``True``, the executors will be forced to rebind.
>         shared_module : Module
>             Default is ``None``. This is used in bucketing. When not ``None``, the shared module
>             essentially corresponds to a different bucket -- a module with different symbol
>             but with the same sets of parameters (e.g. unrolled RNNs with different lengths).
>         """
>         # force rebinding is typically used when one want to switch from
>         # training to prediction phase.
>         if force_rebind:
>             self._reset_bind()
> 
>         if self.binded:
>             self.logger.warning('Already bound, ignoring bind()')
>             return
> 
>         self.for_training = for_training
>         self.inputs_need_grad = inputs_need_grad
>         self._grad_req = grad_req
> 
>         if not for_training:
>             assert not inputs_need_grad
>         else:
>             pass
>             # this is not True, as some module might not contains a loss function
>             # that consumes the labels
>             # assert label_shapes is not None
> 
>         self._data_shapes, self._label_shapes = _parse_data_desc(
>             self.data_names, self.label_names, data_shapes, label_shapes)
> 
>         if shared_module is not None:
>             assert isinstance(shared_module, Module) and \
>                     shared_module.binded and shared_module.params_initialized
>             shared_group = shared_module._exec_group
>             assert len(shared_group.execs) >= len(self._context)
>         else:
>             shared_group = None
>         self._exec_group = DataParallelExecutorGroup(self._symbol, self._context,
>                                                      self._work_load_list, self._data_shapes,
>                                                      self._label_shapes, self._param_names,
>                                                      for_training, inputs_need_grad,
>                                                      shared_group, logger=self.logger,
>                                                      fixed_param_names=self._fixed_param_names,
>                                                      grad_req=grad_req, group2ctxs=self._group2ctxs,
>                                                      state_names=self._state_names)
>         self._total_exec_bytes = self._exec_group._total_exec_bytes
>         if shared_module is not None:
>             self.params_initialized = True
>             self._arg_params = shared_module._arg_params
>             self._aux_params = shared_module._aux_params
>         elif self.params_initialized:
>             # if the parameters are already initialized, we are re-binding
>             # so automatically copy the already initialized params
>             self._exec_group.set_params(self._arg_params, self._aux_params)
>         else:
>             assert self._arg_params is None and self._aux_params is None
>             param_arrays = [
>                 zeros(shape=x[0].shape, dtype=x[0].dtype, stype=x[0].stype)
>                 for x in self._exec_group.param_arrays
>             ]
>             self._arg_params = {name:arr for name, arr in zip(self._param_names, param_arrays)}
> 
>             aux_arrays = [
>                 zeros(x[0].shape, dtype=x[0].dtype)
>                 for x in self._exec_group.aux_arrays
>             ]
>             self._aux_params = {name:arr for name, arr in zip(self._aux_names, aux_arrays)}
> 
>         if shared_module is not None and shared_module.optimizer_initialized:
>             self.borrow_optimizer(shared_module)
> 
>         self.binded = True
>         #print(self._arg_params)
>         #while True:
>         #    pass
> 
>     def reshape(self, data_shapes, label_shapes=None):
>         """Reshapes the module for new input shapes.
> 
>         Parameters
>         ----------
>         data_shapes : list of (str, tuple)
>             Typically is ``data_iter.provide_data``.
>         label_shapes : list of (str, tuple)
>             Typically is ``data_iter.provide_label``.
>         """
>         assert self.binded
>         self._data_shapes, self._label_shapes = _parse_data_desc(
>             self.data_names, self.label_names, data_shapes, label_shapes)
> 
>         self._exec_group.reshape(self._data_shapes, self._label_shapes)
> 
>     def init_optimizer(self, kvstore='local', optimizer='sgd',
>                        optimizer_params=(('learning_rate', 0.01),), force_init=False):
>         """Installs and initializes optimizers.
> 
>         Parameters
>         ----------
>         kvstore : str or KVStore
>             Default `'local'`.
>         optimizer : str or Optimizer
>             Default `'sgd'`
>         optimizer_params : dict
>             Default `(('learning_rate', 0.01),)`. The default value is not a dictionary,
>             just to avoid pylint warning of dangerous default values.
>         force_init : bool
>             Default ``False``, indicating whether we should force re-initializing the
>             optimizer in the case an optimizer is already installed.
>         """
>         assert self.binded and self.params_initialized
> 
>         if self.optimizer_initialized and not force_init:
>             self.logger.warning('optimizer already initialized, ignoring...')
>             return
> 
>         if self._params_dirty:
>             self._sync_params_from_devices()
> 
>         (kvstore, update_on_kvstore) = \
>                 _create_kvstore(kvstore, len(self._context), self._arg_params)
> 
>         batch_size = self._exec_group.batch_size
>         if kvstore and 'dist' in kvstore.type and '_sync' in kvstore.type:
>             batch_size *= kvstore.num_workers
>         rescale_grad = 1.0/batch_size
> 
>         idx2name = {}
>         if update_on_kvstore:
>             idx2name.update(enumerate(self._exec_group.param_names))
>         else:
>             for k in range(len(self._context)):
>                 idx2name.update({i*len(self._context)+k: n
>                                  for i, n in enumerate(self._exec_group.param_names)})
>         if isinstance(optimizer, str):
>             optimizer_params = dict(optimizer_params)
>             if 'rescale_grad' not in optimizer_params:
>                 optimizer_params['rescale_grad'] = rescale_grad
>             optimizer = opt.create(optimizer,
>                                    sym=self.symbol, param_idx2name=idx2name,
>                                    **optimizer_params)
>         else:
>             assert isinstance(optimizer, opt.Optimizer)
>             if optimizer.rescale_grad != rescale_grad:
>                 #pylint: disable=no-member
>                 warnings.warn(
>                     "Optimizer created manually outside Module but rescale_grad " +
>                     "is not normalized to 1.0/batch_size/num_workers (%s vs. %s). "%(
>                         optimizer.rescale_grad, rescale_grad) +
>                     "Is this intended?", stacklevel=2)
>             if not optimizer.idx2name:
>                 optimizer.idx2name = idx2name.copy()
> 
>         self._optimizer = optimizer
>         self._kvstore = kvstore
>         self._update_on_kvstore = update_on_kvstore
>         self._updater = None
> 
>         if kvstore:
>             if self._compression_params:
>                 kvstore.set_gradient_compression(self._compression_params)
>             if update_on_kvstore:
>                 kvstore.set_optimizer(self._optimizer)
>             # copy initialized local parameters to kvstore
>             _initialize_kvstore(kvstore=kvstore,
>                                 param_arrays=self._exec_group.param_arrays,
>                                 arg_params=self._arg_params,
>                                 param_names=self._param_names,
>                                 update_on_kvstore=update_on_kvstore)
> 
>         if not update_on_kvstore:
>             self._updater = opt.get_updater(optimizer)
> 
>         self.optimizer_initialized = True
> 
>         if self._preload_opt_states is not None:
>             self.load_optimizer_states(self._preload_opt_states)
>             self._preload_opt_states = None
> 
>     def borrow_optimizer(self, shared_module):
>         """Borrows optimizer from a shared module. Used in bucketing, where exactly the same
>         optimizer (esp. kvstore) is used.
> 
>         Parameters
>         ----------
>         shared_module : Module
>         """
>         assert shared_module.optimizer_initialized
>         self._optimizer = shared_module._optimizer
>         self._kvstore = shared_module._kvstore
>         self._update_on_kvstore = shared_module._update_on_kvstore
>         self._updater = shared_module._updater
>         self.optimizer_initialized = True
> 
>     def forward(self, data_batch, is_train=None):
>         """Forward computation. It supports data batches with different shapes, such as
>         different batch sizes or different image sizes.
>         If reshaping of data batch relates to modification of symbol or module, such as
>         changing image layout ordering or switching from training to predicting, module
>         rebinding is required.
> 
>         See Also
>         ----------
>         :meth:`BaseModule.forward`.
> 
>         Parameters
>         ----------
>         data_batch : DataBatch
>             Could be anything with similar API implemented.
>         is_train : bool
>             Default is ``None``, which means ``is_train`` takes the value of ``self.for_training``.
>         """
>         assert self.binded and self.params_initialized
> 
>         curr_data_shapes = tuple(i.shape for i in self._data_shapes)
>         if isinstance(data_batch, list):
>             assert data_batch is not None, "Encountered empty data batch"
>             new_data_shapes = []
>             for i in range(len(data_batch[0].data)):
>                 shape = data_batch[0].data[i].shape
>                 for db in data_batch:
>                     assert shape == db.data[i].shape, \
>                         "All data batches in a list need to have the same shape"
>                 new_batch_size = len(data_batch) * shape[0]
>                 new_data_shapes.append((new_batch_size,) + shape[1:])
>             new_data_shapes = tuple(new_data_shapes)
>         else:
>             new_data_shapes = tuple(i.shape for i in data_batch.data)
> 
>         if curr_data_shapes != new_data_shapes:
>             if hasattr(data_batch, "provide_data") and data_batch.provide_data:
>                 new_dshape = data_batch.provide_data
>             else:
>                 new_dshape = [DataDesc(i.name, shape, i.dtype, i.layout) \
>                               for i, shape in zip(self._data_shapes, new_data_shapes)]
> 
>             if hasattr(data_batch, "provide_label") and data_batch.provide_label:
>                 new_lshape = data_batch.provide_label
>             elif hasattr(data_batch, "label") and data_batch.label:
>                 new_lshape = [DataDesc(i.name, j.shape, i.dtype, i.layout) \
>                               for i, j in zip(self._label_shapes, data_batch.label)]
>             else:
>                 new_lshape = None
> 
>             self.reshape(new_dshape, new_lshape)
> 
>         self._exec_group.forward(data_batch, is_train)
>         #for i in range(len(self._exec_group.param_arrays)):
>         #    print(i)
>         #    print(self._exec_group.param_arrays[i])
>         #print(len(self._exec_group.param_arrays))
> 
>     def backward(self, out_grads=None):
>         """Backward computation.
> 
>         See Also
>         ----------
>         :meth:`BaseModule.backward`.
> 
>         Parameters
>         ----------
>         out_grads : NDArray or list of NDArray, optional
>             Gradient on the outputs to be propagated back.
>             This parameter is only needed when bind is called
>             on outputs that are not a loss function.
>         """
>         assert self.binded and self.params_initialized
>         self._exec_group.backward(out_grads=out_grads)
> 
>     def update(self):
>         """Updates parameters according to the installed optimizer and the gradients computed
>         in the previous forward-backward batch.
> 
>         When KVStore is used to update parameters for multi-device or multi-machine training,
>         a copy of the parameters are stored in KVStore. Note that for `row_sparse` parameters,
>         this function does update the copy of parameters in KVStore, but doesn't broadcast the
>         updated parameters to all devices / machines. Please call `prepare` to broadcast
>         `row_sparse` parameters with the next batch of data.
> 
>         See Also
>         ----------
>         :meth:`BaseModule.update`.
>         """
>         assert self.binded and self.params_initialized and self.optimizer_initialized
> 
>         self._params_dirty = True
>         if self._update_on_kvstore:
>             _update_params_on_kvstore(self._exec_group.param_arrays,
>                                       self._exec_group.grad_arrays,
>                                       self._kvstore, self._exec_group.param_names)
>         else:
>             _update_params(self._exec_group.param_arrays,
>                            self._exec_group.grad_arrays,
>                            updater=self._updater,
>                            num_device=len(self._context),
>                            kvstore=self._kvstore,
>                            param_names=self._exec_group.param_names)
> 
>     def get_outputs(self, merge_multi_context=True):
>         """Gets outputs of the previous forward computation.
> 
>         If ``merge_multi_context`` is ``True``, it is like ``[out1, out2]``. Otherwise, it
>         is like ``[[out1_dev1, out1_dev2], [out2_dev1, out2_dev2]]``. All the output
>         elements are `NDArray`. When `merge_multi_context` is `False`, those `NDArray`
>         might live on different devices.
> 
>         Parameters
>         ----------
>         merge_multi_context : bool
>             Default is ``True``. In the case when data-parallelism is used, the outputs
>             will be collected from multiple devices. A ``True`` value indicate that we
>             should merge the collected results so that they look like from a single
>             executor.
> 
>         Returns
>         -------
>         list of NDArray or list of list of NDArray
>             Output.
>         """
>         assert self.binded and self.params_initialized
>         return self._exec_group.get_outputs(merge_multi_context=merge_multi_context)
> 
>     def get_input_grads(self, merge_multi_context=True):
>         """Gets the gradients with respect to the inputs of the module.
> 
>         If ``merge_multi_context`` is ``True``, it is like ``[grad1, grad2]``. Otherwise, it
>         is like ``[[grad1_dev1, grad1_dev2], [grad2_dev1, grad2_dev2]]``. All the output
>         elements are `NDArray`.
> 
>         Parameters
>         ----------
>         merge_multi_context : bool
>             Default is ``True``. In the case when data-parallelism is used, the outputs
>             will be collected from multiple devices. A ``True`` value indicate that we
>             should merge the collected results so that they look like from a single
>             executor.
> 
>         Returns
>         -------
>         list of NDArray or list of list of NDArray
>               Input gradients
>         """
>         assert self.binded and self.params_initialized and self.inputs_need_grad
>         return self._exec_group.get_input_grads(merge_multi_context=merge_multi_context)
> 
>     def get_states(self, merge_multi_context=True):
>         """Gets states from all devices.
> 
>         If `merge_multi_context` is ``True``, it is like ``[out1, out2]``. Otherwise, it
>         is like ``[[out1_dev1, out1_dev2], [out2_dev1, out2_dev2]]``. All the output
>         elements are `NDArray`.
> 
>         Parameters
>         ----------
>         merge_multi_context : bool
>             Default is ``True``. In the case when data-parallelism is used, the states
>             will be collected from multiple devices. A ``True`` value indicate that we
>             should merge the collected results so that they look like from a single
>             executor.
> 
>         Returns
>         -------
>         list of NDArray or list of list of NDArray
>             States
>         """
>         assert self.binded and self.params_initialized
>         return self._exec_group.get_states(merge_multi_context=merge_multi_context)
> 
>     def set_states(self, states=None, value=None):
>         """Sets value for states. Only one of the states & value can be specified.
> 
>         Parameters
>         ----------
>         states : list of list of NDArrays
>             source states arrays formatted like ``[[state1_dev1, state1_dev2],
>             [state2_dev1, state2_dev2]]``.
>         value : number
>             a single scalar value for all state arrays.
>         """
>         assert self.binded and self.params_initialized
>         self._exec_group.set_states(states, value)
> 
>     def update_metric(self, eval_metric, labels, pre_sliced=False):
>         """Evaluates and accumulates evaluation metric on outputs of the last forward computation.
> 
>         See Also
>         ----------
>         :meth:`BaseModule.update_metric`.
> 
>         Parameters
>         ----------
>         eval_metric : EvalMetric
>             Evaluation metric to use.
>         labels : list of NDArray if `pre_sliced` parameter is set to `False`,
>             list of lists of NDArray otherwise. Typically `data_batch.label`.
>         pre_sliced: bool
>             Whether the labels are already sliced per device (default: False).
>         """
>         self._exec_group.update_metric(eval_metric, labels, pre_sliced)
> 
>     def _sync_params_from_devices(self):
>         """Synchronizes parameters from devices to CPU. This function should be called after
>         calling `update` that updates the parameters on the devices, before one can read the
>         latest parameters from ``self._arg_params`` and ``self._aux_params``.
> 
>         For row_sparse parameters on devices, ther are pulled from KVStore with all row ids.
> 
>         """
>         self._exec_group.get_params(self._arg_params, self._aux_params)
>         if self._kvstore and self._update_on_kvstore:
>             for param_name, param_val in sorted(self._arg_params.items()):
>                 if param_val.stype == 'row_sparse':
>                     row_ids = nd.arange(0, param_val.shape[0], dtype='int64')
>                     self._kvstore.row_sparse_pull(param_name, param_val, row_ids=row_ids)
>         self._params_dirty = False
> 
>     def save_optimizer_states(self, fname):
>         """Saves optimizer (updater) state to a file.
> 
>         Parameters
>         ----------
>         fname : str
>             Path to output states file.
>         """
>         assert self.optimizer_initialized
> 
>         if self._update_on_kvstore:
>             self._kvstore.save_optimizer_states(fname)
>         else:
>             with open(fname, 'wb') as fout:
>                 fout.write(self._updater.get_states())
> 
>     def load_optimizer_states(self, fname):
>         """Loads optimizer (updater) state from a file.
> 
>         Parameters
>         ----------
>         fname : str
>             Path to input states file.
>         """
>         assert self.optimizer_initialized
> 
>         if self._update_on_kvstore:
>             self._kvstore.load_optimizer_states(fname)
>         else:
>             self._updater.set_states(open(fname, 'rb').read())
> 
>     def install_monitor(self, mon):
>         """Installs monitor on all executors. """
>         assert self.binded
>         self._exec_group.install_monitor(mon)
> 
>     def prepare(self, data_batch, sparse_row_id_fn=None):
>         '''Prepares the module for processing a data batch.
> 
>         Usually involves switching bucket and reshaping.
>         For modules that contain `row_sparse` parameters in KVStore,
>         it prepares the `row_sparse` parameters based on the sparse_row_id_fn.
> 
>         When KVStore is used to update parameters for multi-device or multi-machine training,
>         a copy of the parameters are stored in KVStore. Note that for `row_sparse` parameters,
>         the `update()` updates the copy of parameters in KVStore, but doesn't broadcast
>         the updated parameters to all devices / machines. The `prepare` function is used to
>         broadcast `row_sparse` parameters with the next batch of data.
> 
>         Parameters
>         ----------
>         data_batch : DataBatch
>             The current batch of data for forward computation.
> 
>         sparse_row_id_fn : A callback function
>             The function  takes `data_batch` as an input and returns a dict of
>             str -> NDArray. The resulting dict is used for pulling row_sparse
>             parameters from the kvstore, where the str key is the name of the param,
>             and the value is the row id of the param to pull.
>         '''
>         assert self.binded
>         if sparse_row_id_fn is not None:
>             if not self._kvstore or not self._update_on_kvstore:
>                 warnings.warn(UserWarning("Parameters are not updated in the KVStore. "
>                                           "No need to call sparse_row_id_fn."))
>             else:
>                 row_ids = sparse_row_id_fn(data_batch)
>                 assert(isinstance(row_ids, dict)), "Expected dict output from sparse_row_id_fn"
>                 for param_name, row_id in row_ids.items():
>                     param_idx = self._exec_group.param_names.index(param_name)
>                     param_val = self._exec_group.param_arrays[param_idx]
>                     assert(isinstance(param_val, (tuple, list)))
>                     if param_val[0].stype != 'row_sparse':
>                         warnings.warn(UserWarning("%s.stype is not 'row_sparse'. No need to "
>                                                   "perform row_sparse_pull." % param_name))
>                     else:
>                         self._kvstore.row_sparse_pull(param_name, param_val, row_ids=row_id,
>                                                       priority=-param_idx)
Only in /home/xxx/diff/: run.sh
diff -r /home/xxx/diff/src/ndarray/ndarray.cc /home/xxx/yyy/incubator-mxnet/src/ndarray/ndarray.cc
98a99,105
> void NDArray::SetShapeFromChunk() {
>   if (Imperative::Get()->is_np_shape() ||
>       !(ptr_->storage_shape.ndim() == 1 && ptr_->storage_shape[0] == 0)) {
>     shape_ = ptr_->storage_shape;
>   }
> }
> 
110,112d116
<   return;
< //  if (!this->is_shared)
<   //  this->shandle.dptr = this->true_handle;
318c322
<   if (shape_.ndim() > 1) {
---
>   if (shape_.ndim() > 1 || Imperative::Get()->is_np_shape()) {
401d404
<   //LOG(INFO) << storage_type << " st " << kDefaultStorage;
431c434
<   // If the memory already uses the speciified layout, don't do anything.
---
>   // If the memory already uses the specified layout, don't do anything.
468,481d470
< void NDArray::SetSharedMem(const mkldnn::memory::primitive_desc &pd, void* handle){
<   ptr_->mkl_mem_ = std::make_shared< MKLDNNMemory>(pd, handle, 1);
<   return;
< }
< 
< int NDArray::TimesVisit() const{
<   return ptr_->mkl_mem_->TimesVisit();
< }
< 
< void NDArray::Visit() const{
<   ptr_->mkl_mem_->Visit();
<   return;
< }
< 
646d634
< 
650,651d637
<   //LOG(INFO) << " is   " << IsView();
<   if (ptr_ != nullptr && ptr_->mkl_mem_!=nullptr && (ptr_->mkl_mem_->TimesVisit() > 0)) return  ptr_->mkl_mem_->GetRaw();
680d665
<     //LOG(INFO) <<" new ";
1230c1215,1218
<   if (from.shape().Size() == 0U) return;
---
>   // zero-size array, no need to copy
>   if (from.shape().Size() == 0U) {
>     return;
>   }
1750c1738
<            " to scope of the code of loading the ndarray.";
---
>            " to scope the code of loading the ndarray.";
1889a1878,1881
>   // zero-size array, no need to copy
>   if (size == 0U) {
>     return;
>   }
2020a2013,2016
>   // zero-size array, no need to copy
>   if (size == 0U) {
>     return;
>   }
2116a2113
> .add_alias("_npi_copyto")
diff -r /home/xxx/diff/src/operator/nn/mkldnn/mkldnn_convolution.cc /home/xxx/yyy/incubator-mxnet/src/operator/nn/mkldnn/mkldnn_convolution.cc
29,34d28
< #define FILENAME "/dev/null"
< 
< #include <unistd.h>
< #include <sys/types.h>
< #include <sys/ipc.h>
< #include <sys/shm.h>
39,40d32
< #include <string>
< #include <stdlib.h>
396d387
< 
402c393
<   if (weight.IsView() && weight.IsMKLDNNData()){
---
>   if (weight.IsView() && weight.IsMKLDNNData())
404d394
<   }
418c408
<       weight_mem = GetWeights(weight, fwd->fwd_pd.weights_primitive_desc(),
---
>     weight_mem = GetWeights(weight, fwd->fwd_pd.weights_primitive_desc(),
423d412
<     //LOG(INFO) << weight.IsDefaultData();
435d423
<     //weight_mem = weight.GetMKLDNNData();
451,505d438
< //  LOG(INFO)<< weight.TimesVisit();
< /*
<   if (weight.TimesVisit() == 0){
<     key_t shm_key = MKLDNNStream::Get()->OpNum();
<     int shm_size = weight_mem->get_primitive_desc().get_size();
<     int shm_id = shmget(shm_key, 10000000, 0644 | IPC_CREAT);
<     if(shm_id == -1){
<       perror("shmget");
<       exit(EXIT_FAILURE);
<     }
<     void* d = (void*)shmat(shm_id, NULL, 0);
<     memcpy(d, weight_mem->get_data_handle(), shm_size);
<     weight_mem->set_data_handle(d);
<   }
<   weight.Visit();    
< */
< /*
<   if (data.TimesVisit() == 0){
<     LOG(INFO) << data.TimesVisit();
<     key_t shm_key = MKLDNNStream::Get()->OpNum();
<     int shm_size = data_mem->get_primitive_desc().get_size();
<     int shm_id = shmget(1, 10000000, 0644 | IPC_CREAT);
<     if(shm_id == -1){
<       perror("shmget");
<       exit(EXIT_FAILURE);
<     }
<     void* d = (void*)shmat(shm_id, NULL, 0);
<     //memcpy(d, data_mem->get_data_handle(), shm_size);
<     data_mem->set_data_handle(d);
<     }
<   data.Visit();
<   */                                          
<  /* 
<   if (data.TimesVisit() == 0){
<     LOG(INFO) << data.TimesVisit();
<     key_t shm_key = MKLDNNStream::Get()->OpNum();
<     int shm_size = data_mem->get_primitive_desc().get_size();
<     //int shm_id = shmget(shm_key, shm_size, 0644 | IPC_CREAT);
<     int shm_id = shmget(1, 10000000, 0644 | IPC_CREAT);
<     if(shm_id == -1){
<       perror("shmget");
<       exit(EXIT_FAILURE);
<     }
<     void* d = (void*)shmat(shm_id, 0, 0);
<     memcpy(d, data_mem->get_data_handle(), shm_size);
<     data_mem->set_data_handle(d);
<   }
< 
< 
<     data.Visit();
< */
< //  for (int i = 0; i < 10; i++)
<   //   LOG(INFO) << weight_mem->get_primitive_desc().get_size() <<  "    " << *(float *)(weight_mem->get_data_handle()+i*4);
< //  LOG(INFO) << "xx";
< 
508c441
<   
---
> 
577,579c510,512
< void SetWeightNewMem(const mkldnn::memory &data,
<                      const mkldnn::memory &out_grad,
<                      const mkldnn::memory &in_grad_weight) {
---
>   void SetWeightNewMem(const mkldnn::memory &data,
>                        const mkldnn::memory &out_grad,
>                        const mkldnn::memory &in_grad_weight) {
694,711d626
<  //LOG(INFO)<< weight.TimesVisit();
< //  weight.Visit();
< /*  if (!weight.IsShared()){
<   const mkldnn::memory *weight_mem;
<     weight_mem = weight.GetMKLDNNData();
<     key_t shm_key = MKLDNNStream::Get()->OpNum();
<     int shm_size = weight_mem->get_primitive_desc().get_size();
<     int shm_id = shmget(1, 100000000, 0644 | IPC_CREAT);
<     int sum_size = MKLDNNStream::Get()->ShareMemSize(shm_size);
<     if(shm_id == -1){
<       perror("shmget");
<       exit(EXIT_FAILURE);
<     }
<     void* d = (void*)shmat(shm_id, NULL, 0);
<     //memcpy(d + sum_size, weight_mem->get_data_handle(), shm_size);
<     weight.SetSharedMem(weight_mem->get_primitive_desc(), d);
<   }
< */
737c652
<   if (req[conv::kWeight]) {
---
>   if (req[conv::kWeight] || req[conv::kBias]) {
750d664
<     mkldnn_output_t in_grad_bias;
753c667
<                               *in_grad_weight.second);
---
>           *in_grad_weight.second);
756c670
<       in_grad_bias = CreateMKLDNNMem(
---
>       auto in_grad_bias = CreateMKLDNNMem(
760c674
<                               *in_grad_weight.second, *in_grad_bias.second);
---
>           *in_grad_weight.second, *in_grad_bias.second);
diff -r /home/xxx/diff/src/operator/nn/mkldnn/mkldnn_convolution-inl.h /home/xxx/yyy/incubator-mxnet/src/operator/nn/mkldnn/mkldnn_convolution-inl.h
73,79d72
< struct MKLDNNPostActParam {
<   mkldnn::algorithm alg = mkldnn::algorithm::algorithm_undef;
<   float scale = 1.f;
<   float alpha = 0.f;
<   float beta = 1.f;
< };
< 
85,86c78,79
<   MKLDNNPostActParam act_param;
<   MKLDNNPostActParam postsum_act_param;
---
>   MKLDNNPostEltwiseParam act_param;
>   MKLDNNPostEltwiseParam postsum_act_param;
diff -r /home/xxx/diff/src/operator/nn/mkldnn/mkldnn_fully_connected.cc /home/xxx/yyy/incubator-mxnet/src/operator/nn/mkldnn/mkldnn_fully_connected.cc
29,33d28
< #include <unistd.h>
< #include <sys/types.h>
< #include <sys/ipc.h>
< #include <sys/shm.h>
< 
52,56c47,51
<   if (full_param.mkldnn_param.with_relu) {
<     const float scale = 1.0f;
<     const float alpha = 0.0f;
<     const float beta = 1.0f;
<     ops.append_eltwise(scale, eltwise_relu, alpha, beta);
---
>   if (full_param.mkldnn_param.with_eltwise) {
>     ops.append_eltwise(full_param.eltwise_param.scale,
>                        full_param.eltwise_param.alg,
>                        full_param.eltwise_param.alpha,
>                        full_param.eltwise_param.beta);
251c246
<   //LOG(INFO) << " x";
---
> 
271,290d265
< /*
<   if (weight.TimesVisit() == 0){
<     key_t shm_key = MKLDNNStream::Get()->OpNum();
<     int shm_size = weight_mem->get_primitive_desc().get_size();
<     //LOG(INFO) << shm_key<< "  " <<shm_size <<"  " << data_mem->get_primitive_desc().get_size();
<     int shm_id = shmget(1, 10000000, 0644 | IPC_CREAT);
<     //        //int sum_size = MKLDNNStream::Get()->ShareMemSize(shm_size);
<     if(shm_id == -1){
<       perror("shmget");
<       exit(EXIT_FAILURE);
<     }
<     void* d = (void*)shmat(shm_id, NULL, 0);
<     memcpy(d, weight_mem->get_data_handle(), shm_size);
<     LOG(INFO) << shm_key << "  " << shm_size;
<     weight_mem->set_data_handle(d);
<     //weight.SetSharedMem(weight_mem->get_primitive_desc(), d);
<   }
<   weight.Visit();
< */
< //  LOG(INFO) << weight.TimesVisit();  
diff -r /home/xxx/diff/src/operator/nn/mkldnn/mkldnn_fully_connected-inl.h /home/xxx/yyy/incubator-mxnet/src/operator/nn/mkldnn/mkldnn_fully_connected-inl.h
43c43
<   bool with_relu;
---
>   bool with_eltwise;
52,53c52,53
<     DMLC_DECLARE_FIELD(with_relu).set_default(false)
<     .describe("Whether there's a post relu after FullyConnected operator");
---
>     DMLC_DECLARE_FIELD(with_eltwise).set_default(false)
>     .describe("Whether there's a post elemwise after FullyConnected operator");
69a70
>   MKLDNNPostEltwiseParam eltwise_param;
88d88
< 
diff -r /home/xxx/diff/src/operator/tensor/indexing_op.cc /home/xxx/yyy/incubator-mxnet/src/operator/tensor/indexing_op.cc
26,29d25
< #include <unistd.h>
< #include <sys/types.h>
< #include <sys/ipc.h>
< #include <sys/shm.h>
85d80
<   //weight.dptr_ = NULL;
88,89c83
< //  auto x = weight.dptr;
<   //LOG(INFO)<< " xx ";
---
> 
94c88
<       Tensor<cpu, 2, DType>  wmat = weight.get<cpu, 2, DType>(s);
---
>       Tensor<cpu, 2, DType> wmat = weight.get<cpu, 2, DType>(s);
109,128d102
<   //LOG(INFO) << " xx";
<   //
< /*
<   if (!weight.isshared()){
<       LOG(INFO) << " x" << weight.shape().Size();
<      //int shm_key = weight.shape().Size() % 100;
<      int shm_key = weight.shape().Size() % 100;
<      //if (weight.shape()[0] > 30000)
<      //   shm_key = 52; 
<      int shm_id = shmget(1, 50000000, 0644 | IPC_CREAT);
<      if(shm_id == -1){
<        perror("shmget");
<        exit(EXIT_FAILURE);
<      }
<      void* d = (void*)shmat(shm_id, NULL, 0);
<      memcpy(d, weight.data().dptr_, weight.shape().Size());
<      weight.shareTB(d);
<   }
< */
< 
494a469
> .add_alias("_npx_embedding")
547c522
< .set_num_outputs(1)
---
> .set_num_outputs(3)
552a528,531
> .set_attr<nnvm::FListOutputNames>("FListOutputNames",
>   [](const NodeAttrs& attrs) {
>     return std::vector<std::string>{"output", "min_output", "max_output"};
>   })
792a772
> .add_alias("_npx_one_hot")
841a822
> .add_alias("_npi_gather_nd")
1034a1016
> .add_alias("_npi_scatter_set_nd")
diff -r /home/xxx/diff/src/operator/tensor/indexing_op.h /home/xxx/yyy/incubator-mxnet/src/operator/tensor/indexing_op.h
57c57
< enum EmbeddingOpOutputs {kOut};
---
> enum EmbeddingOpOutputs {kOut, kMin, kMax};
159d158
< 
160a160,161
>   out_attrs->push_back(mxnet::TShape(1, 1));
>   out_attrs->push_back(mxnet::TShape(1, 1));
170c171
<   CHECK_GE(out_type->size(), 1U);
---
>   CHECK_GE(out_type->size(), 3U);
186a188,191
>   int dtype_out_min = 0;
>   int dtype_out_max = 0;
>   out_type->push_back(dtype_out_min);
>   out_type->push_back(dtype_out_max);
197c202
<   CHECK_EQ(out_attrs->size(), 1U);
---
>   CHECK_EQ(out_attrs->size(), 3U);
200a206,207
>   int& out_stype_min = out_attrs->at(embedding::kMin);
>   int& out_stype_max = out_attrs->at(embedding::kMax);
205a213,216
>     dispatched = storage_type_assign(&out_stype_min, kDefaultStorage,
>                                      dispatch_mode, DispatchMode::kFCompute);
>     dispatched = storage_type_assign(&out_stype_max, kDefaultStorage,
>                                      dispatch_mode, DispatchMode::kFCompute);
455c466
<   CHECK_EQ(outputs.size(), 1U);
---
>   CHECK_EQ(outputs.size(), 3U);
461d471
<   
463a474,477
>   float output_neg_min = -2.345f;
>   float output_pos_max = 2.345f;
>   outputs[embedding::kMin].dptr<float>()[0] = output_neg_min;
>   outputs[embedding::kMax].dptr<float>()[0] = output_pos_max;
477,488d490
< /*
< 
<   if (weight.IsView() && weight.IsMKLDNNData()){
<     weight = weight.Reorder2Default();
<   }
<   weight.Reorder2DefaultAsync();
< 
<    LOG(INFO) << " xx";
<   //weight.MKLDNNDataReorderAsync(fwd->fwd_pd.weights_primitive_desc());
< 
<   LOG(INFO) << (weight.TimesVisit());
<   //weight.Visit();*/
