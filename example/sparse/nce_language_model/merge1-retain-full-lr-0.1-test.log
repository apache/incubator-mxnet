2018-02-15 11:54:00,156 Namespace(batch_size=128, bench=False, bptt=20, checkpoint_dir='./checkpoint0/', checkpoint_interval=1, clip=10.0, data='/home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/*', dense=False, dropout=0.1, emsize=512, epochs=1, eps=0.0, expected_count=True, gpus='0,1,2,3', init=1, k=8192, kvstore='device', load_epoch=-1, log_interval=10, lr=0.1, nhid=2048, nlayers=1, num_proj=512, per_ctx_clip=True, profile=False, py_sampler=False, rescale_embed=True, seed=1, unique=False, vocab='./data/ptb_vocab.txt', wd=0.0, where_minus=True)
train.py:109: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.25 vs. 0.001953125). Is this intended?
  module.init_optimizer(optimizer=optimizer, kvstore=kvstore)
2018-02-15 11:54:11,932 Training started ... 
/home/ubuntu/tf/python/mxnet/ndarray/ndarray.py:1874: RuntimeWarning: You are attempting to copy an array to itself
  warnings.warn('You are attempting to copy an array to itself', RuntimeWarning)
2018-02-15 11:54:18,167 Iter[0] Batch [10]	Speed: 45322.83 samples/sec
2018-02-15 11:54:18,167 Iter[0] Batch [10] 	loss 13.8073230, ppl 991845.8668612
2018-02-15 11:54:21,038 Iter[0] Batch [20]	Speed: 35660.56 samples/sec
2018-02-15 11:54:21,039 Iter[0] Batch [20] 	loss 7.5974347, ppl 1993.0764772
2018-02-15 11:54:22,214 Saved checkpoint to "./checkpoint0/-0000.params"
2018-02-15 11:54:23,138 Saved optimizer state to "./checkpoint0/-0000.states"
2018-02-15 11:54:24,821 eval batch 9 : 12.2409273
2018-02-15 11:54:26,415 eval batch 19 : 12.2601024
2018-02-15 11:54:28,007 eval batch 29 : 12.2697581
2018-02-15 11:54:29,804 eval batch 39 : 12.2699531
2018-02-15 11:54:31,387 eval batch 49 : 12.2708579
2018-02-15 11:54:32,979 eval batch 59 : 12.2750770
2018-02-15 11:54:34,642 eval batch 69 : 12.2734122
2018-02-15 11:54:36,346 eval batch 79 : 12.2768636
2018-02-15 11:54:37,995 eval batch 89 : 12.2826381
2018-02-15 11:54:39,588 eval batch 99 : 12.2821073
2018-02-15 11:54:41,240 eval batch 109 : 12.2873820
2018-02-15 11:54:42,751 eval batch 119 : 12.2818507
2018-02-15 11:54:44,402 eval batch 129 : 12.2817532
2018-02-15 11:54:46,035 eval batch 139 : 12.2812262
2018-02-15 11:54:47,510 eval batch 149 : 12.2806658
2018-02-15 11:54:49,127 eval batch 159 : 12.2792061
2018-02-15 11:54:50,987 eval batch 169 : 12.2768276
2018-02-15 11:54:52,629 eval batch 179 : 12.2763884
2018-02-15 11:54:54,330 eval batch 189 : 12.2738798
2018-02-15 11:54:56,024 eval batch 199 : 12.2756387
2018-02-15 11:54:57,635 eval batch 209 : 12.2746364
2018-02-15 11:54:59,369 eval batch 219 : 12.2749540
2018-02-15 11:55:00,966 eval batch 229 : 12.2761638
2018-02-15 11:55:02,609 eval batch 239 : 12.2739109
2018-02-15 11:55:04,377 eval batch 249 : 12.2646872
2018-02-15 11:55:05,152 Iter[0]		 CE loss 12.0988036, ppl 179656.7994882. Time cost = 41.93 seconds
2018-02-15 11:55:05,152 Training completed. 
['train.py', '--checkpoint-dir=./checkpoint0/', '--data=/home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/*', '--emsize=512', '--eps=0', '--gpus=0,1,2,3', '--nhid=2048', '--num_proj=512', '--per-ctx-clip', '--rescale-embed', '--clip=10', '--lr=0.1', '--expected-count', '--where-minus', '--dropout=0.1', '--log-interval=10', '--epoch=1']
Processing file: /home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Finished processing!
['lstm_l0_h2h_bias', 'lstm_l0_i2h_weight', 'lstm_l0_i2h_bias', 'lstm_l0_pj_weight', 'lstm_l0_h2h_weight']
[]
Processing file: /home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Finished processing!
reset
reset
Processing file: /home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Processing file: /home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
