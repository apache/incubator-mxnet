2018-02-18 01:06:29,476 Namespace(batch_size=128, bench=False, bptt=20, checkpoint_dir='./checkpoint3/', checkpoint_interval=1, clip=10.0, data='/home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/*', dense=False, dropout=0.1, emsize=512, epochs=1, eps=0.0, expected_count=True, gpus='4,5,6,7', init=1, k=8192, kvstore='device', load_epoch=-1, log_interval=10, lr=0.1, nhid=2048, nlayers=1, num_proj=512, per_ctx_clip=True, profile=False, py_sampler=False, rescale_embed=True, seed=1, unique=False, vocab='./data/ptb_vocab.txt', wd=0.0)
train.py:109: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.25 vs. 0.001953125). Is this intended?
  module.init_optimizer(optimizer=optimizer, kvstore=kvstore)
2018-02-18 01:06:39,693 Training started ... 
/home/ubuntu/tf/python/mxnet/ndarray/ndarray.py:1882: RuntimeWarning: You are attempting to copy an array to itself
  warnings.warn('You are attempting to copy an array to itself', RuntimeWarning)
2018-02-18 01:06:45,618 Iter[0] Batch [10]	Speed: 86276.41 samples/sec
2018-02-18 01:06:45,618 Iter[0] Batch [10] 	loss 12.0744873, ppl 175340.8906497
2018-02-18 01:06:47,227 Iter[0] Batch [20]	Speed: 63641.37 samples/sec
2018-02-18 01:06:47,228 Iter[0] Batch [20] 	loss 6.6681348, ppl 786.9264325
2018-02-18 01:06:48,072 Saved checkpoint to "./checkpoint3/-0000.params"
2018-02-18 01:06:49,052 Saved optimizer state to "./checkpoint3/-0000.states"
2018-02-18 01:06:50,714 eval batch 9 : 8.4689388
2018-02-18 01:06:51,803 eval batch 19 : 8.4382759
2018-02-18 01:06:53,096 eval batch 29 : 8.4281812
2018-02-18 01:06:54,305 eval batch 39 : 8.4269713
2018-02-18 01:06:55,562 eval batch 49 : 8.4346321
2018-02-18 01:06:56,914 eval batch 59 : 8.4331932
2018-02-18 01:06:58,234 eval batch 69 : 8.4304521
2018-02-18 01:06:59,617 eval batch 79 : 8.4267343
2018-02-18 01:07:00,666 eval batch 89 : 8.4316520
2018-02-18 01:07:01,824 eval batch 99 : 8.4284665
2018-02-18 01:07:03,023 eval batch 109 : 8.4260297
2018-02-18 01:07:04,341 eval batch 119 : 8.4246044
2018-02-18 01:07:05,879 eval batch 129 : 8.4274305
2018-02-18 01:07:06,986 eval batch 139 : 8.4249784
2018-02-18 01:07:08,023 eval batch 149 : 8.4267817
2018-02-18 01:07:09,191 eval batch 159 : 8.4302651
2018-02-18 01:07:10,761 eval batch 169 : 8.4323939
2018-02-18 01:07:12,258 eval batch 179 : 8.4335472
2018-02-18 01:07:13,752 eval batch 189 : 8.4332870
2018-02-18 01:07:15,107 eval batch 199 : 8.4310564
2018-02-18 01:07:16,345 eval batch 209 : 8.4297005
2018-02-18 01:07:17,754 eval batch 219 : 8.4295216
2018-02-18 01:07:19,078 eval batch 229 : 8.4299111
2018-02-18 01:07:20,425 eval batch 239 : 8.4292319
2018-02-18 01:07:21,894 eval batch 249 : 8.4239045
2018-02-18 01:07:22,365 Iter[0]		 CE loss 8.3095878, ppl 4062.6381339. Time cost = 33.21 seconds
2018-02-18 01:07:22,365 Training completed. 
['train.py', '--checkpoint-dir=./checkpoint3/', '--data=/home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/*', '--emsize=512', '--eps=0', '--gpus=4,5,6,7', '--nhid=2048', '--num_proj=512', '--per-ctx-clip', '--rescale-embed', '--clip=10', '--lr=0.1', '--expected-count', '--dropout=0.1', '--log-interval=10', '--epoch=1']
Processing file: /home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Finished processing!
['lstm_l0_h2h_bias', 'lstm_l0_i2h_weight', 'lstm_l0_i2h_bias', 'lstm_l0_pj_weight', 'lstm_l0_h2h_weight']
[]
Processing file: /home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Finished processing!
reset
reset
Processing file: /home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Processing file: /home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
