ubuntu@ip-172-31-13-31:~/mxnet/example/sparse/nce_language_model$ MXNET_SP_ADAGRAD=1 PYTHONPATH=~/mxnet/python python train.py --nhid 2048 --emsize 256 --batch_size=128 --k=8192 --n
um_proj=256 --dropout=0.01 --optimizer=adagrad --checkpoint-dir=./checkpoint_1_test/ --data=/home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/*  --vocab=./data/1b_w
ord_vocab.txt --lr=0.1 --log-interval=60 --nlayers=1 --clip=1   --checkpoint-interval=10 --kvstore=device --clip-lstm --wd=0 --gpus=1
2018-01-10 06:32:57,990 Namespace(batch_size=128, bench=False, beta1=0.9, bptt=20, checkpoint_dir='./checkpoint_1_test/', checkpoint_interval=10, clip=1.0, clip_lstm=True, data='/ho
me/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/*', dense=False, dropout=0.01, emsize=256, epochs=60, gpus='1', k=8192, kvstore='device', load_epoch=-1, log_interval
=60, lr=0.1, minlr=1e-05, mom=0.0, nhid=2048, nlayers=1, num_proj=256, optimizer='adagrad', profile=False, seed=1, tf_nce=False, vocab='./data/1b_word_vocab.txt', wd=0.0)
Processing file: /home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Finished processing!
['lstm_l0_h2h_bias', 'lstm_l0_i2h_weight', 'lstm_l0_i2h_bias', 'lstm_l0_h2h_weight']
train.py:136: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0078125). Is this intended?
  module.init_optimizer(optimizer=optimizer, kvstore=kvstore)
2018-01-10 06:33:10,426 Training started ...
/home/ubuntu/mxnet/python/mxnet/ndarray/ndarray.py:1874: RuntimeWarning: You are attempting to copy an array to itself
  warnings.warn('You are attempting to copy an array to itself', RuntimeWarning)
2018-01-10 06:33:28,274 Iter[0] Batch [60]      Speed: 11025.76 samples/sec
2018-01-10 06:33:28,274 Iter[0] Batch [60]      loss 185.9118193, ppl -1.0000000
Processing file: /home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Finished processing!
2018-01-10 06:33:41,710 Iter[1] Batch [60]      Speed: 12722.83 samples/sec
2018-01-10 06:33:41,711 Iter[1] Batch [60]      loss 7.1991463, ppl 1338.2878050
Processing file: /home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Finished processing!
2018-01-10 06:33:55,318 Iter[2] Batch [60]      Speed: 12397.17 samples/sec
2018-01-10 06:33:55,319 Iter[2] Batch [60]      loss 6.0093717, ppl 407.2273862
Processing file: /home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Finished processing!
2018-01-10 06:34:09,069 Iter[3] Batch [60]      Speed: 12211.77 samples/sec
2018-01-10 06:34:09,070 Iter[3] Batch [60]      loss 5.3272261, ppl 205.8661339
Processing file: /home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Finished processing!
2018-01-10 06:34:22,487 Iter[4] Batch [60]      Speed: 12552.97 samples/sec
2018-01-10 06:34:22,488 Iter[4] Batch [60]      loss 4.8069371, ppl 122.3562826
Processing file: /home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Finished processing!
2018-01-10 06:34:36,283 Iter[5] Batch [60]      Speed: 12228.74 samples/sec
2018-01-10 06:34:36,284 Iter[5] Batch [60]      loss 4.3454663, ppl 77.1279942
Processing file: /home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Finished processing!
2018-01-10 06:34:49,838 Iter[6] Batch [60]      Speed: 12516.58 samples/sec
2018-01-10 06:34:49,838 Iter[6] Batch [60]      loss 3.9104918, ppl 49.9234961
Processing file: /home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Finished processing!
2018-01-10 06:35:03,519 Iter[7] Batch [60]      Speed: 12337.88 samples/sec
2018-01-10 06:35:03,519 Iter[7] Batch [60]      loss 3.5103873, ppl 33.4612249
Processing file: /home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Finished processing!
2018-01-10 06:35:17,057 Iter[8] Batch [60]      Speed: 12549.85 samples/sec
2018-01-10 06:35:17,057 Iter[8] Batch [60]      loss 3.1521690, ppl 23.3867353
Processing file: /home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Finished processing!
2018-01-10 06:35:30,410 Iter[9] Batch [60]      Speed: 12659.00 samples/sec
2018-01-10 06:35:30,410 Iter[9] Batch [60]      loss 2.8244375, ppl 16.8514634
[]

ubuntu@ip-172-31-13-31:~/mxnet/example/sparse/nce_language_model$ PYTHONPATH=~/mxnet/python python evaluate.py --nhid 2048 --emsize 256 --batch_size=128 --k=8192 --num_proj=256 --dropout=0.01 --gpus=1 --checkpoint-dir=./checkpoint_1_test/ --data=/home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/*  --vocab=./data/1b_word_vocab.txt --log-interval=100 --nlayers=1 --epochs=40
2018-01-10 06:36:48,292 Namespace(batch_size=128, bench=False, bptt=20, checkpoint_dir='./checkpoint_1_test/', data='/home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/*', dense=False, dropout=0.01, emsize=256, epochs=40, eval_every=1, eval_size=32, gpus='1', k=8192, kvstore='device', log_interval=100, nhid=2048, nlayers=1, num_proj=256, profile=False, seed=1, tf_nce=False, vocab='./data/1b_word_vocab.txt')
Processing file: /home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
 Processing file: /home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Finished processing!
Finished processing!
2018-01-10 06:40:48,327 eval batch 99 : 3.5139205
2018-01-10 06:44:40,630 eval batch 199 : 3.3703587
2018-01-10 06:46:47,942 Iter[0] loss 3.1600985, ppl 23.5729170. Time cost = 595.36 seconds


