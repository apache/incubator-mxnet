2018-02-17 04:14:32,414 Namespace(batch_size=128, bench=False, bptt=20, checkpoint_dir='./checkpoint0/', checkpoint_interval=1, clip=10.0, data='/home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/*', dense=False, dropout=0.1, emsize=512, epochs=1, eps=0.0, expected_count=True, gpus='0,1,2,3', init=1, k=8192, kvstore='device', load_epoch=-1, log_interval=10, lr=0.2, nhid=2048, nlayers=1, num_proj=512, per_ctx_clip=True, profile=False, py_sampler=False, rescale_embed=True, seed=1, unique=False, vocab='./data/ptb_vocab.txt', wd=0.0)
train.py:109: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.25 vs. 0.001953125). Is this intended?
  module.init_optimizer(optimizer=optimizer, kvstore=kvstore)
2018-02-17 04:14:43,598 Training started ... 
/home/ubuntu/tf/python/mxnet/ndarray/ndarray.py:1874: RuntimeWarning: You are attempting to copy an array to itself
  warnings.warn('You are attempting to copy an array to itself', RuntimeWarning)
2018-02-17 04:14:49,108 Iter[0] Batch [10]	Speed: 51394.46 samples/sec
2018-02-17 04:14:49,108 Iter[0] Batch [10] 	loss 23.7099878, ppl 19820603074.7031174
2018-02-17 04:14:51,292 Iter[0] Batch [20]	Speed: 46892.42 samples/sec
2018-02-17 04:14:51,292 Iter[0] Batch [20] 	loss 6.9593616, ppl 1052.9611030
2018-02-17 04:14:52,317 Saved checkpoint to "./checkpoint0/-0000.params"
2018-02-17 04:14:53,311 Saved optimizer state to "./checkpoint0/-0000.states"
2018-02-17 04:14:55,108 eval batch 9 : 7.9246016
2018-02-17 04:14:56,430 eval batch 19 : 7.8657692
2018-02-17 04:14:57,972 eval batch 29 : 7.8475947
2018-02-17 04:14:59,323 eval batch 39 : 7.8383023
2018-02-17 04:15:00,688 eval batch 49 : 7.8461934
2018-02-17 04:15:02,163 eval batch 59 : 7.8423382
2018-02-17 04:15:03,248 eval batch 69 : 7.8447748
2018-02-17 04:15:04,727 eval batch 79 : 7.8358006
2018-02-17 04:15:06,078 eval batch 89 : 7.8372996
2018-02-17 04:15:07,129 eval batch 99 : 7.8341514
2018-02-17 04:15:08,685 eval batch 109 : 7.8294066
2018-02-17 04:15:09,854 eval batch 119 : 7.8308602
2018-02-17 04:15:11,255 eval batch 129 : 7.8345904
2018-02-17 04:15:12,592 eval batch 139 : 7.8302860
2018-02-17 04:15:14,016 eval batch 149 : 7.8334825
2018-02-17 04:15:15,552 eval batch 159 : 7.8397655
2018-02-17 04:15:17,114 eval batch 169 : 7.8446603
2018-02-17 04:15:18,293 eval batch 179 : 7.8471026
2018-02-17 04:15:19,716 eval batch 189 : 7.8472421
2018-02-17 04:15:21,053 eval batch 199 : 7.8421341
2018-02-17 04:15:22,429 eval batch 209 : 7.8390147
2018-02-17 04:15:23,730 eval batch 219 : 7.8386368
2018-02-17 04:15:24,853 eval batch 229 : 7.8384195
2018-02-17 04:15:26,136 eval batch 239 : 7.8394568
2018-02-17 04:15:27,368 eval batch 249 : 7.8342840
2018-02-17 04:15:27,790 Iter[0]		 CE loss 7.7278618, ppl 2270.7417824. Time cost = 34.37 seconds
2018-02-17 04:15:27,791 Training completed. 
['train.py', '--checkpoint-dir=./checkpoint0/', '--data=/home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/*', '--emsize=512', '--eps=0', '--gpus=0,1,2,3', '--nhid=2048', '--num_proj=512', '--per-ctx-clip', '--rescale-embed', '--clip=10', '--lr=0.2', '--expected-count', '--dropout=0.1', '--log-interval=10', '--epoch=1']
Processing file: /home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Finished processing!
['lstm_l0_h2h_bias', 'lstm_l0_i2h_weight', 'lstm_l0_i2h_bias', 'lstm_l0_pj_weight', 'lstm_l0_h2h_weight']
[]
Processing file: /home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Finished processing!
reset
reset
Processing file: /home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Processing file: /home/ubuntu/gbw-validation/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050
Exception in thread Thread-1 (most likely raised during interpreter shutdown):