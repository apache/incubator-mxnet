# -*- mode: dockerfile -*-
# dockerfile to build libmxnet.so, and a python wheel for the Jetson TX1/TX2

FROM nvidia/cuda:9.0-cudnn7-devel as cudabuilder

FROM dockcross/linux-arm64

ENV ARCH aarch64
ENV NVCCFLAGS "-m64"
ENV CUDA_ARCH "-gencode arch=compute_53,code=sm_53 -gencode arch=compute_62,code=sm_62"
ENV BUILD_OPTS "USE_OPENCV=0 USE_BLAS=openblas USE_SSE=0 USE_CUDA=1 USE_CUDNN=1 ENABLE_CUDA_RTC=0 USE_NCCL=0 USE_CUDA_PATH=/usr/local/cuda/"
ENV CC /usr/bin/aarch64-linux-gnu-gcc
ENV CXX /usr/bin/aarch64-linux-gnu-g++
ENV FC /usr/bin/aarch64-linux-gnu-gfortran-4.9
ENV HOSTCC gcc

WORKDIR /work

# Build OpenBLAS
ADD https://api.github.com/repos/xianyi/OpenBLAS/git/refs/heads/master /tmp/openblas_version.json
RUN git clone https://github.com/xianyi/OpenBLAS.git && \
    cd OpenBLAS && \
    make -j$(nproc) TARGET=ARMV8 && \
    PREFIX=/usr make install

# Setup CUDA build env (including configuring and copying nvcc)
COPY --from=cudabuilder /usr/local/cuda /usr/local/cuda
ENV PATH $PATH:/usr/local/cuda/bin
ENV TARGET_ARCH aarch64
ENV TARGET_OS linux

# Install ARM depedencies based on Jetpack 3.2
RUN JETPACK_DOWNLOAD_PREFIX=http://developer.download.nvidia.com/devzone/devcenter/mobile/jetpack_l4t/3.2GA/m892ki/JetPackL4T_32_b196/ && \
    ARM_CUDA_INSTALLER_PACKAGE=cuda-repo-l4t-9-0-local_9.0.252-1_arm64.deb && \
    ARM_CUDNN_INSTALLER_PACKAGE=libcudnn7_7.0.5.13-1+cuda9.0_arm64.deb && \
    ARM_CUDNN_DEV_INSTALLER_PACKAGE=libcudnn7-dev_7.0.5.13-1+cuda9.0_arm64.deb && \
    wget -nv $JETPACK_DOWNLOAD_PREFIX/$ARM_CUDA_INSTALLER_PACKAGE && \
    wget -nv $JETPACK_DOWNLOAD_PREFIX/$ARM_CUDNN_INSTALLER_PACKAGE && \
    wget -nv $JETPACK_DOWNLOAD_PREFIX/$ARM_CUDNN_DEV_INSTALLER_PACKAGE && \
    dpkg -i $ARM_CUDA_INSTALLER_PACKAGE && \
    apt-key add /var/cuda-repo-9-0-local/7fa2af80.pub && \
    dpkg -i $ARM_CUDNN_INSTALLER_PACKAGE && \
    dpkg -i $ARM_CUDNN_DEV_INSTALLER_PACKAGE && \
    apt update -y  && \
    apt install -y unzip cuda-cudart-dev-9-0 cuda-cublas-9-0 cuda-nvml-dev-9-0 \
    cuda-nvrtc-dev-9-0 cuda-cufft-dev-9-0 cuda-curand-dev-9-0 cuda-cusolver-9-0 \
    cuda-cusparse-dev-9-0 cuda-misc-headers-9-0 cuda-npp-dev-9-0 libcudnn7 && \
    cp /usr/local/cuda-9.0/targets/aarch64-linux/lib/* /usr/local/cuda/lib64/ && \
    cp /usr/local/cuda-9.0/lib64/* /usr/local/cuda/lib64/ && \
    cp /usr/local/cuda-9.0/targets/aarch64-linux/lib/stubs/*.so /usr/local/cuda/lib64/stubs/ && \
    cp -r /usr/local/cuda-9.0/targets/aarch64-linux/include/ /usr/local/cuda/include/ && \
    rm $ARM_CUDA_INSTALLER_PACKAGE $ARM_CUDNN_INSTALLER_PACKAGE $ARM_CUDNN_DEV_INSTALLER_PACKAGE

# Build MXNet
RUN git clone --recurse https://github.com/apache/incubator-mxnet.git mxnet

WORKDIR /work/mxnet

# Add ARM specific settings
ADD arm.crosscompile.mk make/config.mk

# Build and link
RUN make -j$(nproc) $BUILD_OPTS

# Create a binary wheel for easy installation.
# When using tool.py output will be in the jetson folder.
# Scp the .whl file to your target device, and install via
# pip install
WORKDIR /work/mxnet/python
RUN python setup.py  bdist_wheel --universal

# Copy build artifacts to output folder for tool.py script
RUN mkdir -p /work/build & cp dist/*.whl /work/build && cp ../lib/* /work/build

# Fix pathing issues in the wheel.  We need to move libmxnet.so from the data folder to the root
# of the wheel, then repackage the wheel.
# Create a temp dir to do the work.
WORKDIR /work/build
RUN apt-get install -y unzip && \
    mkdir temp && \
    cp *.whl temp

# Extract the wheel, move the libmxnet.so file, repackage the wheel.
WORKDIR /work/build/temp
RUN unzip *.whl &&  \
    rm *.whl && \
    mv *.data/data/mxnet/libmxnet.so mxnet && \
    zip -r ../temp.zip *

# Replace the existing wheel with our fixed version.
WORKDIR /work/build
RUN rm -rf temp && \
    for f in *.whl; do rm "$f" && mv temp.zip "$f"; done
