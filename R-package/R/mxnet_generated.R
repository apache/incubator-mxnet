######
# Generated by mxnet.export, do not edit by hand.
######

#' Elementwise activation function.
#' 
#' The following activation types are supported (operations are applied elementwisely to each
#' scalar of the input tensor):
#' 
#' - `relu`: Rectified Linear Unit, `y = max(x, 0)`
#' - `sigmoid`: `y = 1 / (1 + exp(-x))`
#' - `tanh`: Hyperbolic tangent, `y = (exp(x) - exp(-x)) / (exp(x) + exp(-x))`
#' - `softrelu`: Soft ReLU, or SoftPlus, `y = log(1 + exp(x))`
#' 
#' See `LeakyReLU` for other activations with parameters.
#' 
#' 
#' @param act.type  {'relu', 'sigmoid', 'softrelu', 'tanh'}, required
#'     Activation function to be applied.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.Activation
NULL

#' Apply batch normalization to input.
#' 
#' @param data  Symbol
#'     Input data to batch normalization
#' @param eps  float, optional, default=0.001
#'     Epsilon to prevent div 0
#' @param momentum  float, optional, default=0.9
#'     Momentum for moving average
#' @param fix.gamma  boolean, optional, default=True
#'     Fix gamma while training
#' @param use.global.stats  boolean, optional, default=False
#'     Whether use global moving statistics instead of local batch-norm. This will force change batch-norm into a scale shift operator.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.BatchNorm
NULL

#' Get output from a symbol and pass 0 gradient back
#' 
#' @param data  Symbol
#'     Input data.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.BlockGrad
NULL

#' Cast array to a different data type.
#' 
#' @param data  Symbol
#'     Input data to cast function.
#' @param dtype  {'float16', 'float32', 'float64', 'int32', 'uint8'}, required
#'     Target data type.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.Cast
NULL

#' Perform an feature concat on channel dim (defaut is 1) over all
#' 
#' @param data  Symbol[]
#'     List of tensors to concatenate
#' @param num.args  int, required
#'     Number of inputs to be concated.
#' @param dim  int, optional, default='1'
#'     the dimension to be concated.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.Concat
NULL

#' Apply convolution to input then add a bias.
#' 
#' @param data  Symbol
#'     Input data to the ConvolutionOp.
#' @param weight  Symbol
#'     Weight matrix.
#' @param bias  Symbol
#'     Bias parameter.
#' @param kernel  Shape(tuple), required
#'     convolution kernel size: (y, x) or (d, y, x)
#' @param stride  Shape(tuple), optional, default=(1,1)
#'     convolution stride: (y, x) or (d, y, x)
#' @param dilate  Shape(tuple), optional, default=(1,1)
#'     convolution dilate: (y, x)
#' @param pad  Shape(tuple), optional, default=(0,0)
#'     pad for convolution: (y, x) or (d, y, x)
#' @param num.filter  int (non-negative), required
#'     convolution filter(channel) number
#' @param num.group  int (non-negative), optional, default=1
#'     Number of groups partition. This option is not supported by CuDNN, you can use SliceChannel to num_group,apply convolution and concat instead to achieve the same need.
#' @param workspace  long (non-negative), optional, default=1024
#'     Tmp workspace for convolution (MB).
#' @param no.bias  boolean, optional, default=False
#'     Whether to disable bias parameter.
#' @param cudnn.tune  {'fastest', 'limited_workspace', 'off'},optional, default='off'
#'     Whether to find convolution algo by running performance test.Leads to higher startup time but may give better speed.auto tune is turned off by default.Set environment varialbe MXNET_CUDNN_AUTOTUNE_DEFAULT=1 to turn on by default.
#' @param cudnn.off  boolean, optional, default=False
#'     Turn off cudnn.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.Convolution
NULL

#' Apply correlation to inputs
#' 
#' @param data1  Symbol
#'     Input data1 to the correlation.
#' @param data2  Symbol
#'     Input data2 to the correlation.
#' @param kernel.size  int (non-negative), optional, default=1
#'     kernel size for Correlation must be an odd number
#' @param max.displacement  int (non-negative), optional, default=1
#'     Max displacement of Correlation 
#' @param stride1  int (non-negative), optional, default=1
#'     stride1 quantize data1 globally
#' @param stride2  int (non-negative), optional, default=1
#'     stride2 quantize data2 within the neighborhood centered around data1
#' @param pad.size  int (non-negative), optional, default=0
#'     pad for Correlation
#' @param is.multiply  boolean, optional, default=True
#'     operation type is either multiplication or subduction
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.Correlation
NULL

#' Crop the 2nd and 3rd dim of input data, with the corresponding size of h_w or with width and height of the second input symbol, i.e., with one input, we need h_w to specify the crop height and width, otherwise the second input symbol's size will be used
#' 
#' @param data  Symbol or Symbol[]
#'     Tensor or List of Tensors, the second input will be used as crop_like shape reference
#' @param num.args  int, required
#'     Number of inputs for crop, if equals one, then we will use the h_wfor crop height and width, else if equals two, then we will use the heightand width of the second input symbol, we name crop_like here
#' @param offset  Shape(tuple), optional, default=(0,0)
#'     crop offset coordinate: (y, x)
#' @param h.w  Shape(tuple), optional, default=(0,0)
#'     crop height and weight: (h, w)
#' @param center.crop  boolean, optional, default=False
#'     If set to true, then it will use be the center_crop,or it will crop using the shape of crop_like
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.Crop
NULL

#' Custom operator implemented in frontend.
#' 
#' @param op.type  string
#'     Type of custom operator. Must be registered first.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.Custom
NULL

#' Apply deconvolution to input then add a bias.
#' 
#' @param data  Symbol
#'     Input data to the DeconvolutionOp.
#' @param weight  Symbol
#'     Weight matrix.
#' @param bias  Symbol
#'     Bias parameter.
#' @param kernel  Shape(tuple), required
#'     deconvolution kernel size: (y, x)
#' @param stride  Shape(tuple), optional, default=(1,1)
#'     deconvolution stride: (y, x)
#' @param pad  Shape(tuple), optional, default=(0,0)
#'     pad for deconvolution: (y, x), a good number is : (kernel-1)/2, if target_shape set, pad will be ignored and will be computed automatically
#' @param adj  Shape(tuple), optional, default=(0,0)
#'     adjustment for output shape: (y, x), if target_shape set, adj will be ignored and will be computed automatically
#' @param target.shape  Shape(tuple), optional, default=(0,0)
#'     output shape with targe shape : (y, x)
#' @param num.filter  int (non-negative), required
#'     deconvolution filter(channel) number
#' @param num.group  int (non-negative), optional, default=1
#'     number of groups partition
#' @param workspace  long (non-negative), optional, default=512
#'     Tmp workspace for deconvolution (MB)
#' @param no.bias  boolean, optional, default=True
#'     Whether to disable bias parameter.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.Deconvolution
NULL

#' Apply dropout to input.
#' During training, each element of the input is randomly set to zero with probability p.
#' And then the whole tensor is rescaled by 1/(1-p) to keep the expectation the same as
#' before applying dropout. During the test time, this behaves as an identity map.
#' 
#' 
#' @param data  Symbol
#'     Input data to dropout.
#' @param p  float, optional, default=0.5
#'     Fraction of the input that gets dropped out at training time
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.Dropout
NULL

#' Perform element sum of inputs
#' 
#' From:src/operator/tensor/elemwise_sum.cc:49
#' 
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.ElementWiseSum
NULL

#' Map integer index to vector representations (embeddings). Those
#' embeddings are learnable parameters. For a input of shape `(d1, ..., dK)`, the
#' output shape is `(d1, ..., dK, output_dim)`. All the input values should be
#' integers in the range `[0, input_dim)`.
#' 
#' @param data  Symbol
#'     Input data to the EmbeddingOp.
#' @param weight  Symbol
#'     Enbedding weight matrix.
#' @param input.dim  int, required
#'     vocabulary size of the input indices.
#' @param output.dim  int, required
#'     dimension of the embedding vectors.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.Embedding
NULL

#' Flatten input into 2D by collapsing all the higher dimensions.
#' A (d1, d2, ..., dK) tensor is flatten to (d1, d2* ... *dK) matrix.
#' 
#' @param data  Symbol
#'     Input data to flatten.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.Flatten
NULL

#' Apply matrix multiplication to input then add a bias.
#' It maps the input of shape `(batch_size, input_dim)` to the shape of
#' `(batch_size, num_hidden)`. Learnable parameters include the weights
#' of the linear transform and an optional bias vector.
#' 
#' @param data  Symbol
#'     Input data to the FullyConnectedOp.
#' @param weight  Symbol
#'     Weight matrix.
#' @param bias  Symbol
#'     Bias parameter.
#' @param num.hidden  int, required
#'     Number of hidden nodes of the output.
#' @param no.bias  boolean, optional, default=False
#'     Whether to disable bias parameter.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.FullyConnected
NULL

#' Apply a sparse regularization to the output a sigmoid activation function.
#' 
#' @param data  Symbol
#'     Input data.
#' @param sparseness.target  float, optional, default=0.1
#'     The sparseness target
#' @param penalty  float, optional, default=0.001
#'     The tradeoff parameter for the sparseness penalty
#' @param momentum  float, optional, default=0.9
#'     The momentum for running average
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.IdentityAttachKLSparseReg
NULL

#' An operator taking in a n-dimensional input tensor (n > 2), and normalizing the input by subtracting the mean and variance calculated over the spatial dimensions. This is an implemention of the operator described in "Instance Normalization: The Missing Ingredient for Fast Stylization", D. Ulyanov, A. Vedaldi, V. Lempitsky, 2016 (arXiv:1607.08022v2). This layer is similar to batch normalization, with two differences: first, the normalization is carried out per example ('instance'), not over a batch. Second, the same normalization is applied both at test and train time. This operation is also known as 'contrast normalization'.
#' 
#' @param data  Symbol
#'     A n-dimensional tensor (n > 2) of the form [batch, channel, spatial_dim1, spatial_dim2, ...].
#' @param gamma  Symbol
#'     A vector of length 'channel', which multiplies the normalized input.
#' @param beta  Symbol
#'     A vector of length 'channel', which is added to the product of the normalized input and the weight.
#' @param eps  float, optional, default=0.001
#'     Epsilon to prevent division by 0.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.InstanceNorm
NULL

#' Set the l2 norm of each instance to a constant.
#' 
#' @param data  Symbol
#'     Input data to the L2NormalizationOp.
#' @param eps  float, optional, default=1e-10
#'     Epsilon to prevent div 0
#' @param mode  {'channel', 'instance', 'spatial'},optional, default='instance'
#'     Normalization Mode. If set to instance, this operator will compute a norm for each instance in the batch; this is the default mode. If set to channel, this operator will compute a cross channel norm at each position of each instance. If set to spatial, this operator will compute a norm for each channel.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.L2Normalization
NULL

#' Apply convolution to input then add a bias.
#' 
#' @param data  Symbol
#'     Input data to the ConvolutionOp.
#' @param alpha  float, optional, default=0.0001
#'     value of the alpha variance scaling parameter in the normalization formula
#' @param beta  float, optional, default=0.75
#'     value of the beta power parameter in the normalization formula
#' @param knorm  float, optional, default=2
#'     value of the k parameter in normalization formula
#' @param nsize  int (non-negative), required
#'     normalization window width in elements.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.LRN
NULL

#' Apply activation function to input.
#' 
#' @param data  Symbol
#'     Input data to activation function.
#' @param act.type  {'elu', 'leaky', 'prelu', 'rrelu'},optional, default='leaky'
#'     Activation function to be applied.
#' @param slope  float, optional, default=0.25
#'     Init slope for the activation. (For leaky and elu only)
#' @param lower.bound  float, optional, default=0.125
#'     Lower bound of random slope. (For rrelu only)
#' @param upper.bound  float, optional, default=0.334
#'     Upper bound of random slope. (For rrelu only)
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.LeakyReLU
NULL

#' Use linear regression for final output, this is used on final output of a net.
#' 
#' @param data  Symbol
#'     Input data to function.
#' @param label  Symbol
#'     Input label to function.
#' @param grad.scale  float, optional, default=1
#'     Scale the gradient by a float factor
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.LinearRegressionOutput
NULL

#' Use Logistic regression for final output, this is used on final output of a net.
#' Logistic regression is suitable for binary classification or probability prediction tasks.
#' 
#' @param data  Symbol
#'     Input data to function.
#' @param label  Symbol
#'     Input label to function.
#' @param grad.scale  float, optional, default=1
#'     Scale the gradient by a float factor
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.LogisticRegressionOutput
NULL

#' Use mean absolute error regression for final output, this is used on final output of a net.
#' 
#' @param data  Symbol
#'     Input data to function.
#' @param label  Symbol
#'     Input label to function.
#' @param grad.scale  float, optional, default=1
#'     Scale the gradient by a float factor
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.MAERegressionOutput
NULL

#' Get output from a symbol and pass 1 gradient back. This is used as a terminal loss if unary and binary operator are used to composite a loss with no declaration of backward dependency
#' 
#' @param data  Symbol
#'     Input data.
#' @param grad.scale  float, optional, default=1
#'     gradient scale as a supplement to unary and binary operators
#' @param valid.thresh  float, optional, default=0
#'     regard element valid when x > valid_thresh, this is used only in valid normalization mode.
#' @param normalization  {'batch', 'null', 'valid'},optional, default='null'
#'     If set to null, op will not normalize on output gradient.If set to batch, op will normalize gradient by divide batch size.If set to valid, op will normalize gradient by divide # sample marked as valid
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.MakeLoss
NULL

#' Pads an n-dimensional input tensor. Allows for precise control of the padding type and how much padding to apply on both sides of a given dimension.
#' 
#' @param data  Symbol
#'     An n-dimensional input tensor.
#' @param mode  {'constant', 'edge'}, required
#'     Padding type to use. "constant" pads all values with a constant value, the value of which can be specified with the constant_value option. "edge" uses the boundary values of the array as padding.
#' @param pad.width  Shape(tuple), required
#'     A tuple of padding widths of length 2*r, where r is the rank of the input tensor, specifying number of values padded to the edges of each axis. (before_1, after_1, ... , before_N, after_N) unique pad widths for each axis. Equivalent to pad_width in numpy.pad, but flattened.
#' @param constant.value  double, optional, default=0
#'     This option is only used when mode is "constant". This value will be used as the padding value. Defaults to 0 if not specified.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.Pad
NULL

#' Perform spatial pooling on inputs.
#' 
#' @param data  Symbol
#'     Input data to the pooling operator.
#' @param global.pool  boolean, optional, default=False
#'     Ignore kernel size, do global pooling based on current input feature map. This is useful for input with different shape
#' @param kernel  Shape(tuple), required
#'     pooling kernel size: (y, x) or (d, y, x)
#' @param pool.type  {'avg', 'max', 'sum'}, required
#'     Pooling type to be applied.
#' @param pooling.convention  {'full', 'valid'},optional, default='valid'
#'     Pooling convention to be applied.kValid is default setting of Mxnet and rounds down the output pooling size.kFull is compatible with Caffe and rounds up the output pooling size.
#' @param stride  Shape(tuple), optional, default=(1,1)
#'     stride: for pooling (y, x) or (d, y, x)
#' @param pad  Shape(tuple), optional, default=(0,0)
#'     pad for pooling: (y, x) or (d, y, x)
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.Pooling
NULL

#' Apply a recurrent layer to input.
#' 
#' @param data  Symbol
#'     Input data to RNN
#' @param parameters  Symbol
#'     Vector of all RNN trainable parameters concatenated
#' @param state  Symbol
#'     initial hidden state of the RNN
#' @param state.cell  Symbol
#'     initial cell state for LSTM networks (only for LSTM)
#' @param state.size  int (non-negative), required
#'     size of the state for each layer
#' @param num.layers  int (non-negative), required
#'     number of stacked layers
#' @param bidirectional  boolean, optional, default=False
#'     whether to use bidirectional recurrent layers
#' @param mode  {'gru', 'lstm', 'rnn_relu', 'rnn_tanh'}, required
#'     the type of RNN to compute
#' @param p  float, optional, default=0
#'     Dropout probability, fraction of the input that gets dropped out at training time
#' @param state.outputs  boolean, optional, default=False
#'     Whether to have the states as symbol outputs.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.RNN
NULL

#' Performs region-of-interest pooling on inputs. Resize bounding box coordinates by spatial_scale and crop input feature maps accordingly. The cropped feature maps are pooled by max pooling to a fixed size output indicated by pooled_size. batch_size will change to the number of region bounding boxes after ROIPooling
#' 
#' @param data  Symbol
#'     Input data to the pooling operator, a 4D Feature maps
#' @param rois  Symbol
#'     Bounding box coordinates, a 2D array of [[batch_index, x1, y1, x2, y2]]. (x1, y1) and (x2, y2) are top left and down right corners of designated region of interest. batch_index indicates the index of corresponding image in the input data
#' @param pooled.size  Shape(tuple), required
#'     fix pooled size: (h, w)
#' @param spatial.scale  float, required
#'     Ratio of input feature map height (or w) to raw image height (or w). Equals the reciprocal of total stride in convolutional layers
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.ROIPooling
NULL

#' Reshape input according to a target shape spec.
#' The target shape is a tuple and can be a simple list of dimensions such as (12,3) or it can incorporate special codes that correspond to contextual operations that refer to the input dimensions.
#' The special codes are all expressed as integers less than 1. These codes effectively refer to a machine that pops input dims off the beginning of the input dims list and pushes resulting output dims onto the end of the output dims list, which starts empty. The codes are:
#'   0  Copy     Pop one input dim and push it onto the output dims
#'  -1  Infer    Push a dim that is inferred later from all other output dims
#'  -2  CopyAll  Pop all remaining input dims and push them onto output dims
#'  -3  Merge2   Pop two input dims, multiply them, and push result
#'  -4  Split2   Pop one input dim, and read two next target shape specs,
#'               push them both onto output dims (either can be -1 and will
#'               be inferred from the other
#'  The exact mathematical behavior of these codes is given in the description of the 'shape' parameter. All non-codes (positive integers) just pop a dim off the input dims (if any), throw it away, and then push the specified integer onto the output dims.
#' Examples:
#' Type     Input      Target            Output
#' Copy     (2,3,4)    (4,0,2)           (4,3,2)
#' Copy     (2,3,4)    (2,0,0)           (2,3,4)
#' Infer    (2,3,4)    (6,1,-1)          (6,1,4)
#' Infer    (2,3,4)    (3,-1,8)          (3,1,8)
#' CopyAll  (9,8,7)    (-2)              (9,8,7)
#' CopyAll  (9,8,7)    (9,-2)            (9,8,7)
#' CopyAll  (9,8,7)    (-2,1,1)          (9,8,7,1,1)
#' Merge2   (3,4)      (-3)              (12)
#' Merge2   (3,4,5)    (-3,0)            (12,5)
#' Merge2   (3,4,5)    (0,-3)            (3,20)
#' Merge2   (3,4,5,6)  (-3,0,0)          (12,5,6)
#' Merge2   (3,4,5,6)  (-3,-2)           (12,5,6)
#' Split2   (12)       (-4,6,2)          (6,2)
#' Split2   (12)       (-4,2,6)          (2,6)
#' Split2   (12)       (-4,-1,6)         (2,6)
#' Split2   (12,9)     (-4,2,6,0)        (2,6,9)
#' Split2   (12,9,9,9) (-4,2,6,-2)       (2,6,9,9,9)
#' Split2   (12,12)    (-4,2,-1,-4,-1,2) (2,6,6,2)
#' 
#' 
#' @param data  Symbol
#'     Input data to reshape.
#' @param target.shape  Shape(tuple), optional, default=(0,0)
#'     (Deprecated! Use shape instead.) Target new shape. One and only one dim can be 0, in which case it will be inferred from the rest of dims
#' @param keep.highest  boolean, optional, default=False
#'     (Deprecated! Use shape instead.) Whether keep the highest dim unchanged.If set to true, then the first dim in target_shape is ignored,and always fixed as input
#' @param shape  , optional, default=()
#'     Target shape, a tuple, t=(t_1,t_2,..,t_m).
#' Let the input dims be s=(s_1,s_2,..,s_n).
#' The output dims u=(u_1,u_2,..,u_p) are computed from s and t.
#' The target shape tuple elements t_i are read in order, and used to  generate successive output dims u_p:
#' t_i:       meaning:      behavior:
#' +ve        explicit      u_p = t_i
#' 0          copy          u_p = s_i
#' -1         infer         u_p = (Prod s_i) / (Prod u_j | j != p)
#' -2         copy all      u_p = s_i, u_p+1 = s_i+1, ...
#' -3         merge two     u_p = s_i * s_i+1
#' -4,a,b     split two     u_p = a, u_p+1 = b | a * b = s_i
#' The split directive (-4) in the target shape tuple is followed by two dimensions, one of which can be -1, which means it will be inferred from the other one and the original dimension.
#' The can only be one globally inferred dimension (-1), aside from any -1 occuring in a split directive.
#' @param reverse  boolean, optional, default=False
#'     Whether to match the shapes from the backward. If reverse is true, 0 values in the `shape` argument will be searched from the backward. E.g the original shape is (10, 5, 4) and the shape argument is (-1, 0). If reverse is true, the new shape should be (50, 4). Otherwise it will be (40, 5).
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.Reshape
NULL

#' Support Vector Machine based transformation on input, backprop L2-SVM
#' 
#' @param data  Symbol
#'     Input data to svm.
#' @param label  Symbol
#'     Label data.
#' @param margin  float, optional, default=1
#'     Scale the DType(param_.margin) for activation size
#' @param regularization.coefficient  float, optional, default=1
#'     Scale the coefficient responsible for balacing coefficient size and error tradeoff
#' @param use.linear  boolean, optional, default=False
#'     If set true, uses L1-SVM objective function. Default uses L2-SVM objective
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.SVMOutput
NULL

#' Takes the last element of a sequence. Takes an n-dimensional tensor of the form [max sequence length, batchsize, other dims] and returns a (n-1)-dimensional tensor of the form [batchsize, other dims]. This operator takes an optional input tensor sequence_length of positive ints of dimension [batchsize] when the sequence_length option is set to true. This allows the operator to handle variable-length sequences. If sequence_length is false, then each example in the batch is assumed to have the max sequence length.
#' 
#' @param data  Symbol
#'     n-dimensional input tensor of the form [max sequence length, batchsize, other dims]
#' @param sequence.length  Symbol
#'     vector of sequence lengths of size batchsize
#' @param use.sequence.length  boolean, optional, default=False
#'     If set to true, this layer takes in extra input sequence_length to specify variable length sequence
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.SequenceLast
NULL

#' Sets all elements outside the sequence to zero. Takes an n-dimensional tensor of the form [max sequence length, batchsize, other dims] and returns a tensor of the same shape. This operator takes an optional input tensor sequence_length of positive ints of dimension [batchsize] when the sequence_length option is set to true. This allows the operator to handle variable-length sequences. If sequence_length is false, then each example in the batch is assumed to have the max sequence length, and this operator becomes the identity operator.
#' 
#' @param data  Symbol
#'     n-dimensional input tensor of the form [max sequence length, batchsize, other dims]
#' @param sequence.length  Symbol
#'     vector of sequence lengths of size batchsize
#' @param use.sequence.length  boolean, optional, default=False
#'     If set to true, this layer takes in extra input sequence_length to specify variable length sequence
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.SequenceMask
NULL

#' Reverses the elements of each sequence. Takes an n-dimensional tensor of the form [max sequence length, batchsize, other dims] and returns a tensor of the same shape. This operator takes an optional input tensor sequence_length of positive ints of dimension [batchsize] when the sequence_length option is set to true. This allows the operator to handle variable-length sequences. If sequence_length is false, then each example in the batch is assumed to have the max sequence length.
#' 
#' @param data  Symbol
#'     n-dimensional input tensor of the form [max sequence length, batchsize, other dims]
#' @param sequence.length  Symbol
#'     vector of sequence lengths of size batchsize
#' @param use.sequence.length  boolean, optional, default=False
#'     If set to true, this layer takes in extra input sequence_length to specify variable length sequence
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.SequenceReverse
NULL

#' Slice input equally along specified axis
#' 
#' @param num.outputs  int, required
#'     Number of outputs to be sliced.
#' @param axis  int, optional, default='1'
#'     Dimension along which to slice.
#' @param squeeze.axis  boolean, optional, default=False
#'     If true AND the sliced dimension becomes 1, squeeze that dimension.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.SliceChannel
NULL

#' DEPRECATED: Perform a softmax transformation on input. Please use SoftmaxOutput
#' 
#' @param data  Symbol
#'     Input data to softmax.
#' @param grad.scale  float, optional, default=1
#'     Scale the gradient by a float factor
#' @param ignore.label  float, optional, default=-1
#'     the label value will be ignored during backward (only works if use_ignore is set to be true).
#' @param multi.output  boolean, optional, default=False
#'     If set to true, for a (n,k,x_1,..,x_n) dimensional input tensor, softmax will generate n*x_1*...*x_n output, each has k classes
#' @param use.ignore  boolean, optional, default=False
#'     If set to true, the ignore_label value will not contribute to the backward gradient
#' @param preserve.shape  boolean, optional, default=False
#'     If true, for a (n_1, n_2, ..., n_d, k) dimensional input tensor, softmax will generate (n1, n2, ..., n_d, k) output, normalizing the k classes as the last dimension.
#' @param normalization  {'batch', 'null', 'valid'},optional, default='null'
#'     If set to null, op will do nothing on output gradient.If set to batch, op will normalize gradient by divide batch sizeIf set to valid, op will normalize gradient by divide sample not ignored
#' @param out.grad  boolean, optional, default=False
#'     Apply weighting from output gradient
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.Softmax
NULL

#' Apply softmax activation to input. This is intended for internal layers. For output (loss layer) please use SoftmaxOutput. If mode=instance, this operator will compute a softmax for each instance in the batch; this is the default mode. If mode=channel, this operator will compute a num_channel-class softmax at each position of each instance; this can be used for fully convolutional network, image segmentation, etc.
#' 
#' @param data  Symbol
#'     Input data to activation function.
#' @param mode  {'channel', 'instance'},optional, default='instance'
#'     Softmax Mode. If set to instance, this operator will compute a softmax for each instance in the batch; this is the default mode. If set to channel, this operator will compute a num_channel-class softmax at each position of each instance; this can be used for fully convolutional network, image segmentation, etc.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.SoftmaxActivation
NULL

#' Perform a softmax transformation on input, backprop with logloss.
#' 
#' @param data  Symbol
#'     Input data to softmax.
#' @param label  Symbol
#'     Label data, can also be probability value with same shape as data
#' @param grad.scale  float, optional, default=1
#'     Scale the gradient by a float factor
#' @param ignore.label  float, optional, default=-1
#'     the label value will be ignored during backward (only works if use_ignore is set to be true).
#' @param multi.output  boolean, optional, default=False
#'     If set to true, for a (n,k,x_1,..,x_n) dimensional input tensor, softmax will generate n*x_1*...*x_n output, each has k classes
#' @param use.ignore  boolean, optional, default=False
#'     If set to true, the ignore_label value will not contribute to the backward gradient
#' @param preserve.shape  boolean, optional, default=False
#'     If true, for a (n_1, n_2, ..., n_d, k) dimensional input tensor, softmax will generate (n1, n2, ..., n_d, k) output, normalizing the k classes as the last dimension.
#' @param normalization  {'batch', 'null', 'valid'},optional, default='null'
#'     If set to null, op will do nothing on output gradient.If set to batch, op will normalize gradient by divide batch sizeIf set to valid, op will normalize gradient by divide sample not ignored
#' @param out.grad  boolean, optional, default=False
#'     Apply weighting from output gradient
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.SoftmaxOutput
NULL

#' Apply spatial transformer to input feature map.
#' 
#' @param data  Symbol
#'     Input data to the SpatialTransformerOp.
#' @param loc  Symbol
#'     localisation net, the output dim should be 6 when transform_type is affine, and the name of loc symbol should better starts with 'stn_loc', so that initialization it with iddentify tranform, or you shold initialize the weight and bias by yourself.
#' @param target.shape  Shape(tuple), optional, default=(0,0)
#'     output shape(h, w) of spatial transformer: (y, x)
#' @param transform.type  {'affine'}, required
#'     transformation type
#' @param sampler.type  {'bilinear'}, required
#'     sampling type
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.SpatialTransformer
NULL

#' Apply swapaxis to input.
#' 
#' @param data  Symbol
#'     Input data to the SwapAxisOp.
#' @param dim1  int (non-negative), optional, default=0
#'     the first axis to be swapped.
#' @param dim2  int (non-negative), optional, default=0
#'     the second axis to be swapped.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.SwapAxis
NULL

#' Perform nearest neighboor/bilinear up sampling to inputs
#' 
#' @param data  Symbol[]
#'     Array of tensors to upsample
#' @param scale  int (non-negative), required
#'     Up sampling scale
#' @param num.filter  int (non-negative), optional, default=0
#'     Input filter. Only used by bilinear sample_type.
#' @param sample.type  {'bilinear', 'nearest'}, required
#'     upsampling method
#' @param multi.input.mode  {'concat', 'sum'},optional, default='concat'
#'     How to handle multiple input. concat means concatenate upsampled images along the channel dimension. sum means add all images together, only available for nearest neighbor upsampling.
#' @param num.args  int, required
#'     Number of inputs to be upsampled. For nearest neighbor upsampling, this can be 1-N; the size of output will be(scale*h_0,scale*w_0) and all other inputs will be upsampled to thesame size. For bilinear upsampling this must be 2; 1 input and 1 weight.
#' @param workspace  long (non-negative), optional, default=512
#'     Tmp workspace for deconvolution (MB)
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.UpSampling
NULL

#' Take absolute value of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:60
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.abs
NULL

#' Updater function for adam optimizer
#' 
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.adam.update
NULL

#' Take arccos of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:212
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.arccos
NULL

#' Take arccosh of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:284
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.arccosh
NULL

#' Take arcsin of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:203
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.arcsin
NULL

#' Take arcsinh of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:275
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.arcsinh
NULL

#' Take arctan of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:221
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.arctan
NULL

#' Take arctanh of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:293
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.arctanh
NULL

#' Compute argmax
#' 
#' From:src/operator/tensor/broadcast_reduce_op.cc:70
#' 
#' @param src  NDArray
#'     Source input
#' @param axis  int, optional, default='-1'
#'     Empty or unsigned. The axis to perform the reduction.If left empty, a global reduction will be performed.
#' @param keepdims  boolean, optional, default=False
#'     If true, the axis which is reduced is left in the result as dimension with size one.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.argmax
NULL

#' argmax_channel
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.argmax.channel
NULL

#' Compute argmin
#' 
#' From:src/operator/tensor/broadcast_reduce_op.cc:74
#' 
#' @param src  NDArray
#'     Source input
#' @param axis  int, optional, default='-1'
#'     Empty or unsigned. The axis to perform the reduction.If left empty, a global reduction will be performed.
#' @param keepdims  boolean, optional, default=False
#'     If true, the axis which is reduced is left in the result as dimension with size one.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.argmin
NULL

#' Calculate batched dot product of two matrices. (batch, M, K) batch_dot (batch, K, N) --> (batch, M, N)
#' 
#' From:src/operator/tensor/matrix_op.cc:176
#' 
#' @param lhs  NDArray
#'     Left input
#' @param rhs  NDArray
#'     Right input
#' @param axis  int, required
#'     The dimension to flip
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.batch.dot
NULL

#' broadcast_add
#' 
#' @param lhs  NDArray
#'     first input
#' @param rhs  NDArray
#'     second input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.broadcast.add
NULL

#' Broadcast src along axis
#' 
#' From:src/operator/tensor/broadcast_reduce_op.cc:43
#' 
#' @param src  NDArray
#'     Source input
#' @param axis  Shape(tuple), optional, default=()
#'     The axes to perform the broadcasting.
#' @param size  Shape(tuple), optional, default=()
#'     Target sizes of the broadcasting axes.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.broadcast.axis
NULL

#' broadcast_div
#' 
#' @param lhs  NDArray
#'     first input
#' @param rhs  NDArray
#'     second input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.broadcast.div
NULL

#' broadcast_hypot
#' 
#' @param lhs  NDArray
#'     first input
#' @param rhs  NDArray
#'     second input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.broadcast.hypot
NULL

#' broadcast_maximum
#' 
#' @param lhs  NDArray
#'     first input
#' @param rhs  NDArray
#'     second input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.broadcast.maximum
NULL

#' broadcast_minimum
#' 
#' @param lhs  NDArray
#'     first input
#' @param rhs  NDArray
#'     second input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.broadcast.minimum
NULL

#' broadcast_minus
#' 
#' @param lhs  NDArray
#'     first input
#' @param rhs  NDArray
#'     second input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.broadcast.minus
NULL

#' broadcast_mul
#' 
#' @param lhs  NDArray
#'     first input
#' @param rhs  NDArray
#'     second input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.broadcast.mul
NULL

#' broadcast_plus
#' 
#' @param lhs  NDArray
#'     first input
#' @param rhs  NDArray
#'     second input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.broadcast.plus
NULL

#' broadcast_power
#' 
#' @param lhs  NDArray
#'     first input
#' @param rhs  NDArray
#'     second input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.broadcast.power
NULL

#' broadcast_sub
#' 
#' @param lhs  NDArray
#'     first input
#' @param rhs  NDArray
#'     second input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.broadcast.sub
NULL

#' Broadcast src to shape
#' 
#' From:src/operator/tensor/broadcast_reduce_op.cc:50
#' 
#' @param src  NDArray
#'     Source input
#' @param shape  Shape(tuple), optional, default=()
#'     The shape of the desired array. We can set the dim to zero if it's same as the original. E.g `A = broadcast_to(B, shape=(10, 0, 0))` has the same meaning as `A = broadcast_axis(B, axis=0, size=10)`.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.broadcast.to
NULL

#' Take ceil of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:83
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.ceil
NULL

#' Choose one element from each line(row for python, column for R/Julia) in lhs according to index indicated by rhs. This function assume rhs uses 0-based index.
#' 
#' @param lhs  NDArray
#'     Left operand to the function.
#' @param rhs  NDArray
#'     Right operand to the function.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.choose.element.0index
NULL

#' Clip ndarray elements to range (a_min, a_max)
#' 
#' @param src  NDArray
#'     Source input
#' @param a.min  real_t
#'     Minimum value
#' @param a.max  real_t
#'     Maximum value
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.clip
NULL

#' Take cos of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:185
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.cos
NULL

#' Take cosh of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:257
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.cosh
NULL

#' (Crop the input tensor and return a new one.
#' 
#' Requirements
#' ------------
#' - the input and output (if explicitly given) are of the same data type,
#'   and on the same device.
#' )
#' 
#' From:src/operator/tensor/matrix_op.cc:69
#' 
#' @param src  NDArray
#'     Source input
#' @param begin  Shape(tuple), required
#'     starting coordinates
#' @param end  Shape(tuple), required
#'     ending coordinates
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.crop
NULL

#' Take degrees of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:230
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.degrees
NULL

#' Calculate dot product of two matrices or two vectors.
#' 
#' From:src/operator/tensor/matrix_op.cc:154
#' 
#' @param lhs  NDArray
#'     Left input
#' @param rhs  NDArray
#'     Right input
#' @param axis  int, required
#'     The dimension to flip
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.dot
NULL

#' elemwise_add
#' 
#' @param lhs  NDArray
#'     first input
#' @param rhs  NDArray
#'     second input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.elemwise.add
NULL

#' Take exp of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:131
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.exp
NULL

#' Expand the shape of array by inserting a new axis.
#' 
#' From:src/operator/tensor/matrix_op.cc:48
#' 
#' @param src  NDArray
#'     Source input
#' @param axis  int (non-negative), required
#'     Position (amongst axes) where new axis is to be inserted.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.expand.dims
NULL

#' Take `exp(x) - 1` in a numerically stable way
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:176
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.expm1
NULL

#' Fill one element of each line(row for python, column for R/Julia) in lhs according to index indicated by rhs and values indicated by mhs. This function assume rhs uses 0-based index.
#' 
#' @param lhs  NDArray
#'     Left operand to the function.
#' @param mhs  NDArray
#'     Middle operand to the function.
#' @param rhs  NDArray
#'     Right operand to the function.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.fill.element.0index
NULL

#' Take round of the src to integer nearest 0
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:98
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.fix
NULL

#' Flip the input tensor along axis and return a new one.
#' 
#' From:src/operator/tensor/matrix_op.cc:142
#' 
#' @param src  NDArray
#'     Source input
#' @param axis  int, required
#'     The dimension to flip
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.flip
NULL

#' Take floor of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:88
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.floor
NULL

#' Take the gamma function (extension of the factorial function) of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:302
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.gamma
NULL

#' Take gammaln (log of the absolute value of gamma(x)) of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:311
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.gammaln
NULL

#' Identity mapping, copy src to output
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:14
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.identity
NULL

#' Take log of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:137
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.log
NULL

#' Take base-10 log of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:143
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.log10
NULL

#' Take `log(1 + x)` in a numerically stable way
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:167
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.log1p
NULL

#' Take base-2 log of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:149
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.log2
NULL

#' Compute max along axis. If axis is empty, global reduction is performed
#' 
#' From:src/operator/tensor/broadcast_reduce_op.cc:25
#' 
#' @param src  NDArray
#'     Source input
#' @param axis  Shape(tuple), optional, default=()
#'     Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.
#' @param keepdims  boolean, optional, default=False
#'     If true, the axis which is reduced is left in the result as dimension with size one.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.max
NULL

#' Compute min along axis. If axis is empty, global reduction is performed
#' 
#' From:src/operator/tensor/broadcast_reduce_op.cc:34
#' 
#' @param src  NDArray
#'     Source input
#' @param axis  Shape(tuple), optional, default=()
#'     Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.
#' @param keepdims  boolean, optional, default=False
#'     If true, the axis which is reduced is left in the result as dimension with size one.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.min
NULL

#' Negate src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:54
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.negative
NULL

#' norm
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.norm
NULL

#' Sample a normal distribution
#' 
#' @param loc  float, optional, default=0
#'     Mean of the distribution.
#' @param scale  float, optional, default=1
#'     Standard deviation of the distribution.
#' @param shape  Shape(tuple), optional, default=()
#'     The shape of the output
#' @param ctx  string, optional, default=''
#'     Context of output, in format [cpu|gpu|cpu_pinned](n).Only used for imperative calls.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.normal
NULL

#' Take radians of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:239
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.radians
NULL

#' Take round of the src to nearest integer
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:93
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.rint
NULL

#' Take round of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:78
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.round
NULL

#' Take reciprocal square root of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:121
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.rsqrt
NULL

#' Updater function for sgd optimizer
#' 
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.sgd.mom.update
NULL

#' Updater function for sgd optimizer
#' 
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.sgd.update
NULL

#' Take sign of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:69
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.sign
NULL

#' Take sin of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:158
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.sin
NULL

#' Take sinh of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:248
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.sinh
NULL

#' Slice the input along certain axis and return a sliced array.
#' 
#' From:src/operator/tensor/matrix_op.cc:120
#' 
#' @param src  NDArray
#'     Source input
#' @param axis  int, required
#'     The axis to be sliced
#' @param begin  int, required
#'     The beginning index to be sliced
#' @param end  int, required
#'     The end index to be sliced
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.slice.axis
NULL

#' Calculate Smooth L1 Loss(lhs, scalar)
#' 
#' From:src/operator/tensor/elemwise_binary_scalar_op.cc:98
#' 
#' @param lhs  NDArray
#'     source input
#' @param scalar  float
#'     scalar input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.smooth.l1
NULL

#' Calculate cross_entropy(lhs, one_hot(rhs))
#' 
#' From:src/operator/loss_binary_op.cc:12
#' 
#' @param data  NDArray
#'     Input data
#' @param label  NDArray
#'     Input label
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.softmax.cross.entropy
NULL

#' Take square root of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:112
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.sqrt
NULL

#' Take square of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:103
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.square
NULL

#' Sum src along axis. If axis is empty, global reduction is performed
#' 
#' From:src/operator/tensor/broadcast_reduce_op.cc:16
#' 
#' @param src  NDArray
#'     Source input
#' @param axis  Shape(tuple), optional, default=()
#'     Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.
#' @param keepdims  boolean, optional, default=False
#'     If true, the axis which is reduced is left in the result as dimension with size one.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.sum
NULL

#' Take tan of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:194
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.tan
NULL

#' Take tanh of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:266
#' 
#' @param src  NDArray
#'     Source input
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.tanh
NULL

#' Transpose the input tensor and return a new one
#' 
#' From:src/operator/tensor/matrix_op.cc:20
#' 
#' @param src  NDArray
#'     Source input
#' @param axes  Shape(tuple), optional, default=()
#'     Target axis order. By default the axes will be inverted.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.transpose
NULL

#' Sample a uniform distribution
#' 
#' @param low  float, optional, default=0
#'     The lower bound of distribution
#' @param high  float, optional, default=1
#'     The upper bound of distribution
#' @param shape  Shape(tuple), optional, default=()
#'     The shape of the output
#' @param ctx  string, optional, default=''
#'     Context of output, in format [cpu|gpu|cpu_pinned](n).Only used for imperative calls.
#' @return out The result mx.ndarray
#' 
#' @export
#' @name mx.nd.uniform
NULL

#' Create iterator for dataset in csv.
#' 
#' @param data.csv  string, required
#'     Dataset Param: Data csv path.
#' @param data.shape  Shape(tuple), required
#'     Dataset Param: Shape of the data.
#' @param label.csv  string, optional, default='NULL'
#'     Dataset Param: Label csv path. If is NULL, all labels will be returned as 0
#' @param label.shape  Shape(tuple), optional, default=(1,)
#'     Dataset Param: Shape of the label.
#' @return iter The result mx.dataiter
#' 
#' @export
mx.io.CSVIter <- function(...) {
  mx.varg.io.CSVIter(list(...))
}

#' Create iterator for dataset packed in recordio.
#' 
#' @param path.imglist  string, optional, default=''
#'     Dataset Param: Path to image list.
#' @param path.imgrec  string, optional, default='./data/imgrec.rec'
#'     Dataset Param: Path to image record file.
#' @param aug.seq  string, optional, default='aug_default'
#'     Augmentation Param: the augmenter names to represent sequence of augmenters to be applied, seperated by comma. Additional keyword parameters will be seen by these augmenters.
#' @param label.width  int, optional, default='1'
#'     Dataset Param: How many labels for an image.
#' @param data.shape  Shape(tuple), required
#'     Dataset Param: Shape of each instance generated by the DataIter.
#' @param preprocess.threads  int, optional, default='4'
#'     Backend Param: Number of thread to do preprocessing.
#' @param verbose  boolean, optional, default=True
#'     Auxiliary Param: Whether to output parser information.
#' @param num.parts  int, optional, default='1'
#'     partition the data into multiple parts
#' @param part.index  int, optional, default='0'
#'     the index of the part will read
#' @param shuffle.chunk.size  long (non-negative), optional, default=0
#'     the size(MB) of the shuffle chunk, used with shuffle=True, it can enable global shuffling
#' @param shuffle.chunk.seed  int, optional, default='0'
#'     the seed for chunk shuffling
#' @param shuffle  boolean, optional, default=False
#'     Augmentation Param: Whether to shuffle data.
#' @param seed  int, optional, default='0'
#'     Augmentation Param: Random Seed.
#' @param batch.size  int (non-negative), required
#'     Batch Param: Batch size.
#' @param round.batch  boolean, optional, default=True
#'     Batch Param: Use round robin to handle overflow batch.
#' @param prefetch.buffer  long (non-negative), optional, default=4
#'     Backend Param: Number of prefetched parameters
#' @param dtype  {'float16', 'float32', 'float64', 'invalid'},optional, default='invalid'
#'     Data type.
#' @param resize  int, optional, default='-1'
#'     Augmentation Param: scale shorter edge to size before applying other augmentations.
#' @param rand.crop  boolean, optional, default=False
#'     Augmentation Param: Whether to random crop on the image
#' @param crop.y.start  int, optional, default='-1'
#'     Augmentation Param: Where to nonrandom crop on y.
#' @param crop.x.start  int, optional, default='-1'
#'     Augmentation Param: Where to nonrandom crop on x.
#' @param max.rotate.angle  int, optional, default='0'
#'     Augmentation Param: rotated randomly in [-max_rotate_angle, max_rotate_angle].
#' @param max.aspect.ratio  float, optional, default=0
#'     Augmentation Param: denotes the max ratio of random aspect ratio augmentation.
#' @param max.shear.ratio  float, optional, default=0
#'     Augmentation Param: denotes the max random shearing ratio.
#' @param max.crop.size  int, optional, default='-1'
#'     Augmentation Param: Maximum crop size.
#' @param min.crop.size  int, optional, default='-1'
#'     Augmentation Param: Minimum crop size.
#' @param max.random.scale  float, optional, default=1
#'     Augmentation Param: Maxmum scale ratio.
#' @param min.random.scale  float, optional, default=1
#'     Augmentation Param: Minimum scale ratio.
#' @param max.img.size  float, optional, default=1e+10
#'     Augmentation Param: Maxmum image size after resizing.
#' @param min.img.size  float, optional, default=0
#'     Augmentation Param: Minimum image size after resizing.
#' @param random.h  int, optional, default='0'
#'     Augmentation Param: Maximum value of H channel in HSL color space.
#' @param random.s  int, optional, default='0'
#'     Augmentation Param: Maximum value of S channel in HSL color space.
#' @param random.l  int, optional, default='0'
#'     Augmentation Param: Maximum value of L channel in HSL color space.
#' @param rotate  int, optional, default='-1'
#'     Augmentation Param: Rotate angle.
#' @param fill.value  int, optional, default='255'
#'     Augmentation Param: Maximum value of illumination variation.
#' @param data.shape  Shape(tuple), required
#'     Dataset Param: Shape of each instance generated by the DataIter.
#' @param inter.method  int, optional, default='1'
#'     Augmentation Param: 0-NN 1-bilinear 2-cubic 3-area 4-lanczos4 9-auto 10-rand.
#' @param pad  int, optional, default='0'
#'     Augmentation Param: Padding size.
#' @param mirror  boolean, optional, default=False
#'     Augmentation Param: Whether to mirror the image.
#' @param rand.mirror  boolean, optional, default=False
#'     Augmentation Param: Whether to mirror the image randomly.
#' @param mean.img  string, optional, default=''
#'     Augmentation Param: Mean Image to be subtracted.
#' @param mean.r  float, optional, default=0
#'     Augmentation Param: Mean value on R channel.
#' @param mean.g  float, optional, default=0
#'     Augmentation Param: Mean value on G channel.
#' @param mean.b  float, optional, default=0
#'     Augmentation Param: Mean value on B channel.
#' @param mean.a  float, optional, default=0
#'     Augmentation Param: Mean value on Alpha channel.
#' @param scale  float, optional, default=1
#'     Augmentation Param: Scale in color space.
#' @param max.random.contrast  float, optional, default=0
#'     Augmentation Param: Maximum ratio of contrast variation.
#' @param max.random.illumination  float, optional, default=0
#'     Augmentation Param: Maximum value of illumination variation.
#' @return iter The result mx.dataiter
#' 
#' @export
mx.io.ImageRecordIter <- function(...) {
  mx.varg.io.ImageRecordIter(list(...))
}

#' Create iterator for dataset packed in recordio.
#' 
#' @param path.imglist  string, optional, default=''
#'     Dataset Param: Path to image list.
#' @param path.imgrec  string, optional, default='./data/imgrec.rec'
#'     Dataset Param: Path to image record file.
#' @param aug.seq  string, optional, default='aug_default'
#'     Augmentation Param: the augmenter names to represent sequence of augmenters to be applied, seperated by comma. Additional keyword parameters will be seen by these augmenters.
#' @param label.width  int, optional, default='1'
#'     Dataset Param: How many labels for an image.
#' @param data.shape  Shape(tuple), required
#'     Dataset Param: Shape of each instance generated by the DataIter.
#' @param preprocess.threads  int, optional, default='4'
#'     Backend Param: Number of thread to do preprocessing.
#' @param verbose  boolean, optional, default=True
#'     Auxiliary Param: Whether to output parser information.
#' @param num.parts  int, optional, default='1'
#'     partition the data into multiple parts
#' @param part.index  int, optional, default='0'
#'     the index of the part will read
#' @param shuffle.chunk.size  long (non-negative), optional, default=0
#'     the size(MB) of the shuffle chunk, used with shuffle=True, it can enable global shuffling
#' @param shuffle.chunk.seed  int, optional, default='0'
#'     the seed for chunk shuffling
#' @param shuffle  boolean, optional, default=False
#'     Augmentation Param: Whether to shuffle data.
#' @param seed  int, optional, default='0'
#'     Augmentation Param: Random Seed.
#' @param batch.size  int (non-negative), required
#'     Batch Param: Batch size.
#' @param round.batch  boolean, optional, default=True
#'     Batch Param: Use round robin to handle overflow batch.
#' @param prefetch.buffer  long (non-negative), optional, default=4
#'     Backend Param: Number of prefetched parameters
#' @param dtype  {'float16', 'float32', 'float64', 'invalid'},optional, default='invalid'
#'     Data type.
#' @param resize  int, optional, default='-1'
#'     Augmentation Param: scale shorter edge to size before applying other augmentations.
#' @param rand.crop  boolean, optional, default=False
#'     Augmentation Param: Whether to random crop on the image
#' @param crop.y.start  int, optional, default='-1'
#'     Augmentation Param: Where to nonrandom crop on y.
#' @param crop.x.start  int, optional, default='-1'
#'     Augmentation Param: Where to nonrandom crop on x.
#' @param max.rotate.angle  int, optional, default='0'
#'     Augmentation Param: rotated randomly in [-max_rotate_angle, max_rotate_angle].
#' @param max.aspect.ratio  float, optional, default=0
#'     Augmentation Param: denotes the max ratio of random aspect ratio augmentation.
#' @param max.shear.ratio  float, optional, default=0
#'     Augmentation Param: denotes the max random shearing ratio.
#' @param max.crop.size  int, optional, default='-1'
#'     Augmentation Param: Maximum crop size.
#' @param min.crop.size  int, optional, default='-1'
#'     Augmentation Param: Minimum crop size.
#' @param max.random.scale  float, optional, default=1
#'     Augmentation Param: Maxmum scale ratio.
#' @param min.random.scale  float, optional, default=1
#'     Augmentation Param: Minimum scale ratio.
#' @param max.img.size  float, optional, default=1e+10
#'     Augmentation Param: Maxmum image size after resizing.
#' @param min.img.size  float, optional, default=0
#'     Augmentation Param: Minimum image size after resizing.
#' @param random.h  int, optional, default='0'
#'     Augmentation Param: Maximum value of H channel in HSL color space.
#' @param random.s  int, optional, default='0'
#'     Augmentation Param: Maximum value of S channel in HSL color space.
#' @param random.l  int, optional, default='0'
#'     Augmentation Param: Maximum value of L channel in HSL color space.
#' @param rotate  int, optional, default='-1'
#'     Augmentation Param: Rotate angle.
#' @param fill.value  int, optional, default='255'
#'     Augmentation Param: Maximum value of illumination variation.
#' @param data.shape  Shape(tuple), required
#'     Dataset Param: Shape of each instance generated by the DataIter.
#' @param inter.method  int, optional, default='1'
#'     Augmentation Param: 0-NN 1-bilinear 2-cubic 3-area 4-lanczos4 9-auto 10-rand.
#' @param pad  int, optional, default='0'
#'     Augmentation Param: Padding size.
#' @return iter The result mx.dataiter
#' 
#' @export
mx.io.ImageRecordUInt8Iter <- function(...) {
  mx.varg.io.ImageRecordUInt8Iter(list(...))
}

#' Create iterator for MNIST hand-written digit number recognition dataset.
#' 
#' @param image  string, optional, default='./train-images-idx3-ubyte'
#'     Dataset Param: Mnist image path.
#' @param label  string, optional, default='./train-labels-idx1-ubyte'
#'     Dataset Param: Mnist label path.
#' @param batch.size  int, optional, default='128'
#'     Batch Param: Batch Size.
#' @param shuffle  boolean, optional, default=True
#'     Augmentation Param: Whether to shuffle data.
#' @param flat  boolean, optional, default=False
#'     Augmentation Param: Whether to flat the data into 1D.
#' @param seed  int, optional, default='0'
#'     Augmentation Param: Random Seed.
#' @param silent  boolean, optional, default=False
#'     Auxiliary Param: Whether to print out data info.
#' @param num.parts  int, optional, default='1'
#'     partition the data into multiple parts
#' @param part.index  int, optional, default='0'
#'     the index of the part will read
#' @param prefetch.buffer  long (non-negative), optional, default=4
#'     Backend Param: Number of prefetched parameters
#' @param dtype  {'float16', 'float32', 'float64', 'invalid'},optional, default='invalid'
#'     Data type.
#' @return iter The result mx.dataiter
#' 
#' @export
mx.io.MNISTIter <- function(...) {
  mx.varg.io.MNISTIter(list(...))
}

#' Activation:Elementwise activation function.
#' 
#' The following activation types are supported (operations are applied elementwisely to each
#' scalar of the input tensor):
#' 
#' - `relu`: Rectified Linear Unit, `y = max(x, 0)`
#' - `sigmoid`: `y = 1 / (1 + exp(-x))`
#' - `tanh`: Hyperbolic tangent, `y = (exp(x) - exp(-x)) / (exp(x) + exp(-x))`
#' - `softrelu`: Soft ReLU, or SoftPlus, `y = log(1 + exp(x))`
#' 
#' See `LeakyReLU` for other activations with parameters.
#' 
#' 
#' @param act.type  {'relu', 'sigmoid', 'softrelu', 'tanh'}, required
#'     Activation function to be applied.
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.Activation <- function(...) {
  mx.varg.symbol.Activation(list(...))
}

#' BatchNorm:Apply batch normalization to input.
#' 
#' @param data  Symbol
#'     Input data to batch normalization
#' @param eps  float, optional, default=0.001
#'     Epsilon to prevent div 0
#' @param momentum  float, optional, default=0.9
#'     Momentum for moving average
#' @param fix.gamma  boolean, optional, default=True
#'     Fix gamma while training
#' @param use.global.stats  boolean, optional, default=False
#'     Whether use global moving statistics instead of local batch-norm. This will force change batch-norm into a scale shift operator.
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.BatchNorm <- function(...) {
  mx.varg.symbol.BatchNorm(list(...))
}

#' BlockGrad:Get output from a symbol and pass 0 gradient back
#' 
#' @param data  Symbol
#'     Input data.
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.BlockGrad <- function(...) {
  mx.varg.symbol.BlockGrad(list(...))
}

#' Cast:Cast array to a different data type.
#' 
#' @param data  Symbol
#'     Input data to cast function.
#' @param dtype  {'float16', 'float32', 'float64', 'int32', 'uint8'}, required
#'     Target data type.
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.Cast <- function(...) {
  mx.varg.symbol.Cast(list(...))
}

#' Convolution:Apply convolution to input then add a bias.
#' 
#' @param data  Symbol
#'     Input data to the ConvolutionOp.
#' @param weight  Symbol
#'     Weight matrix.
#' @param bias  Symbol
#'     Bias parameter.
#' @param kernel  Shape(tuple), required
#'     convolution kernel size: (y, x) or (d, y, x)
#' @param stride  Shape(tuple), optional, default=(1,1)
#'     convolution stride: (y, x) or (d, y, x)
#' @param dilate  Shape(tuple), optional, default=(1,1)
#'     convolution dilate: (y, x)
#' @param pad  Shape(tuple), optional, default=(0,0)
#'     pad for convolution: (y, x) or (d, y, x)
#' @param num.filter  int (non-negative), required
#'     convolution filter(channel) number
#' @param num.group  int (non-negative), optional, default=1
#'     Number of groups partition. This option is not supported by CuDNN, you can use SliceChannel to num_group,apply convolution and concat instead to achieve the same need.
#' @param workspace  long (non-negative), optional, default=1024
#'     Tmp workspace for convolution (MB).
#' @param no.bias  boolean, optional, default=False
#'     Whether to disable bias parameter.
#' @param cudnn.tune  {'fastest', 'limited_workspace', 'off'},optional, default='off'
#'     Whether to find convolution algo by running performance test.Leads to higher startup time but may give better speed.auto tune is turned off by default.Set environment varialbe MXNET_CUDNN_AUTOTUNE_DEFAULT=1 to turn on by default.
#' @param cudnn.off  boolean, optional, default=False
#'     Turn off cudnn.
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.Convolution <- function(...) {
  mx.varg.symbol.Convolution(list(...))
}

#' Correlation:Apply correlation to inputs
#' 
#' @param data1  Symbol
#'     Input data1 to the correlation.
#' @param data2  Symbol
#'     Input data2 to the correlation.
#' @param kernel.size  int (non-negative), optional, default=1
#'     kernel size for Correlation must be an odd number
#' @param max.displacement  int (non-negative), optional, default=1
#'     Max displacement of Correlation 
#' @param stride1  int (non-negative), optional, default=1
#'     stride1 quantize data1 globally
#' @param stride2  int (non-negative), optional, default=1
#'     stride2 quantize data2 within the neighborhood centered around data1
#' @param pad.size  int (non-negative), optional, default=0
#'     pad for Correlation
#' @param is.multiply  boolean, optional, default=True
#'     operation type is either multiplication or subduction
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.Correlation <- function(...) {
  mx.varg.symbol.Correlation(list(...))
}

#' Crop:Crop the 2nd and 3rd dim of input data, with the corresponding size of h_w or with width and height of the second input symbol, i.e., with one input, we need h_w to specify the crop height and width, otherwise the second input symbol's size will be used
#' 
#' @param data  Symbol or Symbol[]
#'     Tensor or List of Tensors, the second input will be used as crop_like shape reference
#' @param num.args  int, required
#'     Number of inputs for crop, if equals one, then we will use the h_wfor crop height and width, else if equals two, then we will use the heightand width of the second input symbol, we name crop_like here
#' @param offset  Shape(tuple), optional, default=(0,0)
#'     crop offset coordinate: (y, x)
#' @param h.w  Shape(tuple), optional, default=(0,0)
#'     crop height and weight: (h, w)
#' @param center.crop  boolean, optional, default=False
#'     If set to true, then it will use be the center_crop,or it will crop using the shape of crop_like
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.Crop <- function(...) {
  mx.varg.symbol.Crop(list(...))
}

#' Custom:Custom operator implemented in frontend.
#' 
#' @param op.type  string
#'     Type of custom operator. Must be registered first.
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.Custom <- function(...) {
  mx.varg.symbol.Custom(list(...))
}

#' Deconvolution:Apply deconvolution to input then add a bias.
#' 
#' @param data  Symbol
#'     Input data to the DeconvolutionOp.
#' @param weight  Symbol
#'     Weight matrix.
#' @param bias  Symbol
#'     Bias parameter.
#' @param kernel  Shape(tuple), required
#'     deconvolution kernel size: (y, x)
#' @param stride  Shape(tuple), optional, default=(1,1)
#'     deconvolution stride: (y, x)
#' @param pad  Shape(tuple), optional, default=(0,0)
#'     pad for deconvolution: (y, x), a good number is : (kernel-1)/2, if target_shape set, pad will be ignored and will be computed automatically
#' @param adj  Shape(tuple), optional, default=(0,0)
#'     adjustment for output shape: (y, x), if target_shape set, adj will be ignored and will be computed automatically
#' @param target.shape  Shape(tuple), optional, default=(0,0)
#'     output shape with targe shape : (y, x)
#' @param num.filter  int (non-negative), required
#'     deconvolution filter(channel) number
#' @param num.group  int (non-negative), optional, default=1
#'     number of groups partition
#' @param workspace  long (non-negative), optional, default=512
#'     Tmp workspace for deconvolution (MB)
#' @param no.bias  boolean, optional, default=True
#'     Whether to disable bias parameter.
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.Deconvolution <- function(...) {
  mx.varg.symbol.Deconvolution(list(...))
}

#' Dropout:Apply dropout to input.
#' During training, each element of the input is randomly set to zero with probability p.
#' And then the whole tensor is rescaled by 1/(1-p) to keep the expectation the same as
#' before applying dropout. During the test time, this behaves as an identity map.
#' 
#' 
#' @param data  Symbol
#'     Input data to dropout.
#' @param p  float, optional, default=0.5
#'     Fraction of the input that gets dropped out at training time
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.Dropout <- function(...) {
  mx.varg.symbol.Dropout(list(...))
}

#' ElementWiseSum:Perform element sum of inputs
#' 
#' From:src/operator/tensor/elemwise_sum.cc:49
#' 
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.ElementWiseSum <- function(...) {
  mx.varg.symbol.ElementWiseSum(list(...))
}

#' Embedding:Map integer index to vector representations (embeddings). Those
#' embeddings are learnable parameters. For a input of shape `(d1, ..., dK)`, the
#' output shape is `(d1, ..., dK, output_dim)`. All the input values should be
#' integers in the range `[0, input_dim)`.
#' 
#' @param data  Symbol
#'     Input data to the EmbeddingOp.
#' @param weight  Symbol
#'     Enbedding weight matrix.
#' @param input.dim  int, required
#'     vocabulary size of the input indices.
#' @param output.dim  int, required
#'     dimension of the embedding vectors.
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.Embedding <- function(...) {
  mx.varg.symbol.Embedding(list(...))
}

#' Flatten:Flatten input into 2D by collapsing all the higher dimensions.
#' A (d1, d2, ..., dK) tensor is flatten to (d1, d2* ... *dK) matrix.
#' 
#' @param data  Symbol
#'     Input data to flatten.
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.Flatten <- function(...) {
  mx.varg.symbol.Flatten(list(...))
}

#' FullyConnected:Apply matrix multiplication to input then add a bias.
#' It maps the input of shape `(batch_size, input_dim)` to the shape of
#' `(batch_size, num_hidden)`. Learnable parameters include the weights
#' of the linear transform and an optional bias vector.
#' 
#' @param data  Symbol
#'     Input data to the FullyConnectedOp.
#' @param weight  Symbol
#'     Weight matrix.
#' @param bias  Symbol
#'     Bias parameter.
#' @param num.hidden  int, required
#'     Number of hidden nodes of the output.
#' @param no.bias  boolean, optional, default=False
#'     Whether to disable bias parameter.
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.FullyConnected <- function(...) {
  mx.varg.symbol.FullyConnected(list(...))
}

#' IdentityAttachKLSparseReg:Apply a sparse regularization to the output a sigmoid activation function.
#' 
#' @param data  Symbol
#'     Input data.
#' @param sparseness.target  float, optional, default=0.1
#'     The sparseness target
#' @param penalty  float, optional, default=0.001
#'     The tradeoff parameter for the sparseness penalty
#' @param momentum  float, optional, default=0.9
#'     The momentum for running average
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.IdentityAttachKLSparseReg <- function(...) {
  mx.varg.symbol.IdentityAttachKLSparseReg(list(...))
}

#' InstanceNorm:An operator taking in a n-dimensional input tensor (n > 2), and normalizing the input by subtracting the mean and variance calculated over the spatial dimensions. This is an implemention of the operator described in "Instance Normalization: The Missing Ingredient for Fast Stylization", D. Ulyanov, A. Vedaldi, V. Lempitsky, 2016 (arXiv:1607.08022v2). This layer is similar to batch normalization, with two differences: first, the normalization is carried out per example ('instance'), not over a batch. Second, the same normalization is applied both at test and train time. This operation is also known as 'contrast normalization'.
#' 
#' @param data  Symbol
#'     A n-dimensional tensor (n > 2) of the form [batch, channel, spatial_dim1, spatial_dim2, ...].
#' @param gamma  Symbol
#'     A vector of length 'channel', which multiplies the normalized input.
#' @param beta  Symbol
#'     A vector of length 'channel', which is added to the product of the normalized input and the weight.
#' @param eps  float, optional, default=0.001
#'     Epsilon to prevent division by 0.
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.InstanceNorm <- function(...) {
  mx.varg.symbol.InstanceNorm(list(...))
}

#' L2Normalization:Set the l2 norm of each instance to a constant.
#' 
#' @param data  Symbol
#'     Input data to the L2NormalizationOp.
#' @param eps  float, optional, default=1e-10
#'     Epsilon to prevent div 0
#' @param mode  {'channel', 'instance', 'spatial'},optional, default='instance'
#'     Normalization Mode. If set to instance, this operator will compute a norm for each instance in the batch; this is the default mode. If set to channel, this operator will compute a cross channel norm at each position of each instance. If set to spatial, this operator will compute a norm for each channel.
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.L2Normalization <- function(...) {
  mx.varg.symbol.L2Normalization(list(...))
}

#' LRN:Apply convolution to input then add a bias.
#' 
#' @param data  Symbol
#'     Input data to the ConvolutionOp.
#' @param alpha  float, optional, default=0.0001
#'     value of the alpha variance scaling parameter in the normalization formula
#' @param beta  float, optional, default=0.75
#'     value of the beta power parameter in the normalization formula
#' @param knorm  float, optional, default=2
#'     value of the k parameter in normalization formula
#' @param nsize  int (non-negative), required
#'     normalization window width in elements.
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.LRN <- function(...) {
  mx.varg.symbol.LRN(list(...))
}

#' LeakyReLU:Apply activation function to input.
#' 
#' @param data  Symbol
#'     Input data to activation function.
#' @param act.type  {'elu', 'leaky', 'prelu', 'rrelu'},optional, default='leaky'
#'     Activation function to be applied.
#' @param slope  float, optional, default=0.25
#'     Init slope for the activation. (For leaky and elu only)
#' @param lower.bound  float, optional, default=0.125
#'     Lower bound of random slope. (For rrelu only)
#' @param upper.bound  float, optional, default=0.334
#'     Upper bound of random slope. (For rrelu only)
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.LeakyReLU <- function(...) {
  mx.varg.symbol.LeakyReLU(list(...))
}

#' LinearRegressionOutput:Use linear regression for final output, this is used on final output of a net.
#' 
#' @param data  Symbol
#'     Input data to function.
#' @param label  Symbol
#'     Input label to function.
#' @param grad.scale  float, optional, default=1
#'     Scale the gradient by a float factor
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.LinearRegressionOutput <- function(...) {
  mx.varg.symbol.LinearRegressionOutput(list(...))
}

#' LogisticRegressionOutput:Use Logistic regression for final output, this is used on final output of a net.
#' Logistic regression is suitable for binary classification or probability prediction tasks.
#' 
#' @param data  Symbol
#'     Input data to function.
#' @param label  Symbol
#'     Input label to function.
#' @param grad.scale  float, optional, default=1
#'     Scale the gradient by a float factor
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.LogisticRegressionOutput <- function(...) {
  mx.varg.symbol.LogisticRegressionOutput(list(...))
}

#' MAERegressionOutput:Use mean absolute error regression for final output, this is used on final output of a net.
#' 
#' @param data  Symbol
#'     Input data to function.
#' @param label  Symbol
#'     Input label to function.
#' @param grad.scale  float, optional, default=1
#'     Scale the gradient by a float factor
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.MAERegressionOutput <- function(...) {
  mx.varg.symbol.MAERegressionOutput(list(...))
}

#' MakeLoss:Get output from a symbol and pass 1 gradient back. This is used as a terminal loss if unary and binary operator are used to composite a loss with no declaration of backward dependency
#' 
#' @param data  Symbol
#'     Input data.
#' @param grad.scale  float, optional, default=1
#'     gradient scale as a supplement to unary and binary operators
#' @param valid.thresh  float, optional, default=0
#'     regard element valid when x > valid_thresh, this is used only in valid normalization mode.
#' @param normalization  {'batch', 'null', 'valid'},optional, default='null'
#'     If set to null, op will not normalize on output gradient.If set to batch, op will normalize gradient by divide batch size.If set to valid, op will normalize gradient by divide # sample marked as valid
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.MakeLoss <- function(...) {
  mx.varg.symbol.MakeLoss(list(...))
}

#' Pad:Pads an n-dimensional input tensor. Allows for precise control of the padding type and how much padding to apply on both sides of a given dimension.
#' 
#' @param data  Symbol
#'     An n-dimensional input tensor.
#' @param mode  {'constant', 'edge'}, required
#'     Padding type to use. "constant" pads all values with a constant value, the value of which can be specified with the constant_value option. "edge" uses the boundary values of the array as padding.
#' @param pad.width  Shape(tuple), required
#'     A tuple of padding widths of length 2*r, where r is the rank of the input tensor, specifying number of values padded to the edges of each axis. (before_1, after_1, ... , before_N, after_N) unique pad widths for each axis. Equivalent to pad_width in numpy.pad, but flattened.
#' @param constant.value  double, optional, default=0
#'     This option is only used when mode is "constant". This value will be used as the padding value. Defaults to 0 if not specified.
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.Pad <- function(...) {
  mx.varg.symbol.Pad(list(...))
}

#' Pooling:Perform spatial pooling on inputs.
#' 
#' @param data  Symbol
#'     Input data to the pooling operator.
#' @param global.pool  boolean, optional, default=False
#'     Ignore kernel size, do global pooling based on current input feature map. This is useful for input with different shape
#' @param kernel  Shape(tuple), required
#'     pooling kernel size: (y, x) or (d, y, x)
#' @param pool.type  {'avg', 'max', 'sum'}, required
#'     Pooling type to be applied.
#' @param pooling.convention  {'full', 'valid'},optional, default='valid'
#'     Pooling convention to be applied.kValid is default setting of Mxnet and rounds down the output pooling size.kFull is compatible with Caffe and rounds up the output pooling size.
#' @param stride  Shape(tuple), optional, default=(1,1)
#'     stride: for pooling (y, x) or (d, y, x)
#' @param pad  Shape(tuple), optional, default=(0,0)
#'     pad for pooling: (y, x) or (d, y, x)
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.Pooling <- function(...) {
  mx.varg.symbol.Pooling(list(...))
}

#' RNN:Apply a recurrent layer to input.
#' 
#' @param data  Symbol
#'     Input data to RNN
#' @param parameters  Symbol
#'     Vector of all RNN trainable parameters concatenated
#' @param state  Symbol
#'     initial hidden state of the RNN
#' @param state.cell  Symbol
#'     initial cell state for LSTM networks (only for LSTM)
#' @param state.size  int (non-negative), required
#'     size of the state for each layer
#' @param num.layers  int (non-negative), required
#'     number of stacked layers
#' @param bidirectional  boolean, optional, default=False
#'     whether to use bidirectional recurrent layers
#' @param mode  {'gru', 'lstm', 'rnn_relu', 'rnn_tanh'}, required
#'     the type of RNN to compute
#' @param p  float, optional, default=0
#'     Dropout probability, fraction of the input that gets dropped out at training time
#' @param state.outputs  boolean, optional, default=False
#'     Whether to have the states as symbol outputs.
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.RNN <- function(...) {
  mx.varg.symbol.RNN(list(...))
}

#' ROIPooling:Performs region-of-interest pooling on inputs. Resize bounding box coordinates by spatial_scale and crop input feature maps accordingly. The cropped feature maps are pooled by max pooling to a fixed size output indicated by pooled_size. batch_size will change to the number of region bounding boxes after ROIPooling
#' 
#' @param data  Symbol
#'     Input data to the pooling operator, a 4D Feature maps
#' @param rois  Symbol
#'     Bounding box coordinates, a 2D array of [[batch_index, x1, y1, x2, y2]]. (x1, y1) and (x2, y2) are top left and down right corners of designated region of interest. batch_index indicates the index of corresponding image in the input data
#' @param pooled.size  Shape(tuple), required
#'     fix pooled size: (h, w)
#' @param spatial.scale  float, required
#'     Ratio of input feature map height (or w) to raw image height (or w). Equals the reciprocal of total stride in convolutional layers
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.ROIPooling <- function(...) {
  mx.varg.symbol.ROIPooling(list(...))
}

#' Reshape:Reshape input according to a target shape spec.
#' The target shape is a tuple and can be a simple list of dimensions such as (12,3) or it can incorporate special codes that correspond to contextual operations that refer to the input dimensions.
#' The special codes are all expressed as integers less than 1. These codes effectively refer to a machine that pops input dims off the beginning of the input dims list and pushes resulting output dims onto the end of the output dims list, which starts empty. The codes are:
#'   0  Copy     Pop one input dim and push it onto the output dims
#'  -1  Infer    Push a dim that is inferred later from all other output dims
#'  -2  CopyAll  Pop all remaining input dims and push them onto output dims
#'  -3  Merge2   Pop two input dims, multiply them, and push result
#'  -4  Split2   Pop one input dim, and read two next target shape specs,
#'               push them both onto output dims (either can be -1 and will
#'               be inferred from the other
#'  The exact mathematical behavior of these codes is given in the description of the 'shape' parameter. All non-codes (positive integers) just pop a dim off the input dims (if any), throw it away, and then push the specified integer onto the output dims.
#' Examples:
#' Type     Input      Target            Output
#' Copy     (2,3,4)    (4,0,2)           (4,3,2)
#' Copy     (2,3,4)    (2,0,0)           (2,3,4)
#' Infer    (2,3,4)    (6,1,-1)          (6,1,4)
#' Infer    (2,3,4)    (3,-1,8)          (3,1,8)
#' CopyAll  (9,8,7)    (-2)              (9,8,7)
#' CopyAll  (9,8,7)    (9,-2)            (9,8,7)
#' CopyAll  (9,8,7)    (-2,1,1)          (9,8,7,1,1)
#' Merge2   (3,4)      (-3)              (12)
#' Merge2   (3,4,5)    (-3,0)            (12,5)
#' Merge2   (3,4,5)    (0,-3)            (3,20)
#' Merge2   (3,4,5,6)  (-3,0,0)          (12,5,6)
#' Merge2   (3,4,5,6)  (-3,-2)           (12,5,6)
#' Split2   (12)       (-4,6,2)          (6,2)
#' Split2   (12)       (-4,2,6)          (2,6)
#' Split2   (12)       (-4,-1,6)         (2,6)
#' Split2   (12,9)     (-4,2,6,0)        (2,6,9)
#' Split2   (12,9,9,9) (-4,2,6,-2)       (2,6,9,9,9)
#' Split2   (12,12)    (-4,2,-1,-4,-1,2) (2,6,6,2)
#' 
#' 
#' @param data  Symbol
#'     Input data to reshape.
#' @param target.shape  Shape(tuple), optional, default=(0,0)
#'     (Deprecated! Use shape instead.) Target new shape. One and only one dim can be 0, in which case it will be inferred from the rest of dims
#' @param keep.highest  boolean, optional, default=False
#'     (Deprecated! Use shape instead.) Whether keep the highest dim unchanged.If set to true, then the first dim in target_shape is ignored,and always fixed as input
#' @param shape  , optional, default=()
#'     Target shape, a tuple, t=(t_1,t_2,..,t_m).
#' Let the input dims be s=(s_1,s_2,..,s_n).
#' The output dims u=(u_1,u_2,..,u_p) are computed from s and t.
#' The target shape tuple elements t_i are read in order, and used to  generate successive output dims u_p:
#' t_i:       meaning:      behavior:
#' +ve        explicit      u_p = t_i
#' 0          copy          u_p = s_i
#' -1         infer         u_p = (Prod s_i) / (Prod u_j | j != p)
#' -2         copy all      u_p = s_i, u_p+1 = s_i+1, ...
#' -3         merge two     u_p = s_i * s_i+1
#' -4,a,b     split two     u_p = a, u_p+1 = b | a * b = s_i
#' The split directive (-4) in the target shape tuple is followed by two dimensions, one of which can be -1, which means it will be inferred from the other one and the original dimension.
#' The can only be one globally inferred dimension (-1), aside from any -1 occuring in a split directive.
#' @param reverse  boolean, optional, default=False
#'     Whether to match the shapes from the backward. If reverse is true, 0 values in the `shape` argument will be searched from the backward. E.g the original shape is (10, 5, 4) and the shape argument is (-1, 0). If reverse is true, the new shape should be (50, 4). Otherwise it will be (40, 5).
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.Reshape <- function(...) {
  mx.varg.symbol.Reshape(list(...))
}

#' SVMOutput:Support Vector Machine based transformation on input, backprop L2-SVM
#' 
#' @param data  Symbol
#'     Input data to svm.
#' @param label  Symbol
#'     Label data.
#' @param margin  float, optional, default=1
#'     Scale the DType(param_.margin) for activation size
#' @param regularization.coefficient  float, optional, default=1
#'     Scale the coefficient responsible for balacing coefficient size and error tradeoff
#' @param use.linear  boolean, optional, default=False
#'     If set true, uses L1-SVM objective function. Default uses L2-SVM objective
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.SVMOutput <- function(...) {
  mx.varg.symbol.SVMOutput(list(...))
}

#' SequenceLast:Takes the last element of a sequence. Takes an n-dimensional tensor of the form [max sequence length, batchsize, other dims] and returns a (n-1)-dimensional tensor of the form [batchsize, other dims]. This operator takes an optional input tensor sequence_length of positive ints of dimension [batchsize] when the sequence_length option is set to true. This allows the operator to handle variable-length sequences. If sequence_length is false, then each example in the batch is assumed to have the max sequence length.
#' 
#' @param data  Symbol
#'     n-dimensional input tensor of the form [max sequence length, batchsize, other dims]
#' @param sequence.length  Symbol
#'     vector of sequence lengths of size batchsize
#' @param use.sequence.length  boolean, optional, default=False
#'     If set to true, this layer takes in extra input sequence_length to specify variable length sequence
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.SequenceLast <- function(...) {
  mx.varg.symbol.SequenceLast(list(...))
}

#' SequenceMask:Sets all elements outside the sequence to zero. Takes an n-dimensional tensor of the form [max sequence length, batchsize, other dims] and returns a tensor of the same shape. This operator takes an optional input tensor sequence_length of positive ints of dimension [batchsize] when the sequence_length option is set to true. This allows the operator to handle variable-length sequences. If sequence_length is false, then each example in the batch is assumed to have the max sequence length, and this operator becomes the identity operator.
#' 
#' @param data  Symbol
#'     n-dimensional input tensor of the form [max sequence length, batchsize, other dims]
#' @param sequence.length  Symbol
#'     vector of sequence lengths of size batchsize
#' @param use.sequence.length  boolean, optional, default=False
#'     If set to true, this layer takes in extra input sequence_length to specify variable length sequence
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.SequenceMask <- function(...) {
  mx.varg.symbol.SequenceMask(list(...))
}

#' SequenceReverse:Reverses the elements of each sequence. Takes an n-dimensional tensor of the form [max sequence length, batchsize, other dims] and returns a tensor of the same shape. This operator takes an optional input tensor sequence_length of positive ints of dimension [batchsize] when the sequence_length option is set to true. This allows the operator to handle variable-length sequences. If sequence_length is false, then each example in the batch is assumed to have the max sequence length.
#' 
#' @param data  Symbol
#'     n-dimensional input tensor of the form [max sequence length, batchsize, other dims]
#' @param sequence.length  Symbol
#'     vector of sequence lengths of size batchsize
#' @param use.sequence.length  boolean, optional, default=False
#'     If set to true, this layer takes in extra input sequence_length to specify variable length sequence
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.SequenceReverse <- function(...) {
  mx.varg.symbol.SequenceReverse(list(...))
}

#' SliceChannel:Slice input equally along specified axis
#' 
#' @param num.outputs  int, required
#'     Number of outputs to be sliced.
#' @param axis  int, optional, default='1'
#'     Dimension along which to slice.
#' @param squeeze.axis  boolean, optional, default=False
#'     If true AND the sliced dimension becomes 1, squeeze that dimension.
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.SliceChannel <- function(...) {
  mx.varg.symbol.SliceChannel(list(...))
}

#' Softmax:DEPRECATED: Perform a softmax transformation on input. Please use SoftmaxOutput
#' 
#' @param data  Symbol
#'     Input data to softmax.
#' @param grad.scale  float, optional, default=1
#'     Scale the gradient by a float factor
#' @param ignore.label  float, optional, default=-1
#'     the label value will be ignored during backward (only works if use_ignore is set to be true).
#' @param multi.output  boolean, optional, default=False
#'     If set to true, for a (n,k,x_1,..,x_n) dimensional input tensor, softmax will generate n*x_1*...*x_n output, each has k classes
#' @param use.ignore  boolean, optional, default=False
#'     If set to true, the ignore_label value will not contribute to the backward gradient
#' @param preserve.shape  boolean, optional, default=False
#'     If true, for a (n_1, n_2, ..., n_d, k) dimensional input tensor, softmax will generate (n1, n2, ..., n_d, k) output, normalizing the k classes as the last dimension.
#' @param normalization  {'batch', 'null', 'valid'},optional, default='null'
#'     If set to null, op will do nothing on output gradient.If set to batch, op will normalize gradient by divide batch sizeIf set to valid, op will normalize gradient by divide sample not ignored
#' @param out.grad  boolean, optional, default=False
#'     Apply weighting from output gradient
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.Softmax <- function(...) {
  mx.varg.symbol.Softmax(list(...))
}

#' SoftmaxActivation:Apply softmax activation to input. This is intended for internal layers. For output (loss layer) please use SoftmaxOutput. If mode=instance, this operator will compute a softmax for each instance in the batch; this is the default mode. If mode=channel, this operator will compute a num_channel-class softmax at each position of each instance; this can be used for fully convolutional network, image segmentation, etc.
#' 
#' @param data  Symbol
#'     Input data to activation function.
#' @param mode  {'channel', 'instance'},optional, default='instance'
#'     Softmax Mode. If set to instance, this operator will compute a softmax for each instance in the batch; this is the default mode. If set to channel, this operator will compute a num_channel-class softmax at each position of each instance; this can be used for fully convolutional network, image segmentation, etc.
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.SoftmaxActivation <- function(...) {
  mx.varg.symbol.SoftmaxActivation(list(...))
}

#' SoftmaxOutput:Perform a softmax transformation on input, backprop with logloss.
#' 
#' @param data  Symbol
#'     Input data to softmax.
#' @param label  Symbol
#'     Label data, can also be probability value with same shape as data
#' @param grad.scale  float, optional, default=1
#'     Scale the gradient by a float factor
#' @param ignore.label  float, optional, default=-1
#'     the label value will be ignored during backward (only works if use_ignore is set to be true).
#' @param multi.output  boolean, optional, default=False
#'     If set to true, for a (n,k,x_1,..,x_n) dimensional input tensor, softmax will generate n*x_1*...*x_n output, each has k classes
#' @param use.ignore  boolean, optional, default=False
#'     If set to true, the ignore_label value will not contribute to the backward gradient
#' @param preserve.shape  boolean, optional, default=False
#'     If true, for a (n_1, n_2, ..., n_d, k) dimensional input tensor, softmax will generate (n1, n2, ..., n_d, k) output, normalizing the k classes as the last dimension.
#' @param normalization  {'batch', 'null', 'valid'},optional, default='null'
#'     If set to null, op will do nothing on output gradient.If set to batch, op will normalize gradient by divide batch sizeIf set to valid, op will normalize gradient by divide sample not ignored
#' @param out.grad  boolean, optional, default=False
#'     Apply weighting from output gradient
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.SoftmaxOutput <- function(...) {
  mx.varg.symbol.SoftmaxOutput(list(...))
}

#' SpatialTransformer:Apply spatial transformer to input feature map.
#' 
#' @param data  Symbol
#'     Input data to the SpatialTransformerOp.
#' @param loc  Symbol
#'     localisation net, the output dim should be 6 when transform_type is affine, and the name of loc symbol should better starts with 'stn_loc', so that initialization it with iddentify tranform, or you shold initialize the weight and bias by yourself.
#' @param target.shape  Shape(tuple), optional, default=(0,0)
#'     output shape(h, w) of spatial transformer: (y, x)
#' @param transform.type  {'affine'}, required
#'     transformation type
#' @param sampler.type  {'bilinear'}, required
#'     sampling type
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.SpatialTransformer <- function(...) {
  mx.varg.symbol.SpatialTransformer(list(...))
}

#' SwapAxis:Apply swapaxis to input.
#' 
#' @param data  Symbol
#'     Input data to the SwapAxisOp.
#' @param dim1  int (non-negative), optional, default=0
#'     the first axis to be swapped.
#' @param dim2  int (non-negative), optional, default=0
#'     the second axis to be swapped.
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.SwapAxis <- function(...) {
  mx.varg.symbol.SwapAxis(list(...))
}

#' UpSampling:Perform nearest neighboor/bilinear up sampling to inputs
#' 
#' @param data  Symbol[]
#'     Array of tensors to upsample
#' @param scale  int (non-negative), required
#'     Up sampling scale
#' @param num.filter  int (non-negative), optional, default=0
#'     Input filter. Only used by bilinear sample_type.
#' @param sample.type  {'bilinear', 'nearest'}, required
#'     upsampling method
#' @param multi.input.mode  {'concat', 'sum'},optional, default='concat'
#'     How to handle multiple input. concat means concatenate upsampled images along the channel dimension. sum means add all images together, only available for nearest neighbor upsampling.
#' @param num.args  int, required
#'     Number of inputs to be upsampled. For nearest neighbor upsampling, this can be 1-N; the size of output will be(scale*h_0,scale*w_0) and all other inputs will be upsampled to thesame size. For bilinear upsampling this must be 2; 1 input and 1 weight.
#' @param workspace  long (non-negative), optional, default=512
#'     Tmp workspace for deconvolution (MB)
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.UpSampling <- function(...) {
  mx.varg.symbol.UpSampling(list(...))
}

#' abs:Take absolute value of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:60
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.abs <- function(...) {
  mx.varg.symbol.abs(list(...))
}

#' adam_update:Updater function for adam optimizer
#' 
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.adam_update <- function(...) {
  mx.varg.symbol.adam_update(list(...))
}

#' arccos:Take arccos of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:212
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.arccos <- function(...) {
  mx.varg.symbol.arccos(list(...))
}

#' arccosh:Take arccosh of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:284
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.arccosh <- function(...) {
  mx.varg.symbol.arccosh(list(...))
}

#' arcsin:Take arcsin of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:203
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.arcsin <- function(...) {
  mx.varg.symbol.arcsin(list(...))
}

#' arcsinh:Take arcsinh of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:275
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.arcsinh <- function(...) {
  mx.varg.symbol.arcsinh(list(...))
}

#' arctan:Take arctan of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:221
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.arctan <- function(...) {
  mx.varg.symbol.arctan(list(...))
}

#' arctanh:Take arctanh of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:293
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.arctanh <- function(...) {
  mx.varg.symbol.arctanh(list(...))
}

#' argmax:Compute argmax
#' 
#' From:src/operator/tensor/broadcast_reduce_op.cc:70
#' 
#' @param src  NDArray
#'     Source input
#' @param axis  int, optional, default='-1'
#'     Empty or unsigned. The axis to perform the reduction.If left empty, a global reduction will be performed.
#' @param keepdims  boolean, optional, default=False
#'     If true, the axis which is reduced is left in the result as dimension with size one.
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.argmax <- function(...) {
  mx.varg.symbol.argmax(list(...))
}

#' argmax_channel:
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.argmax_channel <- function(...) {
  mx.varg.symbol.argmax_channel(list(...))
}

#' argmin:Compute argmin
#' 
#' From:src/operator/tensor/broadcast_reduce_op.cc:74
#' 
#' @param src  NDArray
#'     Source input
#' @param axis  int, optional, default='-1'
#'     Empty or unsigned. The axis to perform the reduction.If left empty, a global reduction will be performed.
#' @param keepdims  boolean, optional, default=False
#'     If true, the axis which is reduced is left in the result as dimension with size one.
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.argmin <- function(...) {
  mx.varg.symbol.argmin(list(...))
}

#' batch_dot:Calculate batched dot product of two matrices. (batch, M, K) batch_dot (batch, K, N) --> (batch, M, N)
#' 
#' From:src/operator/tensor/matrix_op.cc:176
#' 
#' @param lhs  NDArray
#'     Left input
#' @param rhs  NDArray
#'     Right input
#' @param axis  int, required
#'     The dimension to flip
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.batch_dot <- function(...) {
  mx.varg.symbol.batch_dot(list(...))
}

#' broadcast_add:
#' 
#' @param lhs  NDArray
#'     first input
#' @param rhs  NDArray
#'     second input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.broadcast_add <- function(...) {
  mx.varg.symbol.broadcast_add(list(...))
}

#' broadcast_axis:Broadcast src along axis
#' 
#' From:src/operator/tensor/broadcast_reduce_op.cc:43
#' 
#' @param src  NDArray
#'     Source input
#' @param axis  Shape(tuple), optional, default=()
#'     The axes to perform the broadcasting.
#' @param size  Shape(tuple), optional, default=()
#'     Target sizes of the broadcasting axes.
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.broadcast_axis <- function(...) {
  mx.varg.symbol.broadcast_axis(list(...))
}

#' broadcast_div:
#' 
#' @param lhs  NDArray
#'     first input
#' @param rhs  NDArray
#'     second input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.broadcast_div <- function(...) {
  mx.varg.symbol.broadcast_div(list(...))
}

#' broadcast_hypot:
#' 
#' @param lhs  NDArray
#'     first input
#' @param rhs  NDArray
#'     second input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.broadcast_hypot <- function(...) {
  mx.varg.symbol.broadcast_hypot(list(...))
}

#' broadcast_maximum:
#' 
#' @param lhs  NDArray
#'     first input
#' @param rhs  NDArray
#'     second input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.broadcast_maximum <- function(...) {
  mx.varg.symbol.broadcast_maximum(list(...))
}

#' broadcast_minimum:
#' 
#' @param lhs  NDArray
#'     first input
#' @param rhs  NDArray
#'     second input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.broadcast_minimum <- function(...) {
  mx.varg.symbol.broadcast_minimum(list(...))
}

#' broadcast_minus:
#' 
#' @param lhs  NDArray
#'     first input
#' @param rhs  NDArray
#'     second input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.broadcast_minus <- function(...) {
  mx.varg.symbol.broadcast_minus(list(...))
}

#' broadcast_mul:
#' 
#' @param lhs  NDArray
#'     first input
#' @param rhs  NDArray
#'     second input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.broadcast_mul <- function(...) {
  mx.varg.symbol.broadcast_mul(list(...))
}

#' broadcast_plus:
#' 
#' @param lhs  NDArray
#'     first input
#' @param rhs  NDArray
#'     second input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.broadcast_plus <- function(...) {
  mx.varg.symbol.broadcast_plus(list(...))
}

#' broadcast_power:
#' 
#' @param lhs  NDArray
#'     first input
#' @param rhs  NDArray
#'     second input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.broadcast_power <- function(...) {
  mx.varg.symbol.broadcast_power(list(...))
}

#' broadcast_sub:
#' 
#' @param lhs  NDArray
#'     first input
#' @param rhs  NDArray
#'     second input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.broadcast_sub <- function(...) {
  mx.varg.symbol.broadcast_sub(list(...))
}

#' broadcast_to:Broadcast src to shape
#' 
#' From:src/operator/tensor/broadcast_reduce_op.cc:50
#' 
#' @param src  NDArray
#'     Source input
#' @param shape  Shape(tuple), optional, default=()
#'     The shape of the desired array. We can set the dim to zero if it's same as the original. E.g `A = broadcast_to(B, shape=(10, 0, 0))` has the same meaning as `A = broadcast_axis(B, axis=0, size=10)`.
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.broadcast_to <- function(...) {
  mx.varg.symbol.broadcast_to(list(...))
}

#' ceil:Take ceil of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:83
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.ceil <- function(...) {
  mx.varg.symbol.ceil(list(...))
}

#' choose_element_0index:Choose one element from each line(row for python, column for R/Julia) in lhs according to index indicated by rhs. This function assume rhs uses 0-based index.
#' 
#' @param lhs  NDArray
#'     Left operand to the function.
#' @param rhs  NDArray
#'     Right operand to the function.
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.choose_element_0index <- function(...) {
  mx.varg.symbol.choose_element_0index(list(...))
}

#' clip:Clip ndarray elements to range (a_min, a_max)
#' 
#' @param src  NDArray
#'     Source input
#' @param a.min  real_t
#'     Minimum value
#' @param a.max  real_t
#'     Maximum value
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.clip <- function(...) {
  mx.varg.symbol.clip(list(...))
}

#' cos:Take cos of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:185
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.cos <- function(...) {
  mx.varg.symbol.cos(list(...))
}

#' cosh:Take cosh of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:257
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.cosh <- function(...) {
  mx.varg.symbol.cosh(list(...))
}

#' crop:(Crop the input tensor and return a new one.
#' 
#' Requirements
#' ------------
#' - the input and output (if explicitly given) are of the same data type,
#'   and on the same device.
#' )
#' 
#' From:src/operator/tensor/matrix_op.cc:69
#' 
#' @param src  NDArray
#'     Source input
#' @param begin  Shape(tuple), required
#'     starting coordinates
#' @param end  Shape(tuple), required
#'     ending coordinates
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.crop <- function(...) {
  mx.varg.symbol.crop(list(...))
}

#' degrees:Take degrees of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:230
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.degrees <- function(...) {
  mx.varg.symbol.degrees(list(...))
}

#' dot:Calculate dot product of two matrices or two vectors.
#' 
#' From:src/operator/tensor/matrix_op.cc:154
#' 
#' @param lhs  NDArray
#'     Left input
#' @param rhs  NDArray
#'     Right input
#' @param axis  int, required
#'     The dimension to flip
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.dot <- function(...) {
  mx.varg.symbol.dot(list(...))
}

#' elemwise_add:
#' 
#' @param lhs  NDArray
#'     first input
#' @param rhs  NDArray
#'     second input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.elemwise_add <- function(...) {
  mx.varg.symbol.elemwise_add(list(...))
}

#' exp:Take exp of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:131
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.exp <- function(...) {
  mx.varg.symbol.exp(list(...))
}

#' expand_dims:Expand the shape of array by inserting a new axis.
#' 
#' From:src/operator/tensor/matrix_op.cc:48
#' 
#' @param src  NDArray
#'     Source input
#' @param axis  int (non-negative), required
#'     Position (amongst axes) where new axis is to be inserted.
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.expand_dims <- function(...) {
  mx.varg.symbol.expand_dims(list(...))
}

#' expm1:Take `exp(x) - 1` in a numerically stable way
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:176
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.expm1 <- function(...) {
  mx.varg.symbol.expm1(list(...))
}

#' fill_element_0index:Fill one element of each line(row for python, column for R/Julia) in lhs according to index indicated by rhs and values indicated by mhs. This function assume rhs uses 0-based index.
#' 
#' @param lhs  NDArray
#'     Left operand to the function.
#' @param mhs  NDArray
#'     Middle operand to the function.
#' @param rhs  NDArray
#'     Right operand to the function.
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.fill_element_0index <- function(...) {
  mx.varg.symbol.fill_element_0index(list(...))
}

#' fix:Take round of the src to integer nearest 0
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:98
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.fix <- function(...) {
  mx.varg.symbol.fix(list(...))
}

#' flip:Flip the input tensor along axis and return a new one.
#' 
#' From:src/operator/tensor/matrix_op.cc:142
#' 
#' @param src  NDArray
#'     Source input
#' @param axis  int, required
#'     The dimension to flip
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.flip <- function(...) {
  mx.varg.symbol.flip(list(...))
}

#' floor:Take floor of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:88
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.floor <- function(...) {
  mx.varg.symbol.floor(list(...))
}

#' gamma:Take the gamma function (extension of the factorial function) of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:302
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.gamma <- function(...) {
  mx.varg.symbol.gamma(list(...))
}

#' gammaln:Take gammaln (log of the absolute value of gamma(x)) of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:311
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.gammaln <- function(...) {
  mx.varg.symbol.gammaln(list(...))
}

#' identity:Identity mapping, copy src to output
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:14
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.identity <- function(...) {
  mx.varg.symbol.identity(list(...))
}

#' log:Take log of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:137
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.log <- function(...) {
  mx.varg.symbol.log(list(...))
}

#' log10:Take base-10 log of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:143
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.log10 <- function(...) {
  mx.varg.symbol.log10(list(...))
}

#' log1p:Take `log(1 + x)` in a numerically stable way
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:167
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.log1p <- function(...) {
  mx.varg.symbol.log1p(list(...))
}

#' log2:Take base-2 log of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:149
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.log2 <- function(...) {
  mx.varg.symbol.log2(list(...))
}

#' max:Compute max along axis. If axis is empty, global reduction is performed
#' 
#' From:src/operator/tensor/broadcast_reduce_op.cc:25
#' 
#' @param src  NDArray
#'     Source input
#' @param axis  Shape(tuple), optional, default=()
#'     Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.
#' @param keepdims  boolean, optional, default=False
#'     If true, the axis which is reduced is left in the result as dimension with size one.
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.max <- function(...) {
  mx.varg.symbol.max(list(...))
}

#' min:Compute min along axis. If axis is empty, global reduction is performed
#' 
#' From:src/operator/tensor/broadcast_reduce_op.cc:34
#' 
#' @param src  NDArray
#'     Source input
#' @param axis  Shape(tuple), optional, default=()
#'     Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.
#' @param keepdims  boolean, optional, default=False
#'     If true, the axis which is reduced is left in the result as dimension with size one.
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.min <- function(...) {
  mx.varg.symbol.min(list(...))
}

#' negative:Negate src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:54
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.negative <- function(...) {
  mx.varg.symbol.negative(list(...))
}

#' norm:
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.norm <- function(...) {
  mx.varg.symbol.norm(list(...))
}

#' normal:Sample a normal distribution
#' 
#' @param loc  float, optional, default=0
#'     Mean of the distribution.
#' @param scale  float, optional, default=1
#'     Standard deviation of the distribution.
#' @param shape  Shape(tuple), optional, default=()
#'     The shape of the output
#' @param ctx  string, optional, default=''
#'     Context of output, in format [cpu|gpu|cpu_pinned](n).Only used for imperative calls.
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.normal <- function(...) {
  mx.varg.symbol.normal(list(...))
}

#' radians:Take radians of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:239
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.radians <- function(...) {
  mx.varg.symbol.radians(list(...))
}

#' rint:Take round of the src to nearest integer
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:93
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.rint <- function(...) {
  mx.varg.symbol.rint(list(...))
}

#' round:Take round of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:78
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.round <- function(...) {
  mx.varg.symbol.round(list(...))
}

#' rsqrt:Take reciprocal square root of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:121
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.rsqrt <- function(...) {
  mx.varg.symbol.rsqrt(list(...))
}

#' sgd_mom_update:Updater function for sgd optimizer
#' 
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.sgd_mom_update <- function(...) {
  mx.varg.symbol.sgd_mom_update(list(...))
}

#' sgd_update:Updater function for sgd optimizer
#' 
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.sgd_update <- function(...) {
  mx.varg.symbol.sgd_update(list(...))
}

#' sign:Take sign of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:69
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.sign <- function(...) {
  mx.varg.symbol.sign(list(...))
}

#' sin:Take sin of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:158
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.sin <- function(...) {
  mx.varg.symbol.sin(list(...))
}

#' sinh:Take sinh of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:248
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.sinh <- function(...) {
  mx.varg.symbol.sinh(list(...))
}

#' slice_axis:Slice the input along certain axis and return a sliced array.
#' 
#' From:src/operator/tensor/matrix_op.cc:120
#' 
#' @param src  NDArray
#'     Source input
#' @param axis  int, required
#'     The axis to be sliced
#' @param begin  int, required
#'     The beginning index to be sliced
#' @param end  int, required
#'     The end index to be sliced
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.slice_axis <- function(...) {
  mx.varg.symbol.slice_axis(list(...))
}

#' smooth_l1:Calculate Smooth L1 Loss(lhs, scalar)
#' 
#' From:src/operator/tensor/elemwise_binary_scalar_op.cc:98
#' 
#' @param lhs  NDArray
#'     source input
#' @param scalar  float
#'     scalar input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.smooth_l1 <- function(...) {
  mx.varg.symbol.smooth_l1(list(...))
}

#' softmax_cross_entropy:Calculate cross_entropy(lhs, one_hot(rhs))
#' 
#' From:src/operator/loss_binary_op.cc:12
#' 
#' @param data  NDArray
#'     Input data
#' @param label  NDArray
#'     Input label
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.softmax_cross_entropy <- function(...) {
  mx.varg.symbol.softmax_cross_entropy(list(...))
}

#' sqrt:Take square root of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:112
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.sqrt <- function(...) {
  mx.varg.symbol.sqrt(list(...))
}

#' square:Take square of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:103
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.square <- function(...) {
  mx.varg.symbol.square(list(...))
}

#' sum:Sum src along axis. If axis is empty, global reduction is performed
#' 
#' From:src/operator/tensor/broadcast_reduce_op.cc:16
#' 
#' @param src  NDArray
#'     Source input
#' @param axis  Shape(tuple), optional, default=()
#'     Empty or unsigned or tuple. The axes to perform the reduction.If left empty, a global reduction will be performed.
#' @param keepdims  boolean, optional, default=False
#'     If true, the axis which is reduced is left in the result as dimension with size one.
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.sum <- function(...) {
  mx.varg.symbol.sum(list(...))
}

#' tan:Take tan of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:194
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.tan <- function(...) {
  mx.varg.symbol.tan(list(...))
}

#' tanh:Take tanh of the src
#' 
#' From:src/operator/tensor/elemwise_unary_op.cc:266
#' 
#' @param src  NDArray
#'     Source input
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.tanh <- function(...) {
  mx.varg.symbol.tanh(list(...))
}

#' transpose:Transpose the input tensor and return a new one
#' 
#' From:src/operator/tensor/matrix_op.cc:20
#' 
#' @param src  NDArray
#'     Source input
#' @param axes  Shape(tuple), optional, default=()
#'     Target axis order. By default the axes will be inverted.
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.transpose <- function(...) {
  mx.varg.symbol.transpose(list(...))
}

#' uniform:Sample a uniform distribution
#' 
#' @param low  float, optional, default=0
#'     The lower bound of distribution
#' @param high  float, optional, default=1
#'     The upper bound of distribution
#' @param shape  Shape(tuple), optional, default=()
#'     The shape of the output
#' @param ctx  string, optional, default=''
#'     Context of output, in format [cpu|gpu|cpu_pinned](n).Only used for imperative calls.
#' @param name  string, optional
#'     Name of the resulting symbol.
#' @return out The result mx.symbol
#' 
#' @export
mx.symbol.uniform <- function(...) {
  mx.varg.symbol.uniform(list(...))
}
