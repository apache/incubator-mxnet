% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mxnet_generated.R
\name{mx.nd.Activation}
\alias{mx.nd.Activation}
\title{Elementwise activation function.}
\arguments{
\item{act.type}{{'relu', 'sigmoid', 'softrelu', 'tanh'}, required
Activation function to be applied.}
}
\value{
out The result mx.ndarray
}
\description{
The following activation types are supported (operations are applied elementwisely to each
scalar of the input tensor):
}
\details{
- `relu`: Rectified Linear Unit, `y = max(x, 0)`
- `sigmoid`: `y = 1 / (1 + exp(-x))`
- `tanh`: Hyperbolic tangent, `y = (exp(x) - exp(-x)) / (exp(x) + exp(-x))`
- `softrelu`: Soft ReLU, or SoftPlus, `y = log(1 + exp(x))`

See `LeakyReLU` for other activations with parameters.
}

